"""ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (ë‹¤ìš´ë¡œë“œ) ëª¨ë“ˆ.

Hugging Face Hubì—ì„œ GPT-2 ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ì™€ ì„¤ì • íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬
ë¡œì»¬ì— ì €ì¥í•œë‹¤. ì´ë¯¸ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì¤‘ë³µ ë‹¤ìš´ë¡œë“œë¥¼ ë°©ì§€í•œë‹¤.
"""

from __future__ import annotations

import logging
from pathlib import Path
from typing import TypedDict

from transformers import AutoConfig, AutoTokenizer, PreTrainedTokenizerBase

logger = logging.getLogger(__name__)


class InitResult(TypedDict):
    """ì´ˆê¸°í™” ê²°ê³¼ íƒ€ì….

    Attributes:
        tokenizer_dir: í† í¬ë‚˜ì´ì €ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬
        vocab_size: ì–´íœ˜ í¬ê¸°
        model_name: Hugging Face Hub ëª¨ë¸ ì´ë¦„
    """

    tokenizer_dir: Path
    vocab_size: int
    model_name: str


def initialize_assets(
    model_name: str,
    tokenizer_dir: Path,
    force: bool,
) -> InitResult:
    """Hugging Face Hubì—ì„œ GPT-2 í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ì„¤ì •ì„ ë‹¤ìš´ë¡œë“œí•œë‹¤.

    Args:
        model_name: Hugging Face Hub ëª¨ë¸ ì´ë¦„
        tokenizer_dir: í† í¬ë‚˜ì´ì € ì €ì¥ ë””ë ‰í† ë¦¬
        force: Trueì´ë©´ ê¸°ì¡´ íŒŒì¼ì´ ìˆì–´ë„ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ

    Returns:
        ì´ˆê¸°í™” ê²°ê³¼ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
    """
    logger.info("ğŸš€ ëª¨ë¸ ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤: %s", model_name)

    # ì´ë¯¸ í† í¬ë‚˜ì´ì €ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    tokenizer_files = list(tokenizer_dir.glob("*")) if tokenizer_dir.exists() else []
    has_tokenizer = any(
        f.name in ["tokenizer.json", "vocab.json", "merges.txt"]
        for f in tokenizer_files
    )

    if has_tokenizer and not force:
        logger.info("âœ… í† í¬ë‚˜ì´ì €ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: %s", tokenizer_dir)
        tokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(
            str(tokenizer_dir)
        )
        vocab_size = len(tokenizer.get_vocab())
        logger.info("  â””â”€ vocab í¬ê¸°: %d", vocab_size)
        return InitResult(
            tokenizer_dir=tokenizer_dir,
            vocab_size=vocab_size,
            model_name=model_name,
        )

    # Hubì—ì„œ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
    logger.info("ğŸ“¥ Hugging Face Hubì—ì„œ í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤: %s", model_name)
    tokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(model_name)

    tokenizer_dir.mkdir(parents=True, exist_ok=True)
    tokenizer.save_pretrained(str(tokenizer_dir))

    vocab_size = len(tokenizer.get_vocab())
    logger.info("âœ… í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: %s", tokenizer_dir)
    logger.info("  â””â”€ vocab í¬ê¸°: %d", vocab_size)

    # ëª¨ë¸ ì„¤ì • ë‹¤ìš´ë¡œë“œ (ê°€ì¤‘ì¹˜ëŠ” ì œì™¸, ì„¤ì •ë§Œ ì €ì¥)
    logger.info("ğŸ“¥ ëª¨ë¸ ì„¤ì •(config)ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤: %s", model_name)
    config = AutoConfig.from_pretrained(model_name)
    config.save_pretrained(str(tokenizer_dir))
    logger.info("âœ… ëª¨ë¸ ì„¤ì • ì €ì¥ ì™„ë£Œ: %s", tokenizer_dir)

    logger.info("ğŸ‰ ì´ˆê¸°í™” ì™„ë£Œ.")
    return InitResult(
        tokenizer_dir=tokenizer_dir,
        vocab_size=vocab_size,
        model_name=model_name,
    )
