"""í† í¬ë‚˜ì´ì € ì¦ë¥˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§"""

from __future__ import annotations

import os
from pathlib import Path
from typing import Iterator, TypedDict, cast

from tokenizers import Tokenizer, models, pre_tokenizers, trainers
from transformers import AutoTokenizer, PreTrainedTokenizerBase, PreTrainedTokenizerFast

from gpt2_ivr.utils.logging_config import get_logger

# ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”
os.environ["TOKENIZERS_PARALLELISM"] = "true"

logger = get_logger(__name__)


class DistillResult(TypedDict):
    """ì¦ë¥˜ ê²°ê³¼ íƒ€ì…"""

    output_dir: Path
    vocab_size: int
    original_vocab_size: int


def get_training_corpus(
    corpus_dir: Path, batch_size: int = 1000
) -> Iterator[list[str]]:
    """í´ë¦° ì½”í¼ìŠ¤ ë””ë ‰í† ë¦¬ì—ì„œ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ í•™ìŠµ ì½”í¼ìŠ¤ ì´í„°ë ˆì´í„°ë¥¼ ìƒì„±í•œë‹¤.

    Args:
        corpus_dir: ì½”í¼ìŠ¤ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        batch_size: ë°°ì¹˜ í¬ê¸°

    Yields:
        í…ìŠ¤íŠ¸ ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
    """
    files = list(corpus_dir.glob("*.txt"))
    if not files:
        logger.warning(
            "ê²½ê³ : %s ë””ë ‰í† ë¦¬ì— í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì € í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            corpus_dir,
        )
        yield []
        return

    logger.info(
        "ğŸ“š %dê°œì˜ ì½”í¼ìŠ¤ íŒŒì¼ì„ '%s'ì—ì„œ ì½ì–´ë“¤ì…ë‹ˆë‹¤.", len(files), corpus_dir
    )
    batch: list[str] = []
    for file_path in files:
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                stripped_line = line.strip()
                if stripped_line:  # ë¹ˆ ì¤„ì€ ë¬´ì‹œ
                    batch.append(stripped_line)
                    if len(batch) >= batch_size:
                        yield batch
                        batch = []
    if batch:  # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        yield batch


def distill_unigram_tokenizer(
    original_tokenizer_dir: Path,
    distilled_tokenizer_dir: Path,
    corpus_dir: Path,
) -> DistillResult:
    """GPT-2 BPE í† í¬ë‚˜ì´ì €ì˜ ë™ì‘ì„ ëª¨ë°©í•˜ëŠ” Unigram í† í¬ë‚˜ì´ì €ë¥¼ distillation ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤.

    ì–´íœ˜ í¬ê¸°ëŠ” ì›ë³¸ í† í¬ë‚˜ì´ì €ì™€ ë™ì¼í•˜ê²Œ ë§ì¶˜ë‹¤.

    Args:
        original_tokenizer_dir: ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬
        distilled_tokenizer_dir: ì¦ë¥˜ëœ í† í¬ë‚˜ì´ì € ì €ì¥ ë””ë ‰í† ë¦¬
        corpus_dir: í•™ìŠµ ì½”í¼ìŠ¤ ë””ë ‰í† ë¦¬
    Returns:
        ì¦ë¥˜ ê²°ê³¼ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬

    Raises:
        Exception: í† í¬ë‚˜ì´ì € ë¡œë“œ ë˜ëŠ” í•™ìŠµ ì‹¤íŒ¨ ì‹œ
    """
    logger.info("ğŸš€ Unigram í† í¬ë‚˜ì´ì € Distillationì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # 1. GPT-2 BPE í† í¬ë‚˜ì´ì € ë¡œë“œ
    # init ë‹¨ê³„ì—ì„œ ë‚´ë ¤ë°›ì€ ë¡œì»¬ í† í¬ë‚˜ì´ì €ë§Œ ì‚¬ìš©í•œë‹¤.
    tokenizer_files = (
        list(original_tokenizer_dir.glob("*"))
        if original_tokenizer_dir.exists()
        else []
    )

    has_tokenizer_files = any(
        f.name in ["tokenizer.json", "vocab.json", "merges.txt"]
        for f in tokenizer_files
    )

    if not has_tokenizer_files:
        raise FileNotFoundError(
            f"ì›ë³¸ í† í¬ë‚˜ì´ì € íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {original_tokenizer_dir}"
        )

    try:
        original_tokenizer = cast(
            PreTrainedTokenizerBase,
            AutoTokenizer.from_pretrained(str(original_tokenizer_dir)),
        )
        logger.info(
            "âœ… ì›ë³¸ GPT-2 í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ. (vocab_size: %d)",
            len(original_tokenizer.get_vocab()),
        )
    except Exception as e:
        raise RuntimeError(
            f"ì›ë³¸ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {original_tokenizer_dir}"
        ) from e

    original_vocab_size = len(original_tokenizer.get_vocab())
    vocab_size = original_vocab_size

    # 2. Unigram í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
    tokenizer = Tokenizer(models.Unigram())

    # ì‚¬ì „ í† í¬ë‚˜ì´ì € ì„¤ì •: ByteLevel PreTokenizer
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    logger.info("âœ¨ Unigram í† í¬ë‚˜ì´ì €ì™€ ByteLevel PreTokenizer ì„¤ì • ì™„ë£Œ.")

    # 3. íŠ¸ë ˆì´ë„ˆ ì„¤ì •
    special_tokens = original_tokenizer.all_special_tokens
    # ì›ë³¸ í† í¬ë‚˜ì´ì €ì˜ unk_token ì‚¬ìš© (GPT-2ëŠ” <|endoftext|>ê°€ unk ì—­í• )
    unk_token = original_tokenizer.unk_token or original_tokenizer.eos_token
    assert isinstance(unk_token, str)
    if unk_token not in special_tokens:
        special_tokens.append(unk_token)

    trainer = trainers.UnigramTrainer(
        vocab_size=vocab_size, special_tokens=special_tokens, unk_token=unk_token
    )
    logger.info(
        "âš™ï¸ UnigramTrainer ì„¤ì • ì™„ë£Œ. (vocab_size: %d, special_tokens: %s)",
        vocab_size,
        special_tokens,
    )

    # 4. ì½”í¼ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì € í•™ìŠµ
    logger.info("ğŸ“š ì½”í¼ìŠ¤ ë””ë ‰í† ë¦¬ '%s'ì—ì„œ í† í¬ë‚˜ì´ì € í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.", corpus_dir)
    # ë°°ì¹˜ í¬ê¸°ë¥¼ í¬ê²Œ í•˜ì—¬ I/O ì˜¤ë²„í—¤ë“œ ê°ì†Œ
    tokenizer.train_from_iterator(
        get_training_corpus(corpus_dir, batch_size=10000), trainer=trainer
    )
    logger.info("âœ… Unigram í† í¬ë‚˜ì´ì € í•™ìŠµ ì™„ë£Œ.")

    # 5. Distilled Unigram í† í¬ë‚˜ì´ì € ì €ì¥
    distilled_tokenizer_dir.mkdir(parents=True, exist_ok=True)
    tokenizer_json_path = distilled_tokenizer_dir / "tokenizer.json"
    tokenizer.save(str(tokenizer_json_path))
    logger.info(
        "ğŸ’¾ Distilled Unigram í† í¬ë‚˜ì´ì €ë¥¼ '%s'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.", tokenizer_json_path
    )

    # Hugging Face PreTrainedTokenizerFastì™€ í˜¸í™˜ë˜ë„ë¡ ì¶”ê°€ íŒŒì¼ ìƒì„±
    hf_tokenizer = PreTrainedTokenizerFast(
        tokenizer_object=tokenizer,
        unk_token=unk_token,
        model_max_length=original_tokenizer.model_max_length,
        bos_token=original_tokenizer.bos_token,
        eos_token=original_tokenizer.eos_token,
        pad_token=original_tokenizer.pad_token,
    )
    hf_tokenizer.save_pretrained(distilled_tokenizer_dir)
    logger.info(
        "ğŸ“„ Hugging Face `PreTrainedTokenizerFast` í˜¸í™˜ íŒŒì¼ì„ '%s'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.",
        distilled_tokenizer_dir,
    )

    logger.info("ğŸ‰ Unigram í† í¬ë‚˜ì´ì € Distillation ë‹¨ê³„ ì™„ë£Œ.")

    return DistillResult(
        output_dir=distilled_tokenizer_dir,
        vocab_size=vocab_size,
        original_vocab_size=original_vocab_size,
    )
