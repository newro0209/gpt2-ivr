"""í† í¬ë‚˜ì´ì € ì¦ë¥˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§"""

from __future__ import annotations

import os
from pathlib import Path
from typing import Iterator, TypedDict

from tokenizers import Tokenizer, models, pre_tokenizers, trainers
from transformers import AutoTokenizer, PreTrainedTokenizerFast

from gpt2_ivr.utils.logging_config import get_logger

# ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”
os.environ["TOKENIZERS_PARALLELISM"] = "true"

logger = get_logger(__name__)


class DistillResult(TypedDict):
    """ì¦ë¥˜ ê²°ê³¼ íƒ€ì…"""

    output_dir: Path
    vocab_size: int
    original_vocab_size: int


def get_training_corpus(
    corpus_dir: Path, batch_size: int = 1000
) -> Iterator[list[str]]:
    """í´ë¦° ì½”í¼ìŠ¤ ë””ë ‰í† ë¦¬ì—ì„œ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ í•™ìŠµ ì½”í¼ìŠ¤ ì´í„°ë ˆì´í„°ë¥¼ ìƒì„±í•œë‹¤.

    Args:
        corpus_dir: ì½”í¼ìŠ¤ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        batch_size: ë°°ì¹˜ í¬ê¸°

    Yields:
        í…ìŠ¤íŠ¸ ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
    """
    files = list(corpus_dir.glob("*.txt"))
    if not files:
        logger.warning(
            "ê²½ê³ : %s ë””ë ‰í† ë¦¬ì— í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì € í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            corpus_dir,
        )
        yield []
        return

    logger.info(
        "ğŸ“š %dê°œì˜ ì½”í¼ìŠ¤ íŒŒì¼ì„ '%s'ì—ì„œ ì½ì–´ë“¤ì…ë‹ˆë‹¤.", len(files), corpus_dir
    )
    batch: list[str] = []
    for file_path in files:
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                stripped_line = line.strip()
                if stripped_line:  # ë¹ˆ ì¤„ì€ ë¬´ì‹œ
                    batch.append(stripped_line)
                    if len(batch) >= batch_size:
                        yield batch
                        batch = []
    if batch:  # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        yield batch


def distill_unigram_tokenizer(
    original_tokenizer_dir: Path,
    distilled_tokenizer_dir: Path,
    corpus_dir: Path,
    vocab_size: int,
    model_name: str,
) -> DistillResult:
    """GPT-2 BPE í† í¬ë‚˜ì´ì €ì˜ ë™ì‘ì„ ëª¨ë°©í•˜ëŠ” Unigram í† í¬ë‚˜ì´ì €ë¥¼ distillation ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤.

    Args:
        original_tokenizer_dir: ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬
        distilled_tokenizer_dir: ì¦ë¥˜ëœ í† í¬ë‚˜ì´ì € ì €ì¥ ë””ë ‰í† ë¦¬
        corpus_dir: í•™ìŠµ ì½”í¼ìŠ¤ ë””ë ‰í† ë¦¬
        vocab_size: í•™ìŠµí•  Unigram í† í¬ë‚˜ì´ì €ì˜ ì–´íœ˜ í¬ê¸°
        model_name: Hugging Face Hubì—ì„œ ë¡œë“œí•  ëª¨ë¸ ì´ë¦„

    Returns:
        ì¦ë¥˜ ê²°ê³¼ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬

    Raises:
        Exception: í† í¬ë‚˜ì´ì € ë¡œë“œ ë˜ëŠ” í•™ìŠµ ì‹¤íŒ¨ ì‹œ
    """
    logger.info("ğŸš€ Unigram í† í¬ë‚˜ì´ì € Distillationì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # 1. GPT-2 BPE í† í¬ë‚˜ì´ì € ë¡œë“œ
    # ì›ë³¸ ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•œ í† í¬ë‚˜ì´ì € íŒŒì¼ì´ ì—†ìœ¼ë©´ Hubì—ì„œ ë¡œë“œ
    tokenizer_files = (
        list(original_tokenizer_dir.glob("*"))
        if original_tokenizer_dir.exists()
        else []
    )

    if tokenizer_files and any(
        f.name in ["tokenizer.json", "vocab.json", "merges.txt"]
        for f in tokenizer_files
    ):
        try:
            original_tokenizer = AutoTokenizer.from_pretrained(
                str(original_tokenizer_dir)
            )
            logger.info(
                "âœ… ì›ë³¸ GPT-2 í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ. (vocab_size: %d)",
                len(original_tokenizer.get_vocab()),
            )
        except Exception as e:
            logger.warning(
                "ì›ë³¸ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨(%s): %s", original_tokenizer_dir, e
            )
            logger.info("Hugging Face Hubì—ì„œ '%s'ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.", model_name)
            original_tokenizer = AutoTokenizer.from_pretrained(model_name)

            # Hubì—ì„œ ë¡œë“œí•œ í† í¬ë‚˜ì´ì €ë¥¼ original ë””ë ‰í† ë¦¬ì— ì €ì¥
            original_tokenizer_dir.mkdir(parents=True, exist_ok=True)
            original_tokenizer.save_pretrained(str(original_tokenizer_dir))
            logger.info(
                "âœ… Hubì—ì„œ '%s' í† í¬ë‚˜ì´ì € ë¡œë“œ ë° ì €ì¥ ì™„ë£Œ. (vocab_size: %d)",
                model_name,
                len(original_tokenizer.get_vocab()),
            )
    else:
        logger.info(
            "ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. Hugging Face Hubì—ì„œ '%s'ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.",
            model_name,
        )
        original_tokenizer = AutoTokenizer.from_pretrained(model_name)

        # Hubì—ì„œ ë¡œë“œí•œ í† í¬ë‚˜ì´ì €ë¥¼ original ë””ë ‰í† ë¦¬ì— ì €ì¥
        original_tokenizer_dir.mkdir(parents=True, exist_ok=True)
        original_tokenizer.save_pretrained(str(original_tokenizer_dir))
        logger.info(
            "âœ… Hubì—ì„œ '%s' í† í¬ë‚˜ì´ì € ë¡œë“œ ë° ì €ì¥ ì™„ë£Œ. (vocab_size: %d)",
            model_name,
            len(original_tokenizer.get_vocab()),
        )

    original_vocab_size = len(original_tokenizer.get_vocab())

    # 2. Unigram í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
    tokenizer = Tokenizer(models.Unigram())

    # ì‚¬ì „ í† í¬ë‚˜ì´ì € ì„¤ì •: ByteLevel PreTokenizer
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    logger.info("âœ¨ Unigram í† í¬ë‚˜ì´ì €ì™€ ByteLevel PreTokenizer ì„¤ì • ì™„ë£Œ.")

    # 3. íŠ¸ë ˆì´ë„ˆ ì„¤ì •
    special_tokens = original_tokenizer.all_special_tokens
    # ì›ë³¸ í† í¬ë‚˜ì´ì €ì˜ unk_token ì‚¬ìš© (GPT-2ëŠ” <|endoftext|>ê°€ unk ì—­í• )
    unk_token = (
        original_tokenizer.unk_token or original_tokenizer.eos_token or "<|endoftext|>"
    )
    if unk_token not in special_tokens:
        special_tokens.append(unk_token)

    trainer = trainers.UnigramTrainer(
        vocab_size=vocab_size,
        special_tokens=special_tokens,
        unk_token=unk_token,
        show_progress=True,
        # EM ìµœì í™” íŒŒë¼ë¯¸í„°
        max_piece_length=16,  # ìµœëŒ€ í† í° ê¸¸ì´ ì œí•œ (ì§§ì„ìˆ˜ë¡ ë¹ ë¦„)
        n_sub_iterations=2,  # EM í•˜ìœ„ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸ê°’ 2, ì¤„ì´ë©´ ë¹ ë¥´ì§€ë§Œ í’ˆì§ˆ ì €í•˜)
        shrinking_factor=0.75,  # vocab ì¶•ì†Œ ê³„ìˆ˜ (í¬ê²Œ í•˜ë©´ ë¹ ë¦„, ê¸°ë³¸ê°’ 0.75)
    )
    logger.info(
        "âš™ï¸ UnigramTrainer ì„¤ì • ì™„ë£Œ. (vocab_size: %d, special_tokens: %s)",
        vocab_size,
        special_tokens,
    )

    # 4. ì½”í¼ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì € í•™ìŠµ
    logger.info("ğŸ“š ì½”í¼ìŠ¤ ë””ë ‰í† ë¦¬ '%s'ì—ì„œ í† í¬ë‚˜ì´ì € í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.", corpus_dir)
    # ë°°ì¹˜ í¬ê¸°ë¥¼ í¬ê²Œ í•˜ì—¬ I/O ì˜¤ë²„í—¤ë“œ ê°ì†Œ
    tokenizer.train_from_iterator(
        get_training_corpus(corpus_dir, batch_size=10000), trainer=trainer
    )
    logger.info("âœ… Unigram í† í¬ë‚˜ì´ì € í•™ìŠµ ì™„ë£Œ.")

    # 5. Distilled Unigram í† í¬ë‚˜ì´ì € ì €ì¥
    distilled_tokenizer_dir.mkdir(parents=True, exist_ok=True)
    tokenizer_json_path = distilled_tokenizer_dir / "tokenizer.json"
    tokenizer.save(str(tokenizer_json_path))
    logger.info(
        "ğŸ’¾ Distilled Unigram í† í¬ë‚˜ì´ì €ë¥¼ '%s'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.", tokenizer_json_path
    )

    # Hugging Face PreTrainedTokenizerFastì™€ í˜¸í™˜ë˜ë„ë¡ ì¶”ê°€ íŒŒì¼ ìƒì„±
    hf_tokenizer = PreTrainedTokenizerFast(
        tokenizer_object=tokenizer,
        unk_token=unk_token,
        model_max_length=original_tokenizer.model_max_length,
        bos_token=original_tokenizer.bos_token,
        eos_token=original_tokenizer.eos_token,
        pad_token=original_tokenizer.pad_token,
    )
    hf_tokenizer.save_pretrained(distilled_tokenizer_dir)
    logger.info(
        "ğŸ“„ Hugging Face `PreTrainedTokenizerFast` í˜¸í™˜ íŒŒì¼ì„ '%s'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.",
        distilled_tokenizer_dir,
    )

    logger.info("ğŸ‰ Unigram í† í¬ë‚˜ì´ì € Distillation ë‹¨ê³„ ì™„ë£Œ.")

    return DistillResult(
        output_dir=distilled_tokenizer_dir,
        vocab_size=vocab_size,
        original_vocab_size=original_vocab_size,
    )
