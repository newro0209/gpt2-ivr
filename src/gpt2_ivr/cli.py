"""GPT2-IVR CLI ì§„ì…ì  ëª¨ë“ˆ.

Tokenizer Model Migration + IVR íŒŒì´í”„ë¼ì¸ì˜ ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•œë‹¤.
Rich ê¸°ë°˜ ì½˜ì†” ì¶œë ¥ ë° ë¡œê¹…ì„ ì§€ì›í•œë‹¤.
"""

from __future__ import annotations

import argparse
import logging

from datetime import datetime
from pathlib import Path
from time import perf_counter
from typing import Any

from pyfiglet import Figlet
from rich.console import Console
from rich.logging import RichHandler
from rich.panel import Panel
from rich.table import Table
from rich.text import Text

from gpt2_ivr.commands import (
    AlignCommand,
    AnalyzeCommand,
    Command,
    DistillCommand,
    InitCommand,
    RemapCommand,
    SelectCommand,
    TrainCommand,
)
from gpt2_ivr.constants import (
    BPE_TOKEN_ID_SEQUENCES_FILE,
    CORPORA_CLEANED_DIR,
    EMBEDDINGS_ROOT,
    REPLACEMENT_CANDIDATES_FILE,
    SELECTION_LOG_FILE,
    TOKENIZER_DISTILLED_UNIGRAM_DIR,
    TOKENIZER_ORIGINAL_DIR,
    TOKENIZER_REMAPPED_DIR,
    TOKEN_FREQUENCY_FILE,
    LOGS_DIR,
)

LOGGER_NAME = "gpt2_ivr.cli"
REMAP_RULES_PATH = Path("src/gpt2_ivr/tokenizer/remap_rules.yaml")

_CONSOLE = Console(stderr=False)


class CliHelpFormatter(
    argparse.ArgumentDefaultsHelpFormatter, argparse.RawTextHelpFormatter
):
    """CLI ë„ì›€ë§ í¬ë§·í„°.

    ArgumentDefaultsHelpFormatterì™€ RawTextHelpFormatterë¥¼ ê²°í•©í•˜ì—¬
    ê¸°ë³¸ê°’ í‘œì‹œì™€ ì›ì‹œ í…ìŠ¤íŠ¸ í¬ë§·ì„ ë™ì‹œì— ì§€ì›í•œë‹¤.
    """


class CliArgumentParser(argparse.ArgumentParser):
    """ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ Rich ìŠ¤íƒ€ì¼ë¡œ ì¶œë ¥í•˜ëŠ” argparse íŒŒì„œ.

    ì¸ì íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ ì‹œ Rich Panelë¡œ ì˜¤ë¥˜ë¥¼ í‘œì‹œí•˜ì—¬
    ì‚¬ìš©ì ê²½í—˜ì„ ê°œì„ í•œë‹¤.
    """

    def error(self, message: str) -> None:
        """ì¸ì íŒŒì‹± ì˜¤ë¥˜ë¥¼ Rich íŒ¨ë„ë¡œ ì¶œë ¥í•œë‹¤.

        Args:
            message: ì˜¤ë¥˜ ë©”ì‹œì§€
        """
        console = _CONSOLE
        console.print(
            Panel.fit(
                f"[bold red]ì¸ì ì˜¤ë¥˜[/bold red]\n{message}\n\n"
                f"[dim]ë„ì›€ë§: uv run ivr --help[/dim]",
                title="CLI ì…ë ¥ ì˜¤ë¥˜",
                border_style="red",
            )
        )
        raise SystemExit(2)


def non_negative_int(value: str) -> int:
    """0 ì´ìƒì˜ ì •ìˆ˜ ì¸ìë¥¼ íŒŒì‹±í•œë‹¤.

    Args:
        value: íŒŒì‹±í•  ë¬¸ìì—´ ê°’

    Returns:
        íŒŒì‹±ëœ 0 ì´ìƒì˜ ì •ìˆ˜ ê°’

    Raises:
        argparse.ArgumentTypeError: ê°’ì´ ì •ìˆ˜ê°€ ì•„ë‹ˆê±°ë‚˜ 0ë³´ë‹¤ ì‘ì€ ê²½ìš°
    """
    try:
        parsed = int(value)
    except ValueError as e:
        raise argparse.ArgumentTypeError("ì •ìˆ˜ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.") from e

    if parsed < 0:
        raise argparse.ArgumentTypeError("0 ì´ìƒì˜ ì •ìˆ˜ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.")
    return parsed


def positive_int(value: str) -> int:
    """1 ì´ìƒì˜ ì •ìˆ˜ ì¸ìë¥¼ íŒŒì‹±í•œë‹¤.

    Args:
        value: íŒŒì‹±í•  ë¬¸ìì—´ ê°’

    Returns:
        íŒŒì‹±ëœ 1 ì´ìƒì˜ ì •ìˆ˜ ê°’

    Raises:
        argparse.ArgumentTypeError: ê°’ì´ ì •ìˆ˜ê°€ ì•„ë‹ˆê±°ë‚˜ 0 ì´í•˜ì¸ ê²½ìš°
    """
    try:
        parsed = int(value)
    except ValueError as e:
        raise argparse.ArgumentTypeError("ì •ìˆ˜ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.") from e

    if parsed <= 0:
        raise argparse.ArgumentTypeError("1 ì´ìƒì˜ ì •ìˆ˜ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.")
    return parsed


def main() -> int:
    """CLI ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸.

    íŒŒì´í”„ë¼ì¸ ëª…ë ¹ì–´ë¥¼ íŒŒì‹±í•˜ê³  ì‹¤í–‰í•œë‹¤. ê° ë‹¨ê³„ë³„ ëª…ë ¹ì–´ëŠ”
    ì„œë¸Œì»¤ë§¨ë“œë¡œ ì œê³µë˜ë©°, Rich ê¸°ë°˜ ì½˜ì†” ì¶œë ¥ê³¼ íŒŒì¼ ë¡œê¹…ì„ ì§€ì›í•œë‹¤.

    Returns:
        ì¢…ë£Œ ì½”ë“œ (0: ì„±ê³µ, 1: ì˜¤ë¥˜, 130: ì‚¬ìš©ì ì¤‘ë‹¨)
    """
    # 1. CLI íŒŒì„œì™€ ì„œë¸Œì»¤ë§¨ë“œë¥¼ ì •ì˜í•œë‹¤.
    parser = CliArgumentParser(
        prog="ivr",
        description="Tokenizer Model Migration + IVR íŒŒì´í”„ë¼ì¸ CLI",
        formatter_class=CliHelpFormatter,
    )
    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="ì½˜ì†” ë¡œê¹… ë ˆë²¨",
    )

    subparsers = parser.add_subparsers(dest="command", required=True, metavar="command")

    init_parser = subparsers.add_parser(
        "init",
        help="ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”",
        formatter_class=CliHelpFormatter,
    )
    init_parser.add_argument(
        "--model-name",
        default="openai-community/gpt2",
        help="Hugging Face Hub ëª¨ë¸ ì´ë¦„",
    )
    init_parser.add_argument(
        "--tokenizer-dir",
        type=Path,
        default=TOKENIZER_ORIGINAL_DIR,
        help="í† í¬ë‚˜ì´ì € ì €ì¥ ë””ë ‰í† ë¦¬",
    )
    init_parser.add_argument(
        "--force",
        action="store_true",
        default=False,
        help="ê¸°ì¡´ íŒŒì¼ì´ ìˆì–´ë„ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ",
    )

    analyze_parser = subparsers.add_parser(
        "analyze",
        help="BPE í† í° ì‹œí€€ìŠ¤ ë¶„ì„",
        formatter_class=CliHelpFormatter,
    )
    analyze_parser.add_argument(
        "--input-dir",
        type=Path,
        default=CORPORA_CLEANED_DIR,
        help="ì½”í¼ìŠ¤ ì…ë ¥ ë””ë ‰í† ë¦¬",
    )
    analyze_parser.add_argument(
        "--output-sequences",
        type=Path,
        default=BPE_TOKEN_ID_SEQUENCES_FILE,
        help="BPE í† í° ì‹œí€€ìŠ¤ ì¶œë ¥ ê²½ë¡œ",
    )
    analyze_parser.add_argument(
        "--output-frequency",
        type=Path,
        default=TOKEN_FREQUENCY_FILE,
        help="í† í° ë¹ˆë„ parquet ì¶œë ¥ ê²½ë¡œ",
    )
    analyze_parser.add_argument(
        "--tokenizer-dir",
        type=Path,
        default=TOKENIZER_ORIGINAL_DIR,
        help="ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
    )
    analyze_parser.add_argument(
        "--workers",
        type=non_negative_int,
        default=0,
        help="ìŠ¤ë ˆë“œ ì›Œì»¤ ìˆ˜ (0ì´ë©´ CPU - 1)",
    )
    analyze_parser.add_argument(
        "--chunk-size",
        type=non_negative_int,
        default=0,
        help="ìŠ¤ë ˆë“œ ì²­í¬ í¬ê¸°(0ì´ë©´ ìë™ ì„¤ì •)",
    )
    analyze_parser.add_argument(
        "--max-texts",
        type=non_negative_int,
        default=0,
        help="ì²˜ë¦¬í•  ìµœëŒ€ í…ìŠ¤íŠ¸ ìˆ˜ (0ì´ë©´ ì „ì²´)",
    )
    analyze_parser.add_argument(
        "--text-key",
        default="text",
        help="json/jsonl í…ìŠ¤íŠ¸ í‚¤",
    )
    analyze_parser.add_argument(
        "--encoding",
        default="utf-8",
        help="ì…ë ¥ íŒŒì¼ ì¸ì½”ë”©",
    )

    distill_parser = subparsers.add_parser(
        "distill-tokenizer",
        help="BPE -> Unigram distillation",
        formatter_class=CliHelpFormatter,
    )
    distill_parser.add_argument(
        "--original-tokenizer-dir",
        type=Path,
        default=TOKENIZER_ORIGINAL_DIR,
        help="ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
    )
    distill_parser.add_argument(
        "--distilled-tokenizer-dir",
        type=Path,
        default=TOKENIZER_DISTILLED_UNIGRAM_DIR,
        help="ì¦ë¥˜ëœ í† í¬ë‚˜ì´ì € ì €ì¥ ë””ë ‰í† ë¦¬",
    )
    distill_parser.add_argument(
        "--corpus-dir",
        type=Path,
        default=CORPORA_CLEANED_DIR,
        help="í•™ìŠµ ì½”í¼ìŠ¤ ë””ë ‰í† ë¦¬",
    )

    select_parser = subparsers.add_parser(
        "select",
        help="IVR ëŒ€ìƒ í† í° ì„ ì •",
        formatter_class=CliHelpFormatter,
    )
    select_parser.add_argument(
        "--frequency-path",
        type=Path,
        default=TOKEN_FREQUENCY_FILE,
        help="í† í° ë¹ˆë„ parquet íŒŒì¼ ê²½ë¡œ",
    )
    select_parser.add_argument(
        "--sequences-path",
        type=Path,
        default=BPE_TOKEN_ID_SEQUENCES_FILE,
        help="BPE í† í° ì‹œí€€ìŠ¤ íŒŒì¼ ê²½ë¡œ",
    )
    select_parser.add_argument(
        "--output-csv",
        type=Path,
        default=REPLACEMENT_CANDIDATES_FILE,
        help="êµì²´ í›„ë³´ CSV ì €ì¥ ê²½ë¡œ",
    )
    select_parser.add_argument(
        "--output-log",
        type=Path,
        default=SELECTION_LOG_FILE,
        help="ì„ ì • ë¡œê·¸ ì €ì¥ ê²½ë¡œ",
    )
    select_parser.add_argument(
        "--tokenizer-dir",
        type=Path,
        default=TOKENIZER_ORIGINAL_DIR,
        help="ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
    )
    select_parser.add_argument(
        "--max-candidates",
        type=positive_int,
        default=1000,
        help="ìµœëŒ€ í›„ë³´ ê°œìˆ˜",
    )
    select_parser.add_argument(
        "--min-token-len",
        type=positive_int,
        default=2,
        help="ë³´í˜¸ í† í° ìµœì†Œ ê¸¸ì´",
    )

    remap_parser = subparsers.add_parser(
        "remap",
        help="í† í° ì¬í• ë‹¹ ê·œì¹™ ì ìš©",
        formatter_class=CliHelpFormatter,
    )
    remap_parser.add_argument(
        "--distilled-tokenizer-dir",
        type=Path,
        default=TOKENIZER_DISTILLED_UNIGRAM_DIR,
        help="ì¦ë¥˜ëœ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
    )
    remap_parser.add_argument(
        "--remapped-tokenizer-dir",
        type=Path,
        default=TOKENIZER_REMAPPED_DIR,
        help="ì¬í• ë‹¹ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
    )
    remap_parser.add_argument(
        "--remap-rules-path",
        type=Path,
        default=REMAP_RULES_PATH,
        help="ì¬í• ë‹¹ ê·œì¹™ íŒŒì¼ ê²½ë¡œ",
    )
    remap_parser.add_argument(
        "--replacement-candidates-path",
        type=Path,
        default=REPLACEMENT_CANDIDATES_FILE,
        help="êµì²´ í›„ë³´ CSV ê²½ë¡œ",
    )

    align_parser = subparsers.add_parser(
        "align",
        help="ì„ë² ë”© ì¬ì •ë ¬",
        formatter_class=CliHelpFormatter,
    )
    align_parser.add_argument(
        "--model-name",
        default="openai-community/gpt2",
        help="GPT-2 ëª¨ë¸ ì´ë¦„",
    )
    align_parser.add_argument(
        "--original-tokenizer-dir",
        type=Path,
        default=TOKENIZER_ORIGINAL_DIR,
        help="ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
    )
    align_parser.add_argument(
        "--remapped-tokenizer-dir",
        type=Path,
        default=TOKENIZER_REMAPPED_DIR,
        help="ì¬í• ë‹¹ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
    )
    align_parser.add_argument(
        "--remap-rules-path",
        type=Path,
        default=REMAP_RULES_PATH,
        help="ì¬í• ë‹¹ ê·œì¹™ íŒŒì¼ ê²½ë¡œ",
    )
    align_parser.add_argument(
        "--embeddings-output-dir",
        type=Path,
        default=EMBEDDINGS_ROOT,
        help="ì„ë² ë”© ì¶œë ¥ ë””ë ‰í† ë¦¬",
    )
    align_parser.add_argument(
        "--init-strategy",
        default="mean",
        choices=["mean", "random", "zeros"],
        help="ì‹ ê·œ í† í° ì„ë² ë”© ì´ˆê¸°í™” ì „ëµ",
    )

    subparsers.add_parser(
        "train",
        help="ë¯¸ì„¸ì¡°ì •",
        formatter_class=CliHelpFormatter,
    )

    args = parser.parse_args()

    # 2. ë¡œê¹… ì„¤ì •
    log_level = getattr(logging, args.log_level.upper(), logging.INFO)

    root_logger = logging.getLogger()

    root_logger.setLevel(log_level)

    # 2.2. Rich ì½˜ì†” í•¸ë“¤ëŸ¬ë¥¼ ë“±ë¡í•œë‹¤.
    console_handler = RichHandler(
        rich_tracebacks=True, markup=True, console=_CONSOLE, show_time=False
    )
    console_handler.setFormatter(logging.Formatter("%(message)s"))
    root_logger.addHandler(console_handler)

    # 2.3. íŒŒì¼ í•¸ë“¤ëŸ¬ë¥¼ ë“±ë¡í•˜ì—¬ ì „ì²´ ë¡œê·¸ë¥¼ ê¸°ë¡í•œë‹¤.
    log_dir = LOGS_DIR
    log_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"ivr_{timestamp}.log"

    file_handler = logging.FileHandler(log_file, encoding="utf-8")
    file_handler.setFormatter(
        logging.Formatter(
            "%(asctime)s [%(levelname)s] [%(filename)s:%(lineno)d] %(message)s"
        )
    )
    root_logger.addHandler(file_handler)

    # 2.4. ë¡œê·¸ íŒŒì¼ ìœ„ì¹˜ë¥¼ ì•ˆë‚´í•œë‹¤.
    root_logger.info("ğŸ“ ë¡œê·¸ íŒŒì¼: %s", log_file)

    logger = logging.getLogger(LOGGER_NAME)

    # 3. ì¸íŠ¸ë¡œ ë°°ë„ˆë¥¼ ì¶œë ¥í•œë‹¤.
    console = _CONSOLE
    title = "Tokenizer Model Migration + IVR"
    subtitle = f"ì‹¤í–‰ ëª…ë ¹ì–´: {args.command}"

    figlet = Figlet(font="standard")
    banner = figlet.renderText("GPT2-IVR").rstrip()
    console.print(
        Panel.fit(
            Text(banner, style="bold cyan"),
            title=title,
            subtitle=subtitle,
            border_style="cyan",
        )
    )

    # 4. ëª…ë ¹ì–´ë¥¼ í•´ì„í•˜ê³  ì‹¤í–‰í•œë‹¤.
    start = perf_counter()

    try:
        command_name = args.command
        if command_name == "init":
            command = InitCommand(
                model_name=args.model_name,
                tokenizer_dir=args.tokenizer_dir,
                force=args.force,
            )
        elif command_name == "analyze":
            command = AnalyzeCommand(
                input_dir=args.input_dir,
                output_sequences=args.output_sequences,
                output_frequency=args.output_frequency,
                tokenizer_dir=args.tokenizer_dir,
                workers=args.workers,
                chunk_size=args.chunk_size,
                max_texts=args.max_texts,
                text_key=args.text_key,
                encoding=args.encoding,
            )
        elif command_name == "distill-tokenizer":
            command = DistillCommand(
                original_tokenizer_dir=args.original_tokenizer_dir,
                distilled_tokenizer_dir=args.distilled_tokenizer_dir,
                corpus_dir=args.corpus_dir,
            )
        elif command_name == "select":
            command = SelectCommand(
                frequency_path=args.frequency_path,
                sequences_path=args.sequences_path,
                output_csv=args.output_csv,
                output_log=args.output_log,
                tokenizer_dir=args.tokenizer_dir,
                max_candidates=args.max_candidates,
                min_token_len=args.min_token_len,
            )
        elif command_name == "remap":
            command = RemapCommand(
                distilled_tokenizer_dir=args.distilled_tokenizer_dir,
                remapped_tokenizer_dir=args.remapped_tokenizer_dir,
                remap_rules_path=args.remap_rules_path,
                replacement_candidates_path=args.replacement_candidates_path,
            )
        elif command_name == "align":
            command = AlignCommand(
                model_name=args.model_name,
                original_tokenizer_dir=args.original_tokenizer_dir,
                remapped_tokenizer_dir=args.remapped_tokenizer_dir,
                remap_rules_path=args.remap_rules_path,
                embeddings_output_dir=args.embeddings_output_dir,
                init_strategy=args.init_strategy,
            )
        elif command_name == "train":
            command = TrainCommand()
        else:
            raise NotImplementedError(f"'{command_name}'ëŠ” ìœ íš¨í•˜ì§€ ì•Šì€ ì»¤ë§¨ë“œì…ë‹ˆë‹¤.")

        resolved_name = command.get_name()
        logger.info("ğŸš€ [%s] ë‹¨ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.", resolved_name)
        result = command.execute()
        elapsed = perf_counter() - start
        logger.info("âœ… [%s] ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (%.2fs)", resolved_name, elapsed)
        table = Table(
            title=f"âœ… {resolved_name} ë‹¨ê³„ ì™„ë£Œ",
            show_header=False,
            border_style="green",
        )
        table.add_column("í•­ëª©", style="bold")
        table.add_column("ê°’")
        table.add_row("ì‹¤í–‰ ì‹œê°„", f"{elapsed:.2f}ì´ˆ")

        # 4.1. ì‹¤í–‰ ê²°ê³¼ë¥¼ í…Œì´ë¸”ë¡œ ì •ë¦¬í•˜ì—¬ ì¶œë ¥í•œë‹¤.
        for key, value in result.items():
            _value_to_format = value
            if isinstance(_value_to_format, Path):
                formatted_value = str(_value_to_format)
            elif isinstance(_value_to_format, dict):
                formatted_value = f"dict({len(_value_to_format)})"
            elif isinstance(_value_to_format, list):
                formatted_value = f"list({len(_value_to_format)})"
            else:
                formatted_value = str(_value_to_format)

            if len(formatted_value) > 120:
                formatted_value = f"{formatted_value[:117]}..."
            table.add_row(str(key), formatted_value)

        _CONSOLE.print(table)
        return 0
    except NotImplementedError as e:
        elapsed = perf_counter() - start
        logger.error("[%s] ë¯¸êµ¬í˜„/ë¯¸ì§€ì› ì˜¤ë¥˜: %s", args.command, e)
        # 4.2. ì˜¤ë¥˜ ìƒí™©ì„ Rich íŒ¨ë„ë¡œ ì•ˆë‚´í•œë‹¤.
        _CONSOLE.print(
            Panel.fit(
                f"[bold red]{args.command} ë‹¨ê³„ ì‹¤í–‰ ì‹¤íŒ¨[/bold red]\n"
                f"{type(e).__name__}: {e}\n"
                f"[dim]ê²½ê³¼ ì‹œê°„: {elapsed:.2f}ì´ˆ[/dim]",
                title="ì‹¤í–‰ ì˜¤ë¥˜",
                border_style="red",
            )
        )
        return 1
    except (FileNotFoundError, ValueError) as e:
        elapsed = perf_counter() - start
        logger.error("[%s] ì…ë ¥ ê²€ì¦ ì˜¤ë¥˜: %s", args.command, e)
        # 4.3. ì˜¤ë¥˜ ìƒí™©ì„ Rich íŒ¨ë„ë¡œ ì•ˆë‚´í•œë‹¤.
        _CONSOLE.print(
            Panel.fit(
                f"[bold red]{args.command} ë‹¨ê³„ ì‹¤í–‰ ì‹¤íŒ¨[/bold red]\n"
                f"{type(e).__name__}: {e}\n"
                f"[dim]ê²½ê³¼ ì‹œê°„: {elapsed:.2f}ì´ˆ[/dim]",
                title="ì‹¤í–‰ ì˜¤ë¥˜",
                border_style="red",
            )
        )
        return 1
    except KeyboardInterrupt:
        logger.warning("â¹ï¸ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì‹¤í–‰ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 130
    except Exception as e:
        elapsed = perf_counter() - start
        logger.exception("[%s] ì‹¤í–‰ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", args.command)
        # 4.4. ì˜¤ë¥˜ ìƒí™©ì„ Rich íŒ¨ë„ë¡œ ì•ˆë‚´í•œë‹¤.
        _CONSOLE.print(
            Panel.fit(
                f"[bold red]{args.command} ë‹¨ê³„ ì‹¤í–‰ ì‹¤íŒ¨[/bold red]\n"
                f"{type(e).__name__}: {e}\n"
                f"[dim]ê²½ê³¼ ì‹œê°„: {elapsed:.2f}ì´ˆ[/dim]",
                title="ì‹¤í–‰ ì˜¤ë¥˜",
                border_style="red",
            )
        )
        return 1


if __name__ == "__main__":
    raise SystemExit(main())
