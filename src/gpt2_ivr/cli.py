from __future__ import annotations

import argparse
import logging
from collections.abc import Callable
from pathlib import Path
from time import perf_counter
from typing import Any

from pyfiglet import Figlet
from rich.panel import Panel
from rich.table import Table
from rich.text import Text

from gpt2_ivr.commands import (
    AlignCommand,
    AnalyzeCommand,
    Command,
    DistillCommand,
    InitCommand,
    RemapCommand,
    SelectCommand,
    TrainCommand,
)
from gpt2_ivr.constants import (
    BPE_TOKEN_ID_SEQUENCES_FILE,
    CORPORA_CLEANED_DIR,
    EMBEDDINGS_ROOT,
    REPLACEMENT_CANDIDATES_FILE,
    SELECTION_LOG_FILE,
    TOKENIZER_DISTILLED_UNIGRAM_DIR,
    TOKENIZER_ORIGINAL_DIR,
    TOKENIZER_REMAPPED_DIR,
    TOKEN_FREQUENCY_FILE,
)
from gpt2_ivr.utils.logging_config import get_console, get_logger, setup_logging

LOGGER_NAME = "gpt2_ivr.cli"
REMAP_RULES_PATH = Path("src/gpt2_ivr/tokenizer/remap_rules.yaml")


class CliHelpFormatter(
    argparse.ArgumentDefaultsHelpFormatter, argparse.RawTextHelpFormatter
):
    """CLI ë„ì›€ë§ í¬ë§·í„°."""


class CliArgumentParser(argparse.ArgumentParser):
    """ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ Rich ìŠ¤íƒ€ì¼ë¡œ ì¶œë ¥í•˜ëŠ” argparse íŒŒì„œ."""

    def error(self, message: str) -> None:
        console = get_console()
        console.print(
            Panel.fit(
                f"[bold red]ì¸ì ì˜¤ë¥˜[/bold red]\n{message}\n\n"
                "[dim]ë„ì›€ë§: uv run ivr --help[/dim]",
                title="CLI ì…ë ¥ ì˜¤ë¥˜",
                border_style="red",
            )
        )
        raise SystemExit(2)


def non_negative_int(value: str) -> int:
    """0 ì´ìƒì˜ ì •ìˆ˜ ì¸ìë¥¼ íŒŒì‹±í•œë‹¤."""
    try:
        parsed = int(value)
    except ValueError as e:
        raise argparse.ArgumentTypeError("ì •ìˆ˜ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.") from e

    if parsed < 0:
        raise argparse.ArgumentTypeError("0 ì´ìƒì˜ ì •ìˆ˜ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.")
    return parsed


def positive_int(value: str) -> int:
    """1 ì´ìƒì˜ ì •ìˆ˜ ì¸ìë¥¼ íŒŒì‹±í•œë‹¤."""
    try:
        parsed = int(value)
    except ValueError as e:
        raise argparse.ArgumentTypeError("ì •ìˆ˜ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.") from e

    if parsed <= 0:
        raise argparse.ArgumentTypeError("1 ì´ìƒì˜ ì •ìˆ˜ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.")
    return parsed


def build_banner(text: str, font: str = "standard") -> str:
    """ë°°ë„ˆ ë¬¸ìì—´ì„ ìƒì„±í•œë‹¤."""
    figlet = Figlet(font=font)
    return figlet.renderText(text)


def build_parser() -> argparse.ArgumentParser:
    """IVR CLI íŒŒì„œë¥¼ ìƒì„±í•œë‹¤."""
    parser = CliArgumentParser(
        prog="ivr",
        description="Tokenizer Model Migration + IVR íŒŒì´í”„ë¼ì¸ CLI",
        formatter_class=CliHelpFormatter,
    )
    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="ì½˜ì†” ë¡œê¹… ë ˆë²¨",
    )
    parser.add_argument(
        "--no-log-file",
        action="store_true",
        default=False,
        help="artifacts/logs íŒŒì¼ ë¡œê·¸ ê¸°ë¡ ë¹„í™œì„±í™”",
    )
    parser.add_argument(
        "--no-banner",
        action="store_true",
        default=False,
        help="ì‹œì‘ ASCII ë°°ë„ˆ ì¶œë ¥ ë¹„í™œì„±í™”",
    )

    subparsers = parser.add_subparsers(dest="command", required=True, metavar="command")

    # init ì„œë¸Œì»¤ë§¨ë“œ
    init_parser = subparsers.add_parser(
        "init",
        help="ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”",
        formatter_class=CliHelpFormatter,
    )
    init_parser.add_argument(
        "--model-name",
        default="openai-community/gpt2",
        help="Hugging Face Hub ëª¨ë¸ ì´ë¦„",
    )
    init_parser.add_argument(
        "--tokenizer-dir",
        type=Path,
        default=TOKENIZER_ORIGINAL_DIR,
        help="í† í¬ë‚˜ì´ì € ì €ì¥ ë””ë ‰í† ë¦¬",
    )
    init_parser.add_argument(
        "--force",
        action="store_true",
        default=False,
        help="ê¸°ì¡´ íŒŒì¼ì´ ìˆì–´ë„ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ",
    )

    # analyze ì„œë¸Œì»¤ë§¨ë“œ
    analyze_parser = subparsers.add_parser(
        "analyze",
        help="BPE í† í° ì‹œí€€ìŠ¤ ë¶„ì„",
        formatter_class=CliHelpFormatter,
    )
    analyze_parser.add_argument(
        "--input-dir",
        type=Path,
        default=CORPORA_CLEANED_DIR,
        help="ì½”í¼ìŠ¤ ì…ë ¥ ë””ë ‰í† ë¦¬",
    )
    analyze_parser.add_argument(
        "--output-sequences",
        type=Path,
        default=BPE_TOKEN_ID_SEQUENCES_FILE,
        help="BPE í† í° ì‹œí€€ìŠ¤ ì¶œë ¥ ê²½ë¡œ",
    )
    analyze_parser.add_argument(
        "--output-frequency",
        type=Path,
        default=TOKEN_FREQUENCY_FILE,
        help="í† í° ë¹ˆë„ parquet ì¶œë ¥ ê²½ë¡œ",
    )
    analyze_parser.add_argument(
        "--tokenizer-dir",
        type=Path,
        default=TOKENIZER_ORIGINAL_DIR,
        help="ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
    )
    analyze_parser.add_argument(
        "--workers",
        type=non_negative_int,
        default=0,
        help="ìŠ¤ë ˆë“œ ì›Œì»¤ ìˆ˜ (0ì´ë©´ CPU - 1)",
    )
    analyze_parser.add_argument(
        "--chunk-size",
        type=non_negative_int,
        default=0,
        help="ìŠ¤ë ˆë“œ ì²­í¬ í¬ê¸°(0ì´ë©´ ìë™ ì„¤ì •)",
    )
    analyze_parser.add_argument(
        "--max-texts",
        type=non_negative_int,
        default=0,
        help="ì²˜ë¦¬í•  ìµœëŒ€ í…ìŠ¤íŠ¸ ìˆ˜ (0ì´ë©´ ì „ì²´)",
    )
    analyze_parser.add_argument(
        "--text-key",
        default="text",
        help="json/jsonl í…ìŠ¤íŠ¸ í‚¤",
    )
    analyze_parser.add_argument(
        "--encoding",
        default="utf-8",
        help="ì…ë ¥ íŒŒì¼ ì¸ì½”ë”©",
    )

    # distill-tokenizer ì„œë¸Œì»¤ë§¨ë“œ
    distill_parser = subparsers.add_parser(
        "distill-tokenizer",
        help="BPE -> Unigram distillation",
        formatter_class=CliHelpFormatter,
    )
    distill_parser.add_argument(
        "--original-tokenizer-dir",
        type=Path,
        default=TOKENIZER_ORIGINAL_DIR,
        help="ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
    )
    distill_parser.add_argument(
        "--distilled-tokenizer-dir",
        type=Path,
        default=TOKENIZER_DISTILLED_UNIGRAM_DIR,
        help="ì¦ë¥˜ëœ í† í¬ë‚˜ì´ì € ì €ì¥ ë””ë ‰í† ë¦¬",
    )
    distill_parser.add_argument(
        "--corpus-dir",
        type=Path,
        default=CORPORA_CLEANED_DIR,
        help="í•™ìŠµ ì½”í¼ìŠ¤ ë””ë ‰í† ë¦¬",
    )

    # select ì„œë¸Œì»¤ë§¨ë“œ
    select_parser = subparsers.add_parser(
        "select",
        help="IVR ëŒ€ìƒ í† í° ì„ ì •",
        formatter_class=CliHelpFormatter,
    )
    select_parser.add_argument(
        "--frequency-path",
        type=Path,
        default=TOKEN_FREQUENCY_FILE,
        help="í† í° ë¹ˆë„ parquet íŒŒì¼ ê²½ë¡œ",
    )
    select_parser.add_argument(
        "--sequences-path",
        type=Path,
        default=BPE_TOKEN_ID_SEQUENCES_FILE,
        help="BPE í† í° ì‹œí€€ìŠ¤ íŒŒì¼ ê²½ë¡œ",
    )
    select_parser.add_argument(
        "--output-csv",
        type=Path,
        default=REPLACEMENT_CANDIDATES_FILE,
        help="êµì²´ í›„ë³´ CSV ì €ì¥ ê²½ë¡œ",
    )
    select_parser.add_argument(
        "--output-log",
        type=Path,
        default=SELECTION_LOG_FILE,
        help="ì„ ì • ë¡œê·¸ ì €ì¥ ê²½ë¡œ",
    )
    select_parser.add_argument(
        "--tokenizer-dir",
        type=Path,
        default=TOKENIZER_ORIGINAL_DIR,
        help="ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
    )
    select_parser.add_argument(
        "--max-candidates",
        type=positive_int,
        default=1000,
        help="ìµœëŒ€ í›„ë³´ ê°œìˆ˜",
    )
    select_parser.add_argument(
        "--min-token-len",
        type=positive_int,
        default=2,
        help="ë³´í˜¸ í† í° ìµœì†Œ ê¸¸ì´",
    )

    # remap ì„œë¸Œì»¤ë§¨ë“œ
    remap_parser = subparsers.add_parser(
        "remap",
        help="í† í° ì¬í• ë‹¹ ê·œì¹™ ì ìš©",
        formatter_class=CliHelpFormatter,
    )
    remap_parser.add_argument(
        "--distilled-tokenizer-dir",
        type=Path,
        default=TOKENIZER_DISTILLED_UNIGRAM_DIR,
        help="ì¦ë¥˜ëœ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
    )
    remap_parser.add_argument(
        "--remapped-tokenizer-dir",
        type=Path,
        default=TOKENIZER_REMAPPED_DIR,
        help="ì¬í• ë‹¹ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
    )
    remap_parser.add_argument(
        "--remap-rules-path",
        type=Path,
        default=REMAP_RULES_PATH,
        help="ì¬í• ë‹¹ ê·œì¹™ íŒŒì¼ ê²½ë¡œ",
    )
    remap_parser.add_argument(
        "--replacement-candidates-path",
        type=Path,
        default=REPLACEMENT_CANDIDATES_FILE,
        help="êµì²´ í›„ë³´ CSV ê²½ë¡œ",
    )

    # align ì„œë¸Œì»¤ë§¨ë“œ
    align_parser = subparsers.add_parser(
        "align",
        help="ì„ë² ë”© ì¬ì •ë ¬",
        formatter_class=CliHelpFormatter,
    )
    align_parser.add_argument(
        "--model-name",
        default="openai-community/gpt2",
        help="GPT-2 ëª¨ë¸ ì´ë¦„",
    )
    align_parser.add_argument(
        "--original-tokenizer-dir",
        type=Path,
        default=TOKENIZER_ORIGINAL_DIR,
        help="ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
    )
    align_parser.add_argument(
        "--remapped-tokenizer-dir",
        type=Path,
        default=TOKENIZER_REMAPPED_DIR,
        help="ì¬í• ë‹¹ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
    )
    align_parser.add_argument(
        "--remap-rules-path",
        type=Path,
        default=REMAP_RULES_PATH,
        help="ì¬í• ë‹¹ ê·œì¹™ íŒŒì¼ ê²½ë¡œ",
    )
    align_parser.add_argument(
        "--embeddings-output-dir",
        type=Path,
        default=EMBEDDINGS_ROOT,
        help="ì„ë² ë”© ì¶œë ¥ ë””ë ‰í† ë¦¬",
    )
    align_parser.add_argument(
        "--init-strategy",
        default="mean",
        choices=["mean", "random", "zeros"],
        help="ì‹ ê·œ í† í° ì„ë² ë”© ì´ˆê¸°í™” ì „ëµ",
    )

    # train ì„œë¸Œì»¤ë§¨ë“œ (í˜„ì¬ stub)
    subparsers.add_parser(
        "train",
        help="ë¯¸ì„¸ì¡°ì •",
        formatter_class=CliHelpFormatter,
    )

    return parser


def format_result_value(value: Any) -> str:
    """ê²°ê³¼ ê°’ ì¶œë ¥ ë¬¸ìì—´ì„ ì •ê·œí™”í•œë‹¤."""
    if isinstance(value, Path):
        return str(value)
    if isinstance(value, dict):
        return f"dict({len(value)})"
    if isinstance(value, list):
        return f"list({len(value)})"

    rendered = str(value)
    if len(rendered) > 120:
        return f"{rendered[:117]}..."
    return rendered


def render_intro(command_name: str, show_banner: bool) -> None:
    """ì‹¤í–‰ ì‹œì‘ ì •ë³´ë¥¼ ì¶œë ¥í•œë‹¤."""
    console = get_console()
    title = "Tokenizer Model Migration + IVR"
    subtitle = f"ì‹¤í–‰ ëª…ë ¹ì–´: {command_name}"

    if show_banner:
        banner = build_banner("GPT2-IVR").rstrip()
        console.print(
            Panel.fit(
                Text(banner, style="bold cyan"),
                title=title,
                subtitle=subtitle,
                border_style="cyan",
            )
        )
        return

    console.print(
        Panel.fit(
            f"[bold cyan]{title}[/bold cyan]\n{subtitle}",
            border_style="cyan",
        )
    )


def render_result_summary(
    command_name: str,
    result: dict[str, Any],
    elapsed_seconds: float,
) -> None:
    """ì»¤ë§¨ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ í…Œì´ë¸”ë¡œ ì¶œë ¥í•œë‹¤."""
    table = Table(
        title=f"âœ… {command_name} ë‹¨ê³„ ì™„ë£Œ",
        show_header=False,
        border_style="green",
    )
    table.add_column("í•­ëª©", style="bold")
    table.add_column("ê°’")
    table.add_row("ì‹¤í–‰ ì‹œê°„", f"{elapsed_seconds:.2f}ì´ˆ")

    for key, value in result.items():
        table.add_row(str(key), format_result_value(value))

    get_console().print(table)


def render_error_panel(
    command_name: str,
    error: Exception,
    elapsed_seconds: float,
) -> None:
    """ì‹¤í–‰ ì˜¤ë¥˜ë¥¼ íŒ¨ë„ë¡œ ì¶œë ¥í•œë‹¤."""
    get_console().print(
        Panel.fit(
            f"[bold red]{command_name} ë‹¨ê³„ ì‹¤í–‰ ì‹¤íŒ¨[/bold red]\n"
            f"{type(error).__name__}: {error}\n"
            f"[dim]ê²½ê³¼ ì‹œê°„: {elapsed_seconds:.2f}ì´ˆ[/dim]",
            title="ì‹¤í–‰ ì˜¤ë¥˜",
            border_style="red",
        )
    )


def _create_init_command(args: argparse.Namespace) -> InitCommand:
    """init ì»¤ë§¨ë“œë¥¼ ìƒì„±í•œë‹¤."""
    return InitCommand(
        model_name=args.model_name,
        tokenizer_dir=args.tokenizer_dir,
        force=args.force,
    )


def _create_analyze_command(args: argparse.Namespace) -> AnalyzeCommand:
    """analyze ì»¤ë§¨ë“œë¥¼ ìƒì„±í•œë‹¤."""
    return AnalyzeCommand(
        input_dir=args.input_dir,
        output_sequences=args.output_sequences,
        output_frequency=args.output_frequency,
        tokenizer_dir=args.tokenizer_dir,
        workers=args.workers,
        chunk_size=args.chunk_size,
        max_texts=args.max_texts,
        text_key=args.text_key,
        encoding=args.encoding,
    )


def _create_distill_command(args: argparse.Namespace) -> DistillCommand:
    """distill-tokenizer ì»¤ë§¨ë“œë¥¼ ìƒì„±í•œë‹¤."""
    return DistillCommand(
        original_tokenizer_dir=args.original_tokenizer_dir,
        distilled_tokenizer_dir=args.distilled_tokenizer_dir,
        corpus_dir=args.corpus_dir,
    )


def _create_select_command(args: argparse.Namespace) -> SelectCommand:
    """select ì»¤ë§¨ë“œë¥¼ ìƒì„±í•œë‹¤."""
    return SelectCommand(
        frequency_path=args.frequency_path,
        sequences_path=args.sequences_path,
        output_csv=args.output_csv,
        output_log=args.output_log,
        tokenizer_dir=args.tokenizer_dir,
        max_candidates=args.max_candidates,
        min_token_len=args.min_token_len,
    )


def _create_remap_command(args: argparse.Namespace) -> RemapCommand:
    """remap ì»¤ë§¨ë“œë¥¼ ìƒì„±í•œë‹¤."""
    return RemapCommand(
        distilled_tokenizer_dir=args.distilled_tokenizer_dir,
        remapped_tokenizer_dir=args.remapped_tokenizer_dir,
        remap_rules_path=args.remap_rules_path,
        replacement_candidates_path=args.replacement_candidates_path,
    )


def _create_align_command(args: argparse.Namespace) -> AlignCommand:
    """align ì»¤ë§¨ë“œë¥¼ ìƒì„±í•œë‹¤."""
    return AlignCommand(
        model_name=args.model_name,
        original_tokenizer_dir=args.original_tokenizer_dir,
        remapped_tokenizer_dir=args.remapped_tokenizer_dir,
        remap_rules_path=args.remap_rules_path,
        embeddings_output_dir=args.embeddings_output_dir,
        init_strategy=args.init_strategy,
    )


def _create_train_command(args: argparse.Namespace) -> TrainCommand:
    """train ì»¤ë§¨ë“œë¥¼ ìƒì„±í•œë‹¤. (í˜„ì¬ stub)"""
    # TODO: train ì»¤ë§¨ë“œì— CLI ì˜µì…˜ì´ ì¶”ê°€ë˜ë©´ argsë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„° ì „ë‹¬
    return TrainCommand()


# ì„œë¸Œì»¤ë§¨ë“œ íŒ©í† ë¦¬ ë§¤í•‘
CommandFactory = Callable[[argparse.Namespace], Command]

COMMAND_FACTORY_MAP: dict[str, CommandFactory] = {
    "init": _create_init_command,
    "analyze": _create_analyze_command,
    "distill-tokenizer": _create_distill_command,
    "select": _create_select_command,
    "remap": _create_remap_command,
    "align": _create_align_command,
    "train": _create_train_command,
}


def create_command(command_name: str, args: argparse.Namespace) -> Command:
    """ì»¤ë§¨ë“œ ì´ë¦„ì— í•´ë‹¹í•˜ëŠ” Command ê°ì²´ë¥¼ ìƒì„±í•œë‹¤."""
    factory = COMMAND_FACTORY_MAP.get(command_name)
    if factory is not None:
        return factory(args)

    raise NotImplementedError(f"'{command_name}'ëŠ” ìœ íš¨í•˜ì§€ ì•Šì€ ì»¤ë§¨ë“œì…ë‹ˆë‹¤.")


def dispatch(
    command_name: str, args: argparse.Namespace, logger: logging.Logger
) -> int:
    """ì„œë¸Œì»¤ë§¨ë“œë¥¼ ì‹¤í–‰í•œë‹¤."""
    start = perf_counter()

    try:
        command = create_command(command_name, args)
        resolved_name = command.get_name()
        logger.info("ğŸš€ [%s] ë‹¨ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.", resolved_name)
        result = command.execute()
        elapsed = perf_counter() - start
        logger.info("âœ… [%s] ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (%.2fs)", resolved_name, elapsed)
        render_result_summary(resolved_name, result, elapsed)
        return 0
    except NotImplementedError as e:
        elapsed = perf_counter() - start
        logger.error("[%s] ë¯¸êµ¬í˜„/ë¯¸ì§€ì› ì˜¤ë¥˜: %s", command_name, e)
        render_error_panel(command_name, e, elapsed)
        return 1
    except (FileNotFoundError, ValueError) as e:
        elapsed = perf_counter() - start
        logger.error("[%s] ì…ë ¥ ê²€ì¦ ì˜¤ë¥˜: %s", command_name, e)
        render_error_panel(command_name, e, elapsed)
        return 1
    except KeyboardInterrupt:
        logger.warning("â¹ï¸ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì‹¤í–‰ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 130
    except Exception as e:
        elapsed = perf_counter() - start
        logger.exception("[%s] ì‹¤í–‰ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", command_name)
        render_error_panel(command_name, e, elapsed)
        return 1


def main() -> int:
    """CLI ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸."""
    parser = build_parser()
    args = parser.parse_args()

    log_level = getattr(logging, args.log_level.upper(), logging.INFO)
    setup_logging(level=log_level, log_to_file=not args.no_log_file)
    logger = get_logger(LOGGER_NAME)

    render_intro(args.command, show_banner=not args.no_banner)
    logger.info("Tokenizer Model Migration + IVR íŒŒì´í”„ë¼ì¸")
    logger.info("BPE -> Unigram í† í¬ë‚˜ì´ì € êµì²´ í›„ IVRë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
    return dispatch(args.command, args, logger)


if __name__ == "__main__":
    raise SystemExit(main())
