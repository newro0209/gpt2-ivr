"""GPT2-IVR CLI ì§„ì…ì  ëª¨ë“ˆ.

Tokenizer Model Migration + IVR íŒŒì´í”„ë¼ì¸ì˜ ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•œë‹¤.
Rich ê¸°ë°˜ ì½˜ì†” ì¶œë ¥ ë° ë¡œê¹…ì„ ì§€ì›í•œë‹¤.
"""

from __future__ import annotations

import argparse
import logging
from collections.abc import Callable
from datetime import datetime
from pathlib import Path
from time import perf_counter
from typing import Any

from pyfiglet import Figlet
from rich.console import Console
from rich.logging import RichHandler
from rich.panel import Panel
from rich.table import Table
from rich.text import Text

from gpt2_ivr.commands import (
    AlignCommand,
    AnalyzeCommand,
    Command,
    DistillCommand,
    InitCommand,
    RemapCommand,
    SelectCommand,
    TrainCommand,
)
from gpt2_ivr.constants import (
    BPE_TOKEN_ID_SEQUENCES_FILE,
    CORPORA_CLEANED_DIR,
    CORPORA_RAW_DIR,
    EMBEDDINGS_ROOT,
    LOGS_DIR,
    REPLACEMENT_CANDIDATES_FILE,
    SELECTION_LOG_FILE,
    TOKENIZER_DISTILLED_UNIGRAM_DIR,
    TOKENIZER_ORIGINAL_DIR,
    TOKENIZER_REMAPPED_DIR,
    TOKEN_FREQUENCY_FILE,
)

LOGGER_NAME = "gpt2_ivr.cli"
REMAP_RULES_PATH = Path("src/gpt2_ivr/tokenizer/remap_rules.yaml")
CONSOLE = Console(stderr=False)


class CliHelpFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawTextHelpFormatter):
    """CLI ë„ì›€ë§ í¬ë§·í„°.

    ArgumentDefaultsHelpFormatterì™€ RawTextHelpFormatterë¥¼ ê²°í•©í•˜ì—¬
    ê¸°ë³¸ê°’ í‘œì‹œì™€ ì›ì‹œ í…ìŠ¤íŠ¸ í¬ë§·ì„ ë™ì‹œì— ì§€ì›í•œë‹¤.
    """


class CliArgumentParser(argparse.ArgumentParser):
    """ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ Rich ìŠ¤íƒ€ì¼ë¡œ ì¶œë ¥í•˜ëŠ” argparse íŒŒì„œ.

    ì¸ì íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ ì‹œ Rich Panelë¡œ ì˜¤ë¥˜ë¥¼ í‘œì‹œí•˜ì—¬
    ì‚¬ìš©ì ê²½í—˜ì„ ê°œì„ í•œë‹¤.
    """

    def error(self, message: str) -> None:
        """ì¸ì íŒŒì‹± ì˜¤ë¥˜ë¥¼ Rich íŒ¨ë„ë¡œ ì¶œë ¥í•œë‹¤.

        Args:
            message: ì˜¤ë¥˜ ë©”ì‹œì§€
        """
        CONSOLE.print(
            Panel.fit(
                f"[bold red]ì¸ì ì˜¤ë¥˜[/bold red]\n{message}\n\n" f"[dim]ë„ì›€ë§: uv run ivr --help[/dim]",
                title="CLI ì…ë ¥ ì˜¤ë¥˜",
                border_style="red",
            )
        )
        raise SystemExit(2)


def validate_int(value: str, minimum: int = 0) -> int:
    """ì •ìˆ˜ ê°’ì„ ê²€ì¦í•œë‹¤.

    Args:
        value: íŒŒì‹±í•  ë¬¸ìì—´ ê°’
        minimum: í—ˆìš©ë˜ëŠ” ìµœì†Œê°’

    Returns:
        íŒŒì‹±ëœ ì •ìˆ˜ ê°’

    Raises:
        argparse.ArgumentTypeError: ê°’ì´ ì •ìˆ˜ê°€ ì•„ë‹ˆê±°ë‚˜ ìµœì†Œê°’ë³´ë‹¤ ì‘ì€ ê²½ìš°
    """
    try:
        parsed = int(value)
    except ValueError as e:
        raise argparse.ArgumentTypeError("ì •ìˆ˜ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.") from e

    if parsed < minimum:
        raise argparse.ArgumentTypeError(f"{minimum} ì´ìƒì˜ ì •ìˆ˜ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.")
    return parsed


def non_negative_int(value: str) -> int:
    """0 ì´ìƒì˜ ì •ìˆ˜ ì¸ìë¥¼ íŒŒì‹±í•œë‹¤."""
    return validate_int(value, minimum=0)


def positive_int(value: str) -> int:
    """1 ì´ìƒì˜ ì •ìˆ˜ ì¸ìë¥¼ íŒŒì‹±í•œë‹¤."""
    return validate_int(value, minimum=1)


def add_common_args(parser: argparse.ArgumentParser, *args: str) -> None:
    """ê³µí†µ ì¸ìë¥¼ íŒŒì„œì— ì¶”ê°€í•œë‹¤.

    Args:
        parser: ì¸ìë¥¼ ì¶”ê°€í•  íŒŒì„œ
        *args: ì¶”ê°€í•  ì¸ì ì´ë¦„ë“¤
    """
    arg_configs = {
        "tokenizer-dir": (
            "--tokenizer-dir",
            {"type": Path, "default": TOKENIZER_ORIGINAL_DIR, "help": "ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬"},
        ),
        "original-tokenizer-dir": (
            "--original-tokenizer-dir",
            {"type": Path, "default": TOKENIZER_ORIGINAL_DIR, "help": "ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬"},
        ),
        "distilled-tokenizer-dir": (
            "--distilled-tokenizer-dir",
            {"type": Path, "default": TOKENIZER_DISTILLED_UNIGRAM_DIR, "help": "ì¦ë¥˜ëœ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬"},
        ),
        "remapped-tokenizer-dir": (
            "--remapped-tokenizer-dir",
            {"type": Path, "default": TOKENIZER_REMAPPED_DIR, "help": "ì¬í• ë‹¹ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬"},
        ),
        "remap-rules-path": (
            "--remap-rules-path",
            {"type": Path, "default": REMAP_RULES_PATH, "help": "ì¬í• ë‹¹ ê·œì¹™ íŒŒì¼ ê²½ë¡œ"},
        ),
    }

    for arg in args:
        if arg in arg_configs:
            flag, kwargs = arg_configs[arg]
            parser.add_argument(flag, **kwargs)


def setup_subparsers(subparsers: argparse._SubParsersAction) -> None:
    """ëª¨ë“  ì„œë¸Œì»¤ë§¨ë“œ íŒŒì„œë¥¼ ì„¤ì •í•œë‹¤.

    Args:
        subparsers: ì„œë¸ŒíŒŒì„œ ì•¡ì…˜ ê°ì²´
    """
    # init
    init_parser = subparsers.add_parser("init", help="ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”", formatter_class=CliHelpFormatter)
    init_parser.add_argument("--model-name", default="openai-community/gpt2", help="Hugging Face Hub ëª¨ë¸ ì´ë¦„")
    init_parser.add_argument(
        "--tokenizer-dir", type=Path, default=TOKENIZER_ORIGINAL_DIR, help="í† í¬ë‚˜ì´ì € ì €ì¥ ë””ë ‰í† ë¦¬"
    )
    init_parser.add_argument("--force", action="store_true", help="ê¸°ì¡´ íŒŒì¼ì´ ìˆì–´ë„ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ")
    init_parser.add_argument(
        "--raw-corpora-dir",
        type=Path,
        default=CORPORA_RAW_DIR,
        help="raw ì½”í¼ìŠ¤ê°€ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬",
    )
    init_parser.add_argument(
        "--cleaned-corpora-dir",
        type=Path,
        default=CORPORA_CLEANED_DIR,
        help="ì •ì œëœ ì½”í¼ìŠ¤ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬",
    )
    init_parser.add_argument("--text-key", default="text", help="JSON/JSONL íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì½ì–´ì˜¬ í‚¤")
    init_parser.add_argument("--encoding", default="utf-8", help="ì…ë ¥ ì½”í¼ìŠ¤ íŒŒì¼ ì¸ì½”ë”©")
    init_parser.add_argument(
        "--normalize-force",
        action="store_true",
        help="ì´ë¯¸ ì •ì œë³¸ì´ ìˆì–´ë„ raw íŒŒì¼ì„ ë‹¤ì‹œ ë³€í™˜í•©ë‹ˆë‹¤",
    )

    # analyze
    analyze_parser = subparsers.add_parser("analyze", help="BPE í† í° ì‹œí€€ìŠ¤ ë¶„ì„", formatter_class=CliHelpFormatter)
    analyze_parser.add_argument("--input-dir", type=Path, default=CORPORA_CLEANED_DIR, help="ì½”í¼ìŠ¤ ì…ë ¥ ë””ë ‰í† ë¦¬")
    analyze_parser.add_argument(
        "--output-sequences", type=Path, default=BPE_TOKEN_ID_SEQUENCES_FILE, help="BPE í† í° ì‹œí€€ìŠ¤ ì¶œë ¥ ê²½ë¡œ"
    )
    analyze_parser.add_argument(
        "--output-frequency", type=Path, default=TOKEN_FREQUENCY_FILE, help="í† í° ë¹ˆë„ parquet ì¶œë ¥ ê²½ë¡œ"
    )
    add_common_args(analyze_parser, "tokenizer-dir")
    analyze_parser.add_argument("--workers", type=non_negative_int, default=0, help="ìŠ¤ë ˆë“œ ì›Œì»¤ ìˆ˜ (0ì´ë©´ CPU - 1)")
    analyze_parser.add_argument(
        "--chunk-size", type=non_negative_int, default=0, help="ìŠ¤ë ˆë“œ ì²­í¬ í¬ê¸°(0ì´ë©´ ìë™ ì„¤ì •)"
    )
    analyze_parser.add_argument(
        "--max-texts", type=non_negative_int, default=0, help="ì²˜ë¦¬í•  ìµœëŒ€ í…ìŠ¤íŠ¸ ìˆ˜ (0ì´ë©´ ì „ì²´)"
    )
    analyze_parser.add_argument("--encoding", default="utf-8", help="ì…ë ¥ íŒŒì¼ ì¸ì½”ë”©")

    # distill-tokenizer
    distill_parser = subparsers.add_parser(
        "distill-tokenizer", help="BPE -> Unigram distillation", formatter_class=CliHelpFormatter
    )
    add_common_args(distill_parser, "original-tokenizer-dir", "distilled-tokenizer-dir")
    distill_parser.add_argument("--corpus-dir", type=Path, default=CORPORA_CLEANED_DIR, help="í•™ìŠµ ì½”í¼ìŠ¤ ë””ë ‰í† ë¦¬")

    # select
    select_parser = subparsers.add_parser("select", help="IVR ëŒ€ìƒ í† í° ì„ ì •", formatter_class=CliHelpFormatter)
    select_parser.add_argument(
        "--frequency-path", type=Path, default=TOKEN_FREQUENCY_FILE, help="í† í° ë¹ˆë„ parquet íŒŒì¼ ê²½ë¡œ"
    )
    select_parser.add_argument(
        "--sequences-path", type=Path, default=BPE_TOKEN_ID_SEQUENCES_FILE, help="BPE í† í° ì‹œí€€ìŠ¤ íŒŒì¼ ê²½ë¡œ"
    )
    select_parser.add_argument(
        "--output-csv", type=Path, default=REPLACEMENT_CANDIDATES_FILE, help="êµì²´ í›„ë³´ CSV ì €ì¥ ê²½ë¡œ"
    )
    select_parser.add_argument("--output-log", type=Path, default=SELECTION_LOG_FILE, help="ì„ ì • ë¡œê·¸ ì €ì¥ ê²½ë¡œ")
    add_common_args(select_parser, "tokenizer-dir")
    select_parser.add_argument("--max-candidates", type=positive_int, default=1000, help="ìµœëŒ€ í›„ë³´ ê°œìˆ˜")
    select_parser.add_argument("--min-token-len", type=positive_int, default=2, help="ë³´í˜¸ í† í° ìµœì†Œ ê¸¸ì´")

    # remap
    remap_parser = subparsers.add_parser("remap", help="í† í° ì¬í• ë‹¹ ê·œì¹™ ì ìš©", formatter_class=CliHelpFormatter)
    add_common_args(remap_parser, "distilled-tokenizer-dir", "remapped-tokenizer-dir", "remap-rules-path")
    remap_parser.add_argument(
        "--replacement-candidates-path", type=Path, default=REPLACEMENT_CANDIDATES_FILE, help="êµì²´ í›„ë³´ CSV ê²½ë¡œ"
    )

    # align
    align_parser = subparsers.add_parser("align", help="ì„ë² ë”© ì¬ì •ë ¬", formatter_class=CliHelpFormatter)
    align_parser.add_argument("--model-name", default="openai-community/gpt2", help="GPT-2 ëª¨ë¸ ì´ë¦„")
    add_common_args(align_parser, "original-tokenizer-dir", "remapped-tokenizer-dir", "remap-rules-path")
    align_parser.add_argument(
        "--embeddings-output-dir", type=Path, default=EMBEDDINGS_ROOT, help="ì„ë² ë”© ì¶œë ¥ ë””ë ‰í† ë¦¬"
    )
    align_parser.add_argument(
        "--init-strategy", default="mean", choices=["mean", "random", "zeros"], help="ì‹ ê·œ í† í° ì„ë² ë”© ì´ˆê¸°í™” ì „ëµ"
    )

    # train
    subparsers.add_parser("train", help="ë¯¸ì„¸ì¡°ì •", formatter_class=CliHelpFormatter)


def setup_parser() -> argparse.ArgumentParser:
    """CLI íŒŒì„œë¥¼ ì„¤ì •í•œë‹¤.

    Returns:
        ì„¤ì •ëœ ArgumentParser ê°ì²´
    """
    parser = CliArgumentParser(
        prog="ivr",
        description="Tokenizer Model Migration + IVR íŒŒì´í”„ë¼ì¸ CLI",
        formatter_class=CliHelpFormatter,
    )
    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="ì½˜ì†” ë¡œê¹… ë ˆë²¨",
    )

    subparsers = parser.add_subparsers(dest="command", required=True, metavar="command")
    setup_subparsers(subparsers)

    return parser


def setup_logging(log_level: str) -> logging.Logger:
    """ë¡œê¹…ì„ ì„¤ì •í•œë‹¤.

    Args:
        log_level: ë¡œê¹… ë ˆë²¨ ë¬¸ìì—´

    Returns:
        ì„¤ì •ëœ ë¡œê±° ê°ì²´
    """
    level = getattr(logging, log_level.upper(), logging.INFO)
    root_logger = logging.getLogger()
    root_logger.setLevel(level)

    # Rich ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = RichHandler(rich_tracebacks=True, markup=True, console=CONSOLE, show_time=False)
    console_handler.setFormatter(logging.Formatter("%(message)s"))
    root_logger.addHandler(console_handler)

    # íŒŒì¼ í•¸ë“¤ëŸ¬
    LOGS_DIR.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = LOGS_DIR / f"ivr_{timestamp}.log"

    file_handler = logging.FileHandler(log_file, encoding="utf-8")
    file_handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] [%(filename)s:%(lineno)d] %(message)s"))
    root_logger.addHandler(file_handler)

    root_logger.info("ğŸ“ ë¡œê·¸ íŒŒì¼: %s", log_file)
    return logging.getLogger(LOGGER_NAME)


def print_banner() -> None:
    """ì‹œì‘ ë°°ë„ˆë¥¼ ì¶œë ¥í•œë‹¤."""
    figlet = Figlet(font="standard")
    banner = figlet.renderText("IVR").rstrip()
    CONSOLE.print(Text(banner, style="bold cyan"))


def create_command(args: argparse.Namespace) -> Command:
    """ì»¤ë§¨ë“œ ê°ì²´ë¥¼ ìƒì„±í•œë‹¤.

    Args:
        args: íŒŒì‹±ëœ ì»¤ë§¨ë“œë¼ì¸ ì¸ì

    Returns:
        ìƒì„±ëœ Command ê°ì²´

    Raises:
        NotImplementedError: ìœ íš¨í•˜ì§€ ì•Šì€ ì»¤ë§¨ë“œì¸ ê²½ìš°
    """
    command_map: dict[str, Callable[[argparse.Namespace], Command]] = {
        "init": lambda a: InitCommand(
            a.model_name,
            a.tokenizer_dir,
            a.force,
            a.raw_corpora_dir,
            a.cleaned_corpora_dir,
            a.text_key,
            a.encoding,
            a.normalize_force,
        ),
        "analyze": lambda a: AnalyzeCommand(
            a.input_dir,
            a.output_sequences,
            a.output_frequency,
            a.tokenizer_dir,
            a.workers,
            a.chunk_size,
            a.max_texts,
            a.encoding,
        ),
        "distill-tokenizer": lambda a: DistillCommand(
            a.original_tokenizer_dir, a.distilled_tokenizer_dir, a.corpus_dir
        ),
        "select": lambda a: SelectCommand(
            a.frequency_path,
            a.sequences_path,
            a.output_csv,
            a.output_log,
            a.tokenizer_dir,
            a.max_candidates,
            a.min_token_len,
        ),
        "remap": lambda a: RemapCommand(
            a.distilled_tokenizer_dir, a.remapped_tokenizer_dir, a.remap_rules_path, a.replacement_candidates_path
        ),
        "align": lambda a: AlignCommand(
            a.model_name,
            a.original_tokenizer_dir,
            a.remapped_tokenizer_dir,
            a.remap_rules_path,
            a.embeddings_output_dir,
            a.init_strategy,
        ),
        "train": lambda a: TrainCommand(),
    }

    factory = command_map.get(args.command)
    if not factory:
        raise NotImplementedError(f"'{args.command}'ëŠ” ìœ íš¨í•˜ì§€ ì•Šì€ ì»¤ë§¨ë“œì…ë‹ˆë‹¤.")

    return factory(args)


def format_value(value: Any) -> str:
    """ê²°ê³¼ ê°’ì„ í¬ë§·íŒ…í•œë‹¤.

    Args:
        value: í¬ë§·íŒ…í•  ê°’

    Returns:
        í¬ë§·íŒ…ëœ ë¬¸ìì—´
    """
    if isinstance(value, Path):
        formatted = str(value)
    elif isinstance(value, dict):
        formatted = f"dict({len(value)})"
    elif isinstance(value, list):
        formatted = f"list({len(value)})"
    else:
        formatted = str(value)

    return formatted[:117] + "..." if len(formatted) > 120 else formatted


def create_result_table(command_name: str, elapsed: float, result: dict[str, Any]) -> Table:
    """ì‹¤í–‰ ê²°ê³¼ í…Œì´ë¸”ì„ ìƒì„±í•œë‹¤.

    Args:
        command_name: ì»¤ë§¨ë“œ ì´ë¦„
        elapsed: ê²½ê³¼ ì‹œê°„
        result: ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬

    Returns:
        ìƒì„±ëœ Rich Table ê°ì²´
    """
    table = Table(
        title=f"âœ… {command_name} ë‹¨ê³„ ì™„ë£Œ",
        show_header=False,
        border_style="green",
    )
    table.add_column("í•­ëª©", style="bold")
    table.add_column("ê°’")
    table.add_row("ì‹¤í–‰ ì‹œê°„", f"{elapsed:.2f}ì´ˆ")

    for key, value in result.items():
        table.add_row(str(key), format_value(value))

    return table


def handle_error(
    error: Exception,
    command: str,
    elapsed: float,
    logger: logging.Logger,
) -> None:
    """ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ê³  ì¶œë ¥í•œë‹¤.

    Args:
        error: ë°œìƒí•œ ì˜ˆì™¸
        command: ì‹¤í–‰ ì¤‘ì´ë˜ ì»¤ë§¨ë“œ ì´ë¦„
        elapsed: ê²½ê³¼ ì‹œê°„
        logger: ë¡œê±° ê°ì²´
    """
    error_type = type(error).__name__

    if isinstance(error, NotImplementedError):
        logger.error("[%s] ë¯¸êµ¬í˜„/ë¯¸ì§€ì› ì˜¤ë¥˜: %s", command, error)
    elif isinstance(error, (FileNotFoundError, ValueError)):
        logger.error("[%s] ì…ë ¥ ê²€ì¦ ì˜¤ë¥˜: %s", command, error)
    else:
        logger.exception("[%s] ì‹¤í–‰ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", command)

    CONSOLE.print(
        Panel.fit(
            f"[bold red]{command} ë‹¨ê³„ ì‹¤í–‰ ì‹¤íŒ¨[/bold red]\n"
            f"{error_type}: {error}\n"
            f"[dim]ê²½ê³¼ ì‹œê°„: {elapsed:.2f}ì´ˆ[/dim]",
            title="ì‹¤í–‰ ì˜¤ë¥˜",
            border_style="red",
        )
    )


def main() -> int:
    """CLI ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸.

    íŒŒì´í”„ë¼ì¸ ëª…ë ¹ì–´ë¥¼ íŒŒì‹±í•˜ê³  ì‹¤í–‰í•œë‹¤. ê° ë‹¨ê³„ë³„ ëª…ë ¹ì–´ëŠ”
    ì„œë¸Œì»¤ë§¨ë“œë¡œ ì œê³µë˜ë©°, Rich ê¸°ë°˜ ì½˜ì†” ì¶œë ¥ê³¼ íŒŒì¼ ë¡œê¹…ì„ ì§€ì›í•œë‹¤.

    Returns:
        ì¢…ë£Œ ì½”ë“œ (0: ì„±ê³µ, 1: ì˜¤ë¥˜, 130: ì‚¬ìš©ì ì¤‘ë‹¨)
    """
    print_banner()
    parser = setup_parser()
    args = parser.parse_args()

    logger = setup_logging(args.log_level)

    start = perf_counter()

    try:
        command = create_command(args)
        command_name = command.get_name()
        logger.info("ğŸš€ [%s] ë‹¨ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.", command_name)
        result = command.execute()
        elapsed = perf_counter() - start

        logger.info("âœ… [%s] ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (%.2fs)", command_name, elapsed)
        CONSOLE.print(create_result_table(command_name, elapsed, result))

        return 0

    except KeyboardInterrupt:
        logger.warning("â¹ï¸ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì‹¤í–‰ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 130

    except Exception as e:
        handle_error(e, args.command, perf_counter() - start, logger)
        return 1


if __name__ == "__main__":
    raise SystemExit(main())
