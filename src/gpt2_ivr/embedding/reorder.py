"""remap ê·œì¹™ ê¸°ì¤€ ì„ë² ë”© ì¬ì •ë ¬ ë¡œì§"""

from __future__ import annotations

import json
import logging
from pathlib import Path

import torch
import yaml
from tokenizers import Tokenizer

from gpt2_ivr.utils.logging_config import get_logger


def reorder_embeddings(
    original_wte_path: Path,
    original_tokenizer_dir: Path,
    remapped_tokenizer_dir: Path,
    remap_rules_path: Path,
    output_dir: Path,
    logger: logging.Logger | None = None,
) -> dict[str, Path]:
    """
    Remap ê·œì¹™ì— ë”°ë¼ í† í° ì„ë² ë”©ì„ ì¬ì •ë ¬í•œë‹¤.

    Args:
        original_wte_path: ì›ë³¸ í† í° ì„ë² ë”© íŒŒì¼ ê²½ë¡œ
        original_tokenizer_dir: ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬
        remapped_tokenizer_dir: ì¬í• ë‹¹ëœ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬
        remap_rules_path: Remap ê·œì¹™ YAML íŒŒì¼ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        logger: ë¡œê±° ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì‚¬í•­)

    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
    """
    if logger is None:
        logger = get_logger("gpt2_ivr.embedding.reorder")

    logger.info("ğŸ”„ ì„ë² ë”© ì¬ì •ë ¬ ì‹œì‘")

    # 1. ì›ë³¸ ì„ë² ë”© ë¡œë“œ
    logger.info("ğŸ“¥ ì›ë³¸ ì„ë² ë”© ë¡œë”©: %s", original_wte_path)
    original_wte = torch.load(original_wte_path)
    vocab_size, embedding_dim = original_wte.shape
    logger.info("ì›ë³¸ ì„ë² ë”© shape: (%d, %d)", vocab_size, embedding_dim)

    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
    logger.info("ğŸ“¥ ì›ë³¸ í† í¬ë‚˜ì´ì € ë¡œë”©: %s", original_tokenizer_dir)
    original_tokenizer = Tokenizer.from_file(
        str(original_tokenizer_dir / "tokenizer.json")
    )

    logger.info("ğŸ“¥ Remapped í† í¬ë‚˜ì´ì € ë¡œë”©: %s", remapped_tokenizer_dir)
    remapped_tokenizer = Tokenizer.from_file(
        str(remapped_tokenizer_dir / "tokenizer.json")
    )

    # 3. Remap ê·œì¹™ ë¡œë“œ
    logger.info("ğŸ“¥ Remap ê·œì¹™ ë¡œë”©: %s", remap_rules_path)
    with open(remap_rules_path, "r", encoding="utf-8") as f:
        remap_rules = yaml.safe_load(f) or {}
    logger.info("ì´ %dê°œì˜ remap ê·œì¹™ ë°œê²¬", len(remap_rules))

    # 4. ìƒˆë¡œìš´ ì„ë² ë”© í…ì„œ ìƒì„±
    new_vocab_size = remapped_tokenizer.get_vocab_size()
    logger.info("ìƒˆë¡œìš´ vocab í¬ê¸°: %d", new_vocab_size)

    # Vocab í¬ê¸° ê²€ì¦
    if new_vocab_size < vocab_size:
        raise ValueError(
            f"ìƒˆ vocab í¬ê¸°({new_vocab_size})ê°€ ì›ë³¸({vocab_size})ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤."
        )

    # ìƒˆ ì„ë² ë”©ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”
    aligned_wte = torch.zeros(new_vocab_size, embedding_dim, dtype=original_wte.dtype)

    if new_vocab_size == vocab_size:
        logger.info("âœ… Vocab í¬ê¸° ë™ì¼: %d", vocab_size)
    else:
        logger.info("âš ï¸ Vocab í¬ê¸° ì¦ê°€: %d -> %d", vocab_size, new_vocab_size)

    # 5. ë¨¼ì € ê¸°ì¡´ í† í°ë“¤ì˜ ì„ë² ë”©ì„ ë³µì‚¬ (remapë˜ì§€ ì•Šì€ í† í° ë³´ì¡´)
    original_vocab = original_tokenizer.get_vocab()
    remapped_vocab = remapped_tokenizer.get_vocab()
    preserved_count = 0

    for token, old_id in original_vocab.items():
        new_id = remapped_vocab.get(token)
        if new_id is not None and token not in remap_rules.values():
            # remapì˜ targetì´ ì•„ë‹Œ í† í°ì€ ê·¸ëŒ€ë¡œ ë³´ì¡´
            aligned_wte[new_id] = original_wte[old_id].clone()
            preserved_count += 1

    logger.info("âœ… %dê°œì˜ ê¸°ì¡´ í† í° ì„ë² ë”© ë³´ì¡´", preserved_count)

    # 6. Remap ê·œì¹™ì— ë”°ë¼ ì„ë² ë”© ì¬ë°°ì¹˜
    remap_count = 0
    for old_token, new_token in remap_rules.items():
        old_id = original_tokenizer.token_to_id(old_token)
        new_id = remapped_tokenizer.token_to_id(new_token)

        if old_id is not None and new_id is not None:
            # ì›ë³¸ í† í°ì˜ ì„ë² ë”©ì„ ìƒˆ í† í° ìœ„ì¹˜ë¡œ ë³µì‚¬
            aligned_wte[new_id] = original_wte[old_id].clone()
            remap_count += 1
            logger.debug(
                "ì¬í• ë‹¹: '%s' (id:%d) -> '%s' (id:%d)",
                old_token,
                old_id,
                new_token,
                new_id,
            )

    logger.info("âœ… ì´ %dê°œì˜ ì„ë² ë”© ì¬í• ë‹¹ ì™„ë£Œ", remap_count)

    # 7. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ë° ì €ì¥
    output_dir.mkdir(parents=True, exist_ok=True)

    aligned_wte_path = output_dir / "aligned_wte.pt"
    torch.save(aligned_wte, aligned_wte_path)
    logger.info("ğŸ’¾ ì¬ì •ë ¬ëœ ì„ë² ë”© ì €ì¥: %s", aligned_wte_path)

    # 8. ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        "original_vocab_size": vocab_size,
        "new_vocab_size": new_vocab_size,
        "embedding_dim": embedding_dim,
        "preserved_count": preserved_count,
        "remap_count": remap_count,
        "aligned_wte_shape": list(aligned_wte.shape),
    }

    metadata_path = output_dir / "reorder_metadata.json"
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    logger.info("ğŸ“‹ ë©”íƒ€ë°ì´í„° ì €ì¥: %s", metadata_path)

    return {
        "aligned_wte": aligned_wte_path,
        "metadata": metadata_path,
    }
