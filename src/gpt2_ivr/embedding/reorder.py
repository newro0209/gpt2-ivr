"""remap ê·œì¹™ ê¸°ì¤€ ì„ë² ë”© ì¬ì •ë ¬ ë¡œì§"""

from __future__ import annotations

from pathlib import Path

import torch
import yaml
from tokenizers import Tokenizer

from gpt2_ivr.utils.logging_config import get_logger


def reorder_embeddings(
    original_embeddings_path: Path = Path("artifacts/embeddings/original_embeddings.pt"),
    original_tokenizer_path: Path = Path("artifacts/tokenizers/original/tokenizer.json"),
    remapped_tokenizer_path: Path = Path("artifacts/tokenizers/remapped/tokenizer.json"),
    remap_rules_path: Path = Path("src/gpt2_ivr/tokenizer/remap_rules.yaml"),
    output_path: Path = Path("artifacts/embeddings/reordered_embeddings.pt"),
) -> dict[str, torch.Tensor]:
    """í† í° ì¬í• ë‹¹ ê·œì¹™ì— ë”°ë¼ ì„ë² ë”© ì¬ì •ë ¬
    
    Args:
        original_embeddings_path: ì›ë³¸ ì„ë² ë”© íŒŒì¼ ê²½ë¡œ
        original_tokenizer_path: ì›ë³¸ í† í¬ë‚˜ì´ì € ê²½ë¡œ
        remapped_tokenizer_path: ì¬í• ë‹¹ëœ í† í¬ë‚˜ì´ì € ê²½ë¡œ
        remap_rules_path: ì¬í• ë‹¹ ê·œì¹™ íŒŒì¼ ê²½ë¡œ
        output_path: ì¬ì •ë ¬ëœ ì„ë² ë”© ì €ì¥ ê²½ë¡œ
        
    Returns:
        ì¬ì •ë ¬ëœ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬
    """
    logger = get_logger("gpt2_ivr.embedding.reorder")
    
    # ì›ë³¸ ì„ë² ë”© ë¡œë“œ
    logger.info(f"ğŸ“‚ ì›ë³¸ ì„ë² ë”© ë¡œë“œ: {original_embeddings_path}")
    embeddings = torch.load(original_embeddings_path)
    wte = embeddings["wte"]
    lm_head = embeddings["lm_head"]
    
    logger.info(f"ğŸ“Š ì›ë³¸ ì„ë² ë”© shape: {wte.shape}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    logger.info(f"ğŸ“– ì›ë³¸ í† í¬ë‚˜ì´ì € ë¡œë“œ: {original_tokenizer_path}")
    original_tokenizer = Tokenizer.from_file(str(original_tokenizer_path))
    
    logger.info(f"ğŸ“– ì¬í• ë‹¹ í† í¬ë‚˜ì´ì € ë¡œë“œ: {remapped_tokenizer_path}")
    remapped_tokenizer = Tokenizer.from_file(str(remapped_tokenizer_path))
    
    # ì¬í• ë‹¹ ê·œì¹™ ë¡œë“œ
    if remap_rules_path.exists():
        with open(remap_rules_path, "r", encoding="utf-8") as f:
            remap_rules = yaml.safe_load(f)
            if remap_rules is None:
                remap_rules = {}
    else:
        logger.warning(f"âš ï¸ ì¬í• ë‹¹ ê·œì¹™ íŒŒì¼ ì—†ìŒ: {remap_rules_path}")
        remap_rules = {}
    
    logger.info(f"ğŸ“‹ ì¬í• ë‹¹ ê·œì¹™ ê°œìˆ˜: {len(remap_rules)}")
    
    # ìƒˆ vocab size
    new_vocab_size = remapped_tokenizer.get_vocab_size()
    embedding_dim = wte.shape[1]
    
    # ìƒˆ ì„ë² ë”© í…ì„œ ì´ˆê¸°í™”
    new_wte = torch.zeros(new_vocab_size, embedding_dim, dtype=wte.dtype)
    new_lm_head = torch.zeros(new_vocab_size, embedding_dim, dtype=lm_head.dtype)
    
    logger.info(f"ğŸ“Š ìƒˆ ì„ë² ë”© shape: {new_wte.shape}")
    
    # ê¸°ì¡´ í† í°ì— ëŒ€í•œ ì„ë² ë”© ë³µì‚¬
    original_vocab = original_tokenizer.get_vocab()
    remapped_vocab = remapped_tokenizer.get_vocab()
    
    copied_count = 0
    new_count = 0
    remapped_count = 0
    
    for token, new_id in remapped_vocab.items():
        if token in original_vocab:
            # ê¸°ì¡´ í† í°: ì›ë³¸ ì„ë² ë”© ë³µì‚¬
            old_id = original_vocab[token]
            if old_id < wte.shape[0]:
                new_wte[new_id] = wte[old_id]
                new_lm_head[new_id] = lm_head[old_id]
                copied_count += 1
        else:
            # ìƒˆ í† í°: remap ê·œì¹™ í™•ì¸
            is_remapped = False
            for old_token, new_token in remap_rules.items():
                if new_token == token and old_token in original_vocab:
                    # ì¬í• ë‹¹ëœ í† í°: ì›ë³¸ í† í°ì˜ ì„ë² ë”© ë³µì‚¬
                    old_id = original_vocab[old_token]
                    if old_id < wte.shape[0]:
                        new_wte[new_id] = wte[old_id]
                        new_lm_head[new_id] = lm_head[old_id]
                        remapped_count += 1
                        is_remapped = True
                        logger.debug(f"  ì¬í• ë‹¹: '{old_token}' (id:{old_id}) -> '{new_token}' (id:{new_id})")
                        break
            
            if not is_remapped:
                # ì™„ì „íˆ ìƒˆë¡œìš´ í† í°: í‰ê· ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
                new_wte[new_id] = wte.mean(dim=0)
                new_lm_head[new_id] = lm_head.mean(dim=0)
                new_count += 1
    
    logger.info(f"âœ… ì„ë² ë”© ì¬ì •ë ¬ ì™„ë£Œ:")
    logger.info(f"  - ë³µì‚¬ëœ í† í°: {copied_count}ê°œ")
    logger.info(f"  - ì¬í• ë‹¹ëœ í† í°: {remapped_count}ê°œ")
    logger.info(f"  - ìƒˆë¡œ ì´ˆê¸°í™”ëœ í† í°: {new_count}ê°œ")
    
    reordered_embeddings = {
        "wte": new_wte,
        "lm_head": new_lm_head,
    }
    
    # ì €ì¥
    output_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(reordered_embeddings, output_path)
    logger.info(f"ğŸ’¾ ì¬ì •ë ¬ëœ ì„ë² ë”© ì €ì¥: {output_path}")
    
    return reordered_embeddings
