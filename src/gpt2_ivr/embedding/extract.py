"""ê¸°ì¡´ ëª¨ë¸ ì„ë² ë”© ì¶”ì¶œ ë¡œì§"""

from __future__ import annotations

from pathlib import Path

import torch
from transformers import AutoModelForCausalLM

from gpt2_ivr.utils.logging_config import get_logger


def extract_embeddings(
    model_name_or_path: str = "openai-community/gpt2",
    output_path: Path = Path("artifacts/embeddings/original_embeddings.pt"),
) -> dict[str, torch.Tensor]:
    """ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì—ì„œ ì„ë² ë”© ì¶”ì¶œ
    
    Args:
        model_name_or_path: ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ
        output_path: ì„ë² ë”© ì €ì¥ ê²½ë¡œ
        
    Returns:
        ì¶”ì¶œëœ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬
    """
    logger = get_logger("gpt2_ivr.embedding.extract")
    
    logger.info(f"ğŸ¤– ëª¨ë¸ ë¡œë“œ: {model_name_or_path}")
    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
    
    # Input embeddings (token embeddings) ì¶”ì¶œ
    input_embeddings = model.get_input_embeddings()
    wte = input_embeddings.weight.data.clone()
    
    # Output embeddings (LM head) ì¶”ì¶œ (GPT-2ëŠ” tied weights ì‚¬ìš©)
    output_embeddings = model.get_output_embeddings()
    lm_head = output_embeddings.weight.data.clone()
    
    logger.info(f"ğŸ“Š Input embeddings shape: {wte.shape}")
    logger.info(f"ğŸ“Š Output embeddings shape: {lm_head.shape}")
    
    embeddings = {
        "wte": wte,  # Word Token Embeddings
        "lm_head": lm_head,  # Language Model Head
    }
    
    # ì €ì¥
    output_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(embeddings, output_path)
    logger.info(f"ğŸ’¾ ì„ë² ë”© ì €ì¥: {output_path}")
    
    return embeddings
