"""ì‹ ê·œ í† í° ì„ë² ë”© ì´ˆê¸°í™” ë¡œì§"""

from __future__ import annotations

import json
import logging
from pathlib import Path

import torch
import yaml
from tokenizers import Tokenizer

from gpt2_ivr.utils.logging_config import get_logger


def initialize_new_token_embeddings(
    aligned_wte_path: Path,
    original_tokenizer_dir: Path,
    remapped_tokenizer_dir: Path,
    remap_rules_path: Path,
    output_dir: Path,
    init_strategy: str = "mean",
    logger: logging.Logger | None = None,
) -> dict[str, Path]:
    """
    ì‹ ê·œ ì¶”ê°€ëœ í† í°ì— ëŒ€í•œ ì„ë² ë”©ì„ ì´ˆê¸°í™”í•œë‹¤.

    Args:
        aligned_wte_path: ì¬ì •ë ¬ëœ í† í° ì„ë² ë”© íŒŒì¼ ê²½ë¡œ
        original_tokenizer_dir: ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬
        remapped_tokenizer_dir: ì¬í• ë‹¹ëœ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬
        remap_rules_path: Remap ê·œì¹™ YAML íŒŒì¼ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        init_strategy: ì´ˆê¸°í™” ì „ëµ ('mean', 'random', 'zeros')
        logger: ë¡œê±° ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì‚¬í•­)

    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
    """
    if logger is None:
        logger = get_logger("gpt2_ivr.embedding.init_new")

    logger.info("ğŸ†• ì‹ ê·œ í† í° ì„ë² ë”© ì´ˆê¸°í™” ì‹œì‘")
    logger.info("ì´ˆê¸°í™” ì „ëµ: %s", init_strategy)

    # 1. ì¬ì •ë ¬ëœ ì„ë² ë”© ë¡œë“œ
    logger.info("ğŸ“¥ ì¬ì •ë ¬ëœ ì„ë² ë”© ë¡œë”©: %s", aligned_wte_path)
    aligned_wte = torch.load(aligned_wte_path)
    vocab_size, embedding_dim = aligned_wte.shape
    logger.info("ì„ë² ë”© shape: (%d, %d)", vocab_size, embedding_dim)

    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
    logger.info("ğŸ“¥ ì›ë³¸ í† í¬ë‚˜ì´ì € ë¡œë”©: %s", original_tokenizer_dir)
    original_tokenizer = Tokenizer.from_file(
        str(original_tokenizer_dir / "tokenizer.json")
    )

    logger.info("ğŸ“¥ Remapped í† í¬ë‚˜ì´ì € ë¡œë”©: %s", remapped_tokenizer_dir)
    remapped_tokenizer = Tokenizer.from_file(
        str(remapped_tokenizer_dir / "tokenizer.json")
    )

    # 3. Remap ê·œì¹™ ë¡œë“œ
    logger.info("ğŸ“¥ Remap ê·œì¹™ ë¡œë”©: %s", remap_rules_path)
    with open(remap_rules_path, "r", encoding="utf-8") as f:
        remap_rules = yaml.safe_load(f) or {}

    # 4. ì‹ ê·œ í† í° ì°¾ê¸° (remappedì—ëŠ” ìˆì§€ë§Œ originalì—ëŠ” ì—†ëŠ” í† í°)
    new_tokens = []
    remapped_vocab = remapped_tokenizer.get_vocab()

    for token, token_id in remapped_vocab.items():
        # remap rulesì— ìˆëŠ” new_token ì¤‘ originalì— ì—†ë˜ ê²ƒ
        is_new = True
        for old_token, new_token in remap_rules.items():
            if token == new_token:
                old_id = original_tokenizer.token_to_id(old_token)
                if old_id is None:
                    # old_tokenë„ ì—†ì—ˆë˜ ê²½ìš° -> ì§„ì§œ ì‹ ê·œ
                    is_new = True
                else:
                    # old_tokenì´ ìˆì—ˆë˜ ê²½ìš° -> ì¬í• ë‹¹ì´ë¯€ë¡œ ì‹ ê·œ ì•„ë‹˜
                    is_new = False
                break
        else:
            # remap_rulesì— ì—†ëŠ” í† í°ì¸ ê²½ìš°, originalì— ìˆëŠ”ì§€ í™•ì¸
            if original_tokenizer.token_to_id(token) is not None:
                is_new = False

        if is_new and token_id < vocab_size:
            # ì„ë² ë”©ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸ (ëª¨ë‘ 0ì¸ ê²½ìš°)
            if torch.all(aligned_wte[token_id] == 0):
                new_tokens.append((token, token_id))

    logger.info("ì‹ ê·œ í† í° %dê°œ ë°œê²¬", len(new_tokens))

    # 5. ì´ˆê¸°í™” ì „ëµì— ë”°ë¼ ì„ë² ë”© ì´ˆê¸°í™”
    if len(new_tokens) > 0:
        if init_strategy == "mean":
            # ê¸°ì¡´ ì„ë² ë”©ë“¤ì˜ í‰ê· ìœ¼ë¡œ ì´ˆê¸°í™”
            # 0ì´ ì•„ë‹Œ ì„ë² ë”©ë“¤ë§Œ ê³ ë ¤
            non_zero_mask = ~torch.all(aligned_wte == 0, dim=1)
            if non_zero_mask.sum() > 0:
                mean_embedding = aligned_wte[non_zero_mask].mean(dim=0)
            else:
                mean_embedding = torch.zeros(embedding_dim)

            for token, token_id in new_tokens:
                aligned_wte[token_id] = mean_embedding
                logger.debug(
                    "í† í° '%s' (id:%d) -> mean ì„ë² ë”©ìœ¼ë¡œ ì´ˆê¸°í™”", token, token_id
                )

        elif init_strategy == "random":
            # ì •ê·œë¶„í¬ë¡œë¶€í„° ëœë¤ ì´ˆê¸°í™”
            std = aligned_wte[~torch.all(aligned_wte == 0, dim=1)].std().item()
            for token, token_id in new_tokens:
                aligned_wte[token_id] = torch.randn(embedding_dim) * std
                logger.debug(
                    "í† í° '%s' (id:%d) -> random ì„ë² ë”©ìœ¼ë¡œ ì´ˆê¸°í™” (std=%.4f)",
                    token,
                    token_id,
                    std,
                )

        elif init_strategy == "zeros":
            # ì´ë¯¸ 0ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ
            logger.info("zeros ì „ëµ ì„ íƒ - ê¸°ì¡´ zero ì„ë² ë”© ìœ ì§€")

        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì´ˆê¸°í™” ì „ëµ: {init_strategy}")

        logger.info("âœ… %dê°œì˜ ì‹ ê·œ í† í° ì„ë² ë”© ì´ˆê¸°í™” ì™„ë£Œ", len(new_tokens))
    else:
        logger.info("ì´ˆê¸°í™”í•  ì‹ ê·œ í† í°ì´ ì—†ìŠµë‹ˆë‹¤")

    # 6. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ë° ì €ì¥
    output_dir.mkdir(parents=True, exist_ok=True)

    final_wte_path = output_dir / "final_wte.pt"
    torch.save(aligned_wte, final_wte_path)
    logger.info("ğŸ’¾ ìµœì¢… ì„ë² ë”© ì €ì¥: %s", final_wte_path)

    # 7. ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        "vocab_size": vocab_size,
        "embedding_dim": embedding_dim,
        "new_tokens_count": len(new_tokens),
        "init_strategy": init_strategy,
        "new_tokens": [
            {"token": token, "id": token_id} for token, token_id in new_tokens
        ],
    }

    metadata_path = output_dir / "init_new_metadata.json"
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    logger.info("ğŸ“‹ ë©”íƒ€ë°ì´í„° ì €ì¥: %s", metadata_path)

    return {
        "final_wte": final_wte_path,
        "metadata": metadata_path,
    }
