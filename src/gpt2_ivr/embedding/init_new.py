"""ì‹ ê·œ í† í° ì„ë² ë”© ì´ˆê¸°í™” ë¡œì§.

ì¬í• ë‹¹ ê³¼ì •ì—ì„œ ìƒˆë¡œ ì¶”ê°€ëœ í† í°ì˜ ì„ë² ë”©ì„ ì´ˆê¸°í™”í•œë‹¤.
ì„¸ ê°€ì§€ ì „ëµì„ ì§€ì›í•œë‹¤:
- mean: ê¸°ì¡´ í† í° ì„ë² ë”©ì˜ í‰ê· ìœ¼ë¡œ ì´ˆê¸°í™”
- random: ê¸°ì¡´ í† í° ì„ë² ë”©ì˜ í‘œì¤€í¸ì°¨ë¥¼ ì‚¬ìš©í•œ ì •ê·œë¶„í¬ì—ì„œ ìƒ˜í”Œë§
- zeros: 0ìœ¼ë¡œ ìœ ì§€
"""

from __future__ import annotations

import json
import logging
from pathlib import Path

import torch
import yaml
from tokenizers import Tokenizer


def initialize_new_token_embeddings(
    aligned_wte_path: Path,
    original_tokenizer_dir: Path,
    remapped_tokenizer_dir: Path,
    remap_rules_path: Path,
    output_dir: Path,
    init_strategy: str = "mean",
    logger: logging.Logger | None = None,
) -> dict[str, Path]:
    """ì‹ ê·œ ì¶”ê°€ëœ í† í°ì— ëŒ€í•œ ì„ë² ë”©ì„ ì´ˆê¸°í™”í•œë‹¤.

    Args:
        aligned_wte_path: ì¬ì •ë ¬ëœ í† í° ì„ë² ë”© íŒŒì¼ ê²½ë¡œ
        original_tokenizer_dir: ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬
        remapped_tokenizer_dir: ì¬í• ë‹¹ëœ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬
        remap_rules_path: Remap ê·œì¹™ YAML íŒŒì¼ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        init_strategy: ì´ˆê¸°í™” ì „ëµ ('mean', 'random', 'zeros')
        logger: ë¡œê±° ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì‚¬í•­)

    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
    """
    if logger is None:
        logger = logging.getLogger("gpt2_ivr.embedding.init_new")

    logger.info("ğŸ†• ì‹ ê·œ í† í° ì„ë² ë”© ì´ˆê¸°í™” ì‹œì‘")
    logger.info("ì´ˆê¸°í™” ì „ëµ: %s", init_strategy)

    # 1. ì¬ì •ë ¬ëœ ì„ë² ë”© ë¡œë“œ
    logger.info("ğŸ“¥ ì¬ì •ë ¬ëœ ì„ë² ë”© ë¡œë”©: %s", aligned_wte_path)
    aligned_wte = torch.load(aligned_wte_path)
    vocab_size, embedding_dim = aligned_wte.shape
    logger.info("ì„ë² ë”© shape: (%d, %d)", vocab_size, embedding_dim)

    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
    logger.info("ğŸ“¥ ì›ë³¸ í† í¬ë‚˜ì´ì € ë¡œë”©: %s", original_tokenizer_dir)
    original_tokenizer = Tokenizer.from_file(
        str(original_tokenizer_dir / "tokenizer.json")
    )

    logger.info("ğŸ“¥ Remapped í† í¬ë‚˜ì´ì € ë¡œë”©: %s", remapped_tokenizer_dir)
    remapped_tokenizer = Tokenizer.from_file(
        str(remapped_tokenizer_dir / "tokenizer.json")
    )

    # 3. Remap ê·œì¹™ ë¡œë“œ
    logger.info("ğŸ“¥ Remap ê·œì¹™ ë¡œë”©: %s", remap_rules_path)
    with open(remap_rules_path, "r", encoding="utf-8") as f:
        remap_rules = yaml.safe_load(f) or {}

    # 4. ì‹ ê·œ í† í° íƒìƒ‰ (remapped vocabì— ìˆì§€ë§Œ original vocabì—ëŠ” ì—†ëŠ” í† í°)
    # 1) originalì— ìˆë˜ í† í°ì€ ì œì™¸
    # 2) remap targetì´ì§€ë§Œ sourceê°€ originalì— ìˆë˜ ê²½ìš°ë„ ì œì™¸ (ì¬í• ë‹¹ì´ë¯€ë¡œ ì‹ ê·œ ì•„ë‹˜)
    # 3) ì„ë² ë”©ì´ 0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ ê²½ìš°ë§Œ ì‹ ê·œë¡œ íŒë‹¨
    new_tokens = []
    remapped_vocab = remapped_tokenizer.get_vocab()

    for token, token_id in remapped_vocab.items():
        if original_tokenizer.token_to_id(token) is not None:
            continue

        is_remapped_target = token in remap_rules.values()
        if is_remapped_target:
            source_existed = any(
                original_tokenizer.token_to_id(old_token) is not None
                for old_token, new_token in remap_rules.items()
                if new_token == token
            )
            if source_existed:
                continue

        if token_id < vocab_size and torch.all(aligned_wte[token_id] == 0):
            new_tokens.append((token, token_id))

    logger.info("ì‹ ê·œ í† í° %dê°œ ë°œê²¬", len(new_tokens))

    # 5. ì´ˆê¸°í™” ì „ëµì— ë”°ë¼ ì„ë² ë”© ì´ˆê¸°í™”
    if len(new_tokens) > 0:
        non_zero_mask = ~torch.all(aligned_wte == 0, dim=1)
        has_non_zero = non_zero_mask.sum() > 0

        if init_strategy == "mean":
            if has_non_zero:
                mean_embedding = aligned_wte[non_zero_mask].mean(dim=0)
            else:
                mean_embedding = torch.zeros(embedding_dim)

            for token, token_id in new_tokens:
                aligned_wte[token_id] = mean_embedding
                logger.debug(
                    "í† í° '%s' (id:%d) -> mean ì„ë² ë”©ìœ¼ë¡œ ì´ˆê¸°í™”", token, token_id
                )

        elif init_strategy == "random":
            if has_non_zero:
                std = aligned_wte[non_zero_mask].std().item()
            else:
                std = 0.02

            for token, token_id in new_tokens:
                aligned_wte[token_id] = torch.randn(embedding_dim) * std
                logger.debug(
                    "í† í° '%s' (id:%d) -> random ì„ë² ë”©ìœ¼ë¡œ ì´ˆê¸°í™” (std=%.4f)",
                    token,
                    token_id,
                    std,
                )

        elif init_strategy == "zeros":
            logger.info("zeros ì „ëµ ì„ íƒ - ê¸°ì¡´ zero ì„ë² ë”© ìœ ì§€")

        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì´ˆê¸°í™” ì „ëµ: {init_strategy}")

        logger.info("âœ… %dê°œì˜ ì‹ ê·œ í† í° ì„ë² ë”© ì´ˆê¸°í™” ì™„ë£Œ", len(new_tokens))
    else:
        logger.info("ì´ˆê¸°í™”í•  ì‹ ê·œ í† í°ì´ ì—†ìŠµë‹ˆë‹¤")

    # 6. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ë° ì €ì¥
    output_dir.mkdir(parents=True, exist_ok=True)

    final_wte_path = output_dir / "final_wte.pt"
    torch.save(aligned_wte, final_wte_path)
    logger.info("ğŸ’¾ ìµœì¢… ì„ë² ë”© ì €ì¥: %s", final_wte_path)

    # 7. ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        "vocab_size": vocab_size,
        "embedding_dim": embedding_dim,
        "new_tokens_count": len(new_tokens),
        "init_strategy": init_strategy,
        "new_tokens": [
            {"token": token, "id": token_id} for token, token_id in new_tokens
        ],
    }

    metadata_path = output_dir / "init_new_metadata.json"
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    logger.info("ğŸ“‹ ë©”íƒ€ë°ì´í„° ì €ì¥: %s", metadata_path)

    return {
        "final_wte": final_wte_path,
        "metadata": metadata_path,
    }
