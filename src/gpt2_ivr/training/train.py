"""accelerate ê¸°ë°˜ í•™ìŠµ ì‹¤í–‰ ë¡œì§"""

from __future__ import annotations

import logging
from pathlib import Path
from typing import Any

import torch
import yaml
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)
from transformers.tokenization_utils import PreTrainedTokenizer

from gpt2_ivr.utils.logging_config import get_logger


def load_training_config(config_path: Path) -> dict[str, Any]:
    """í•™ìŠµ ì„¤ì • íŒŒì¼ ë¡œë“œ
    
    Args:
        config_path: YAML ì„¤ì • íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    return config


def load_dataset(dataset_path: Path, tokenizer: PreTrainedTokenizer) -> dict[str, Any]:
    """ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬
    
    Args:
        dataset_path: ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        tokenizer: í† í¬ë‚˜ì´ì € ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„°ì…‹
    """
    logger = get_logger("gpt2_ivr.training.load_dataset")
    
    # ì½”í¼ìŠ¤ íŒŒì¼ ìˆ˜ì§‘
    corpus_files = list(dataset_path.glob("*.txt"))
    if not corpus_files:
        raise FileNotFoundError(f"ì½”í¼ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_path}")
    
    logger.info(f"ğŸ“‚ {len(corpus_files)}ê°œì˜ ì½”í¼ìŠ¤ íŒŒì¼ ë°œê²¬")
    
    # í…ìŠ¤íŠ¸ ë¡œë“œ
    texts = []
    for file_path in corpus_files:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
            if text:
                texts.append(text)
    
    logger.info(f"ğŸ“ {len(texts)}ê°œì˜ í…ìŠ¤íŠ¸ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ")
    
    # í† í¬ë‚˜ì´ì§•
    tokenized_texts = tokenizer(
        texts,
        truncation=True,
        max_length=1024,
        padding=False,
        return_tensors=None,
    )
    
    logger.info(f"âœ… í† í¬ë‚˜ì´ì§• ì™„ë£Œ: {len(tokenized_texts['input_ids'])}ê°œ ì‹œí€€ìŠ¤")
    
    return {
        "input_ids": tokenized_texts["input_ids"],
        "attention_mask": tokenized_texts["attention_mask"],
    }


def train_model(
    model_name_or_path: str = "openai-community/gpt2",
    tokenizer_path: Path = Path("artifacts/tokenizers/remapped"),
    dataset_path: Path = Path("artifacts/corpora/cleaned"),
    output_dir: Path = Path("artifacts/training/sft_checkpoint"),
    config_path: Path = Path("src/gpt2_ivr/training/sft_config.yaml"),
) -> dict[str, Any]:
    """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
    
    Args:
        model_name_or_path: ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ
        tokenizer_path: í† í¬ë‚˜ì´ì € ê²½ë¡œ (remapped)
        dataset_path: í•™ìŠµ ë°ì´í„°ì…‹ ê²½ë¡œ
        output_dir: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
        config_path: í•™ìŠµ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        
    Returns:
        í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    logger = get_logger("gpt2_ivr.training")
    logger.info("ğŸš€ í•™ìŠµ ì‹œì‘")
    
    # ì„¤ì • ë¡œë“œ
    if config_path.exists():
        config = load_training_config(config_path)
        logger.info(f"âš™ï¸ ì„¤ì • íŒŒì¼ ë¡œë“œ: {config_path}")
    else:
        logger.warning(f"âš ï¸ ì„¤ì • íŒŒì¼ ì—†ìŒ: {config_path}. ê¸°ë³¸ê°’ ì‚¬ìš©")
        config = {}
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer_json_path = tokenizer_path / "tokenizer.json"
    if not tokenizer_json_path.exists():
        raise FileNotFoundError(
            f"í† í¬ë‚˜ì´ì € íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {tokenizer_json_path}\n"
            f"ë¨¼ì € 'uv run ivr remap' ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”."
        )
    
    logger.info(f"ğŸ“– í† í¬ë‚˜ì´ì € ë¡œë“œ: {tokenizer_path}")
    tokenizer = AutoTokenizer.from_pretrained(
        str(tokenizer_path),
        use_fast=True,
    )
    
    # íŠ¹ìˆ˜ í† í° ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        logger.info(f"ğŸ”§ pad_tokenì„ eos_tokenìœ¼ë¡œ ì„¤ì •: {tokenizer.eos_token}")
    
    # ëª¨ë¸ ë¡œë“œ
    logger.info(f"ğŸ¤– ëª¨ë¸ ë¡œë“œ: {model_name_or_path}")
    model_config = AutoConfig.from_pretrained(model_name_or_path)
    
    # vocab size ì—…ë°ì´íŠ¸ (IVRë¡œ í† í°ì´ ì¶”ê°€ëœ ê²½ìš°)
    tokenizer_vocab_size = len(tokenizer)
    if model_config.vocab_size != tokenizer_vocab_size:
        logger.warning(
            f"âš ï¸ ëª¨ë¸ vocab size ({model_config.vocab_size})ì™€ "
            f"í† í¬ë‚˜ì´ì € vocab size ({tokenizer_vocab_size})ê°€ ë‹¤ë¦…ë‹ˆë‹¤. "
            f"ëª¨ë¸ vocab sizeë¥¼ {tokenizer_vocab_size}ë¡œ ì¡°ì •í•©ë‹ˆë‹¤."
        )
        model_config.vocab_size = tokenizer_vocab_size
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,
        config=model_config,
    )
    
    # ì„ë² ë”© í¬ê¸° ì¡°ì • (í•„ìš”í•œ ê²½ìš°)
    if model.get_input_embeddings().weight.shape[0] != tokenizer_vocab_size:
        logger.info(
            f"ğŸ”§ ì„ë² ë”© ë ˆì´ì–´ í¬ê¸° ì¡°ì •: "
            f"{model.get_input_embeddings().weight.shape[0]} -> {tokenizer_vocab_size}"
        )
        model.resize_token_embeddings(tokenizer_vocab_size)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    logger.info(f"ğŸ“š ë°ì´í„°ì…‹ ë¡œë“œ: {dataset_path}")
    dataset = load_dataset(dataset_path, tokenizer)
    
    # Data collator ì„¤ì • (Language Modelingìš©)
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LMì´ë¯€ë¡œ MLM ì‚¬ìš© ì•ˆ í•¨
    )
    
    # Training Arguments ì„¤ì •
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        overwrite_output_dir=config.get("overwrite_output_dir", True),
        num_train_epochs=config.get("num_train_epochs", 3),
        per_device_train_batch_size=config.get("per_device_train_batch_size", 8),
        per_device_eval_batch_size=config.get("per_device_eval_batch_size", 8),
        gradient_accumulation_steps=config.get("gradient_accumulation_steps", 1),
        learning_rate=config.get("learning_rate", 5e-5),
        weight_decay=config.get("weight_decay", 0.01),
        lr_scheduler_type=config.get("lr_scheduler_type", "cosine"),
        warmup_ratio=config.get("warmup_ratio", 0.03),
        logging_dir=str(output_dir / "logs"),
        logging_steps=config.get("logging_steps", 10),
        save_steps=config.get("save_steps", 500),
        save_total_limit=config.get("save_total_limit", 2),
        seed=config.get("seed", 42),
        report_to=config.get("report_to", "tensorboard"),
        remove_unused_columns=False,
    )
    
    logger.info(f"âš™ï¸ TrainingArguments ì„¤ì • ì™„ë£Œ")
    logger.info(f"  - Epochs: {training_args.num_train_epochs}")
    logger.info(f"  - Batch size: {training_args.per_device_train_batch_size}")
    logger.info(f"  - Learning rate: {training_args.learning_rate}")
    
    # Trainer ìƒì„±
    # Datasetì„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜ (Trainerê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹)
    class SimpleDataset(torch.utils.data.Dataset):
        def __init__(self, encodings: dict[str, list[list[int]]]):
            self.encodings = encodings
        
        def __len__(self) -> int:
            return len(self.encodings["input_ids"])
        
        def __getitem__(self, idx: int) -> dict[str, torch.Tensor]:
            return {
                "input_ids": torch.tensor(self.encodings["input_ids"][idx]),
                "attention_mask": torch.tensor(self.encodings["attention_mask"][idx]),
            }
    
    train_dataset = SimpleDataset(dataset)
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
    )
    
    # í•™ìŠµ ì‹¤í–‰
    logger.info("ğŸƒ í•™ìŠµ ì‹œì‘...")
    train_result = trainer.train()
    
    # ëª¨ë¸ ì €ì¥
    logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {output_dir}")
    trainer.save_model(str(output_dir / "final_model"))
    tokenizer.save_pretrained(str(output_dir / "final_model"))
    
    # í•™ìŠµ ë©”íŠ¸ë¦­ ì €ì¥
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    
    logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
    logger.info(f"ğŸ“Š Loss: {metrics.get('train_loss', 'N/A')}")
    logger.info(f"â±ï¸ Runtime: {metrics.get('train_runtime', 'N/A')}s")
    
    return {
        "status": "success",
        "output_dir": str(output_dir),
        "final_model_dir": str(output_dir / "final_model"),
        "metrics": metrics,
    }
