from __future__ import annotations

import json
from collections import Counter
from concurrent.futures import ThreadPoolExecutor
from itertools import islice
import os
from pathlib import Path
from threading import Lock
from typing import Iterable, Iterator, TypedDict, cast

import pyarrow as pa
import pyarrow.parquet as pq
from tqdm import tqdm
from transformers import BatchEncoding, GPT2Tokenizer

from gpt2_ivr.utils.logging_config import get_logger

logger = get_logger(__name__)


class FrequencyResult(TypedDict):
    """ë¹ˆë„ ë¶„ì„ ê²°ê³¼ íƒ€ì…"""

    total_tokens: int
    unique_tokens: int
    sequences_path: Path
    frequency_path: Path


def find_input_files(input_dir: Path, inputs: list[Path]) -> list[Path]:
    """ë¶„ì„ ëŒ€ìƒ íŒŒì¼ ëª©ë¡ì„ ìˆ˜ì§‘í•œë‹¤."""
    allowed_suffixes = {".txt", ".jsonl", ".json"}
    files: list[Path] = []

    if input_dir.exists():
        for path in sorted(input_dir.rglob("*")):
            if path.is_file() and path.suffix.lower() in allowed_suffixes:
                files.append(path)

    for path in inputs:
        if path.is_file():
            files.append(path)

    return sorted(set(files))


def iter_texts(files: list[Path], text_key: str, encoding: str) -> Iterator[str]:
    """íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì„ ìƒì„±í•œë‹¤."""
    for path in files:
        suffix = path.suffix.lower()
        if suffix == ".txt":
            with path.open("r", encoding=encoding) as handle:
                for line in handle:
                    text = line.rstrip("\n")
                    if text.strip():
                        yield text
            continue

        if suffix == ".jsonl":
            with path.open("r", encoding=encoding) as handle:
                for line in handle:
                    line = line.strip()
                    if not line:
                        continue
                    record = json.loads(line)
                    if text_key in record and isinstance(record[text_key], str):
                        text = record[text_key].strip()
                        if text:
                            yield text
            continue

        if suffix == ".json":
            with path.open("r", encoding=encoding) as handle:
                payload = json.load(handle)
            if isinstance(payload, list):
                for record in payload:
                    if isinstance(record, dict) and text_key in record:
                        text = str(record[text_key]).strip()
                        if text:
                            yield text
            elif isinstance(payload, dict) and text_key in payload:
                text = str(payload[text_key]).strip()
                if text:
                    yield text


def write_frequency_parquet(counter: Counter[int], output_path: Path) -> None:
    """í† í° ë¹ˆë„ë¥¼ parquetë¡œ ì €ì¥í•œë‹¤."""
    rows = sorted(counter.items(), key=lambda item: (-item[1], item[0]))
    token_ids = [token_id for token_id, _ in rows]
    frequencies = [frequency for _, frequency in rows]
    table = pa.Table.from_pydict({"token_id": token_ids, "frequency": frequencies})
    pq.write_table(table, output_path)


def collect_statistics(
    texts: Iterable[str],
    output_sequences: Path,
    tokenizer: GPT2Tokenizer,
    workers: int,
    chunk_size: int,
) -> Counter[int]:
    """í† í° ì‹œí€€ìŠ¤ì™€ ë¹ˆë„ í†µê³„ë¥¼ ìƒì„±í•œë‹¤."""
    counter: Counter[int] = Counter()
    encode_lock = Lock()

    # ì…ë ¥ ìŠ¤íŠ¸ë¦¼ ì²­í¬ ë¶„í• 
    def chunk_iter(source: Iterable[str]) -> Iterator[list[str]]:
        iterator = iter(source)
        while True:
            chunk = list(islice(iterator, chunk_size))
            if not chunk:
                break
            yield chunk

    # ì²­í¬ ë‹¨ìœ„ í† í°í™”
    def encode_chunk(chunk: list[str]) -> list[list[int]]:
        # í† í¬ë‚˜ì´ì € ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´í˜¸
        with encode_lock:
            output: BatchEncoding = tokenizer(
                chunk,
                add_special_tokens=False,
                truncation=True,
                max_length=tokenizer.model_max_length,
            )
        return cast(list[list[int]], output["input_ids"])

    output_sequences.parent.mkdir(parents=True, exist_ok=True)
    with output_sequences.open("w", encoding="utf-8") as handle:
        # ì²­í¬ ìŠ¤ë ˆë“œ ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=workers) as executor:
            for chunk_ids in tqdm(
                executor.map(encode_chunk, chunk_iter(texts)),
                desc="í† í°í™”",
                unit="ì²­í¬",
            ):
                # ì²­í¬ ê²°ê³¼ ëˆ„ì  ë° ê¸°ë¡
                for token_ids in chunk_ids:
                    counter.update(token_ids)
                    handle.write(" ".join(str(token_id) for token_id in token_ids))
                    handle.write("\n")

    return counter


def analyze_token_frequency(
    input_dir: Path,
    inputs: list[Path],
    output_sequences: Path,
    output_frequency: Path,
    model_name: str,
    text_key: str,
    workers: int,
    chunk_size: int,
    max_texts: int,
    encoding: str,
) -> FrequencyResult:
    """í† í° ë¹ˆë„ë¥¼ ë¶„ì„í•œë‹¤.

    Args:
        input_dir: ì½”í¼ìŠ¤ ì…ë ¥ ë””ë ‰í† ë¦¬
        inputs: ê°œë³„ ì…ë ¥ íŒŒì¼ ëª©ë¡
        output_sequences: BPE í† í° ì‹œí€€ìŠ¤ ì¶œë ¥ ê²½ë¡œ
        output_frequency: í† í° ë¹ˆë„ parquet ì¶œë ¥ ê²½ë¡œ
        model_name: GPT-2 ëª¨ë¸ ì´ë¦„
        text_key: json/jsonl í…ìŠ¤íŠ¸ í‚¤
        workers: ìŠ¤ë ˆë“œ ì›Œì»¤ ìˆ˜ (0ì´ë©´ CPU * 2)
        chunk_size: ìŠ¤ë ˆë“œ ì²­í¬ í¬ê¸°
        max_texts: ì²˜ë¦¬í•  ìµœëŒ€ í…ìŠ¤íŠ¸ ìˆ˜ (0ì´ë©´ ì „ì²´)
        encoding: ì…ë ¥ íŒŒì¼ ì¸ì½”ë”©

    Returns:
        ë¶„ì„ ê²°ê³¼ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬

    Raises:
        SystemExit: ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
    """
    # ì…ë ¥ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    input_files = find_input_files(input_dir, inputs)
    if not input_files:
        raise SystemExit("ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    logger.info("ğŸ“‚ ì…ë ¥ íŒŒì¼ %dê°œë¥¼ íƒìƒ‰í–ˆìŠµë‹ˆë‹¤.", len(input_files))

    # í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ êµ¬ì„±
    texts = iter_texts(input_files, text_key, encoding)
    if max_texts > 0:
        texts = islice(texts, max_texts)
        logger.info("âš ï¸  ìµœëŒ€ %dê°œ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.", max_texts)

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    logger.info("ğŸ”¤ GPT-2 í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: %s", model_name)
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)

    # ì›Œì»¤ ìˆ˜ ê³„ì‚°
    if workers <= 0:
        workers = max(1, int((os.cpu_count() or 1) * 2))

    logger.info("ğŸ”§ í† í°í™” ì„¤ì •: workers=%d, chunk_size=%d", workers, chunk_size)

    # í† í°í™” ë° ë¹ˆë„ ì§‘ê³„
    counter = collect_statistics(
        texts,
        output_sequences=output_sequences,
        tokenizer=tokenizer,
        workers=workers,
        chunk_size=chunk_size,
    )

    # ê²°ê³¼ë¬¼ ì €ì¥
    output_frequency.parent.mkdir(parents=True, exist_ok=True)
    write_frequency_parquet(counter, output_frequency)

    total_tokens = sum(counter.values())
    unique_tokens = len(counter)

    logger.info("âœ… í† í° ë¹ˆë„ ë¶„ì„ ì™„ë£Œ")
    logger.info("  â””â”€ ì´ í† í°: %dê°œ", total_tokens)
    logger.info("  â””â”€ ê³ ìœ  í† í°: %dê°œ", unique_tokens)
    logger.info("ğŸ“„ í† í° ë¹ˆë„ ì €ì¥: %s", output_frequency)
    logger.info("ğŸ“„ í† í° ì‹œí€€ìŠ¤ ì €ì¥: %s", output_sequences)

    return FrequencyResult(
        total_tokens=total_tokens,
        unique_tokens=unique_tokens,
        sequences_path=output_sequences,
        frequency_path=output_frequency,
    )
