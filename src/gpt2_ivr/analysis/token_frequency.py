"""í† í° ë¹ˆë„ ë¶„ì„ ëª¨ë“ˆ.

ì½”í¼ìŠ¤ë¥¼ í† í°í™”í•˜ì—¬ ê° í† í°ì˜ ì¶œí˜„ ë¹ˆë„ë¥¼ ì§‘ê³„í•˜ê³  Parquet í˜•ì‹ìœ¼ë¡œ ì €ì¥í•œë‹¤.
ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ëŒ€ìš©ëŸ‰ ì½”í¼ìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.
"""

from __future__ import annotations

import logging
from collections import Counter
from concurrent.futures import ThreadPoolExecutor
from itertools import islice
import os
from pathlib import Path
from threading import Lock
from typing import Iterable, Iterator, TypedDict, cast

import pyarrow as pa
import pyarrow.parquet as pq
from transformers import BatchEncoding, GPT2Tokenizer

logger = logging.getLogger(__name__)


class FrequencyResult(TypedDict):
    """ë¹ˆë„ ë¶„ì„ ê²°ê³¼ íƒ€ì….

    Attributes:
        total_tokens: ì „ì²´ í† í° ê°œìˆ˜
        unique_tokens: ê³ ìœ  í† í° ê°œìˆ˜
        sequences_path: í† í° ì‹œí€€ìŠ¤ íŒŒì¼ ê²½ë¡œ
        frequency_path: ë¹ˆë„ parquet íŒŒì¼ ê²½ë¡œ
    """

    total_tokens: int
    unique_tokens: int
    sequences_path: Path
    frequency_path: Path


def find_input_files(input_dir: Path, inputs: list[Path]) -> list[Path]:
    """ë¶„ì„ ëŒ€ìƒ íŒŒì¼ ëª©ë¡ì„ ìˆ˜ì§‘í•œë‹¤.

    input_dirì—ì„œ ì¬ê·€ì ìœ¼ë¡œ íŒŒì¼ì„ íƒìƒ‰í•˜ê³ , inputs ë¦¬ìŠ¤íŠ¸ì˜ íŒŒì¼ì„ ì¶”ê°€í•œë‹¤.
    .txt í™•ì¥ìë§Œ í—ˆìš©í•œë‹¤.

    Args:
        input_dir: ì¬ê·€ íƒìƒ‰í•  ë””ë ‰í† ë¦¬
        inputs: ì¶”ê°€ë¡œ í¬í•¨í•  íŒŒì¼ ëª©ë¡

    Returns:
        ì¤‘ë³µ ì œê±°ëœ ì •ë ¬ëœ íŒŒì¼ ê²½ë¡œ ëª©ë¡
    """
    allowed_suffixes = {".txt"}
    files: list[Path] = []

    if input_dir.exists():
        for path in sorted(input_dir.rglob("*")):
            if path.is_file() and path.suffix.lower() in allowed_suffixes:
                files.append(path)

    for path in inputs:
        if path.is_file():
            files.append(path)

    return sorted(set(files))


def write_frequency_parquet(counter: Counter[int], output_path: Path) -> None:
    """í† í° ë¹ˆë„ë¥¼ parquetë¡œ ì €ì¥í•œë‹¤.

    ë¹ˆë„ ë‚´ë¦¼ì°¨ìˆœ, í† í° ID ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì €ì¥í•œë‹¤.

    Args:
        counter: í† í° IDë³„ ë¹ˆë„ ì¹´ìš´í„°
        output_path: Parquet íŒŒì¼ ì €ì¥ ê²½ë¡œ
    """
    rows = sorted(counter.items(), key=lambda item: (-item[1], item[0]))
    token_ids = [token_id for token_id, _ in rows]
    frequencies = [frequency for _, frequency in rows]
    table = pa.Table.from_pydict({"token_id": token_ids, "frequency": frequencies})
    pq.write_table(table, output_path)


def iter_encoded_chunks(
    texts: Iterable[str],
    tokenizer: GPT2Tokenizer,
    workers: int,
    chunk_size: int,
) -> Iterator[list[list[int]]]:
    """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ì—¬ ì²­í¬ ë‹¨ìœ„ë¡œ ë°˜í™˜í•œë‹¤.

    ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ í† í°í™”ë¥¼ ìˆ˜í–‰í•œë‹¤.
    tokenizer í˜¸ì¶œì€ thread-safeí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ Lockìœ¼ë¡œ ë³´í˜¸í•œë‹¤.

    Args:
        texts: í† í°í™”í•  í…ìŠ¤íŠ¸ iterable
        tokenizer: GPT-2 í† í¬ë‚˜ì´ì €
        workers: ìŠ¤ë ˆë“œ ì›Œì»¤ ìˆ˜
        chunk_size: ì²­í¬ë‹¹ í…ìŠ¤íŠ¸ ê°œìˆ˜

    Yields:
        í† í° ID ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸
    """
    counter: Counter[int] = Counter()
    encode_lock = Lock()

    def chunk_iter(source: Iterable[str]) -> Iterator[list[str]]:
        iterator = iter(source)
        while True:
            chunk = list(islice(iterator, chunk_size))
            if not chunk:
                break
            yield chunk

    def encode_chunk(chunk: list[str]) -> list[list[int]]:
        with encode_lock:
            output: BatchEncoding = tokenizer(
                chunk,
                add_special_tokens=False,
                truncation=True,
                max_length=tokenizer.model_max_length,
            )
        return cast(list[list[int]], output["input_ids"])

    with ThreadPoolExecutor(max_workers=workers) as executor:
        yield from executor.map(encode_chunk, chunk_iter(texts))


def analyze_token_frequency(
    input_dir: Path,
    inputs: list[Path],
    output_frequency: Path,
    tokenizer_dir: Path,
    workers: int,
    chunk_size: int,
    max_texts: int,
    encoding: str,
) -> tuple[Iterator[list[list[int]]], GPT2Tokenizer]:
    """í† í° ë¹ˆë„ë¥¼ ë¶„ì„í•œë‹¤.

    Args:
        input_dir: ì½”í¼ìŠ¤ ì…ë ¥ ë””ë ‰í† ë¦¬
        inputs: ê°œë³„ ì…ë ¥ íŒŒì¼ ëª©ë¡
        output_sequences: BPE í† í° ì‹œí€€ìŠ¤ ì¶œë ¥ ê²½ë¡œ
        output_frequency: í† í° ë¹ˆë„ parquet ì¶œë ¥ ê²½ë¡œ
        tokenizer_dir: ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬
        workers: ìŠ¤ë ˆë“œ ì›Œì»¤ ìˆ˜ (0ì´ë©´ CPU - 1)
        chunk_size: ìŠ¤ë ˆë“œ ì²­í¬ í¬ê¸° (0ì´ë©´ ìë™ ì„¤ì •)
        max_texts: ì²˜ë¦¬í•  ìµœëŒ€ í…ìŠ¤íŠ¸ ìˆ˜ (0ì´ë©´ ì „ì²´)
        encoding: ì…ë ¥ íŒŒì¼ ì¸ì½”ë”©

    Returns:
        ë¶„ì„ ê²°ê³¼ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬

    Raises:
        FileNotFoundError: ì›ë³¸ í† í¬ë‚˜ì´ì € íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
        SystemExit: ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
    """
    # 1) ì…ë ¥ íŒŒì¼ ìˆ˜ì§‘
    input_files = find_input_files(input_dir, inputs)
    if not input_files:
        raise SystemExit("ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    logger.info("ğŸ“‚ ì…ë ¥ íŒŒì¼ %dê°œë¥¼ íƒìƒ‰í–ˆìŠµë‹ˆë‹¤.", len(input_files))

    # 2) í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ êµ¬ì„±
    def _line_iterator() -> Iterator[str]:
        for path in input_files:
            with path.open("r", encoding=encoding) as handle:
                for line in handle:
                    text = line.rstrip("\n")
                    if text:
                        yield text

    texts = _line_iterator()
    if max_texts > 0:
        texts = islice(texts, max_texts)
        logger.info("âš ï¸  ìµœëŒ€ %dê°œ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.", max_texts)

    # 3) í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer_files = list(tokenizer_dir.glob("*")) if tokenizer_dir.exists() else []
    has_tokenizer_files = any(f.name in ["tokenizer.json", "vocab.json", "merges.txt"] for f in tokenizer_files)
    if not has_tokenizer_files:
        raise FileNotFoundError(f"ì›ë³¸ í† í¬ë‚˜ì´ì € íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {tokenizer_dir}")

    logger.info("ğŸ”¤ GPT-2 í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: %s", tokenizer_dir)
    tokenizer = GPT2Tokenizer.from_pretrained(str(tokenizer_dir))

    # 4) ë³‘ë ¬ ì²˜ë¦¬ íŒŒë¼ë¯¸í„° ê³„ì‚°
    if workers <= 0:
        workers = max(1, (os.cpu_count() or 1) - 1)
    if chunk_size <= 0:
        chunk_size = workers * 32
    logger.info("ğŸ”§ í† í°í™” ì„¤ì •: workers=%d, chunk_size=%d", workers, chunk_size)

    # 5) í† í°í™” ì´í„°ë ˆì´í„° ìƒì„± (ë¹ˆë„ ì§‘ê³„ëŠ” í˜¸ì¶œìì—ì„œ ìˆ˜í–‰)
    encoded_chunks_iterator = iter_encoded_chunks(
        texts,
        tokenizer=tokenizer,
        workers=workers,
        chunk_size=chunk_size,
    )

    return encoded_chunks_iterator, tokenizer
