"""IVR êµì²´ í›„ë³´ í† í° ì„ ì • ëª¨ë“ˆ.

token_frequency.parquet ì™€ bpe_token_id_sequences.txt ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
ì €ë¹ˆë„ í¬ìƒ í† í°ê³¼ ê³ ë¹ˆë„ ë„ë©”ì¸ ë°”ì´ê·¸ë¨ ë³‘í•© í›„ë³´ë¥¼ ë§¤ì¹­í•œë‹¤.

ì‚°ì¶œë¬¼:
    - artifacts/analysis/reports/replacement_candidates.csv
    - artifacts/analysis/reports/selection_log.md
"""

from __future__ import annotations

import csv
from collections import Counter
from dataclasses import dataclass
from pathlib import Path
from typing import TypedDict, cast

import pyarrow.parquet as pq
from tqdm import tqdm
from transformers import GPT2Tokenizer

from gpt2_ivr.utils.logging_config import get_logger

logger = get_logger(__name__)


class SelectionResult(TypedDict):
    """ì„ ì • ê²°ê³¼ íƒ€ì…"""

    pairs_count: int
    sacrifice_count: int
    new_token_count: int
    csv_path: Path
    log_path: Path

# ---------------------------------------------------------------------------
# ë°ì´í„° êµ¬ì¡°
# ---------------------------------------------------------------------------


@dataclass(frozen=True, slots=True)
class SacrificeCandidate:
    """í¬ìƒ(ì €ë¹ˆë„) í›„ë³´ í† í°."""

    token_id: int
    token_str: str
    frequency: int


@dataclass(frozen=True, slots=True)
class NewTokenCandidate:
    """ì‹ ê·œ(ë°”ì´ê·¸ë¨ ë³‘í•©) í† í° í›„ë³´."""

    merged_str: str
    left_id: int
    right_id: int
    bigram_freq: int


@dataclass(frozen=True, slots=True)
class ReplacementPair:
    """êµì²´ í›„ë³´ ìŒ: í¬ìƒ í† í° â†’ ì‹ ê·œ í† í°."""

    rank: int
    sacrifice: SacrificeCandidate
    new_token: NewTokenCandidate
    score: float


# ---------------------------------------------------------------------------
# ìœ í‹¸ë¦¬í‹°
# ---------------------------------------------------------------------------


def load_frequency(path: Path) -> dict[int, int]:
    """token_frequency.parquet ì—ì„œ {token_id: frequency} ì‚¬ì „ì„ ë¡œë“œí•œë‹¤."""
    table = pq.read_table(path, columns=["token_id", "frequency"])
    token_ids: list[int] = table.column("token_id").to_pylist()
    frequencies: list[int] = table.column("frequency").to_pylist()
    return dict(zip(token_ids, frequencies))


def get_protected_token_ids(
    tokenizer: GPT2Tokenizer,
    min_token_len: int,
) -> set[int]:
    """ë³´í˜¸ ëŒ€ìƒ í† í° id ì§‘í•©ì„ êµ¬ì„±í•œë‹¤.

    ë³´í˜¸ ëŒ€ìƒ:
        - ìŠ¤í˜ì…œ í† í° (``<|endoftext|>`` ë“±)
        - ë””ì½”ë”© ì‹œ *min_token_len* ë¯¸ë§Œ ë¬¸ìì—´ë¡œ ë³€í™˜ë˜ëŠ” í† í°
          (ë°”ì´íŠ¸ ìˆ˜ì¤€ ë‹¨ì¼ ë¬¸ì í† í° í¬í•¨)
    """
    protected: set[int] = set()

    # ìŠ¤í˜ì…œ í† í° ë³´í˜¸
    for token_id in tokenizer.all_special_ids:
        protected.add(token_id)

    # ì§§ì€ í† í° ë³´í˜¸ (ë°”ì´íŠ¸ ìˆ˜ì¤€ í† í° í¬í•¨)
    vocab_size: int = tokenizer.vocab_size
    for token_id in range(vocab_size):
        decoded = cast(str, tokenizer.decode([token_id]))
        if len(decoded) < min_token_len:
            protected.add(token_id)

    return protected


# ---------------------------------------------------------------------------
# í¬ìƒ í›„ë³´ ì„ ì •
# ---------------------------------------------------------------------------


def select_sacrifice_candidates(
    freq: dict[int, int],
    tokenizer: GPT2Tokenizer,
    protected_ids: set[int],
    max_candidates: int,
) -> list[SacrificeCandidate]:
    """ì €ë¹ˆë„ í¬ìƒ í›„ë³´ í† í°ì„ ì„ ì •í•œë‹¤.

    ë³´í˜¸ ëŒ€ìƒì„ ì œì™¸í•œ ì „ì²´ vocab ì—ì„œ ë¹ˆë„ê°€ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬
    ìƒìœ„ *max_candidates* ê°œë¥¼ ë°˜í™˜í•œë‹¤. ë¹ˆë„ 0(ì½”í¼ìŠ¤ì— ë¯¸ì¶œí˜„)ì¸ í† í°ì´ ìµœìš°ì„ .
    """
    vocab_size: int = tokenizer.vocab_size
    candidates: list[SacrificeCandidate] = []

    for token_id in range(vocab_size):
        if token_id in protected_ids:
            continue
        token_str = cast(str, tokenizer.decode([token_id]))
        frequency = freq.get(token_id, 0)
        candidates.append(SacrificeCandidate(token_id, token_str, frequency))

    # ë¹ˆë„ ì˜¤ë¦„ì°¨ìˆœ â†’ ë™ì¼ ë¹ˆë„ ì‹œ token_id ì˜¤ë¦„ì°¨ìˆœ
    candidates.sort(key=lambda c: (c.frequency, c.token_id))
    return candidates[:max_candidates]


# ---------------------------------------------------------------------------
# ë°”ì´ê·¸ë¨ ê¸°ë°˜ ì‹ ê·œ í† í° í›„ë³´ íƒìƒ‰
# ---------------------------------------------------------------------------


def count_bigrams(
    sequences_path: Path,
    logger: logging.Logger,
) -> Counter[tuple[int, int]]:
    """bpe_token_id_sequences.txt ì—ì„œ ì¸ì ‘ í† í° ë°”ì´ê·¸ë¨ ë¹ˆë„ë¥¼ ì§‘ê³„í•œë‹¤."""
    counter: Counter[tuple[int, int]] = Counter()

    file_size = sequences_path.stat().st_size
    with sequences_path.open("r", encoding="utf-8") as handle:
        with tqdm(
            total=file_size,
            desc="ğŸ” ë°”ì´ê·¸ë¨ ì§‘ê³„",
            unit="B",
            unit_scale=True,
        ) as pbar:
            for line in handle:
                pbar.update(len(line.encode("utf-8")))
                parts = line.split()
                if len(parts) < 2:
                    continue
                ids = [int(p) for p in parts]
                for i in range(len(ids) - 1):
                    counter[(ids[i], ids[i + 1])] += 1

    logger.info("ê³ ìœ  ë°”ì´ê·¸ë¨ %dê°œë¥¼ ì§‘ê³„í–ˆìŠµë‹ˆë‹¤.", len(counter))
    return counter


def discover_new_token_candidates(
    bigram_counts: Counter[tuple[int, int]],
    tokenizer: GPT2Tokenizer,
    max_candidates: int,
    logger: logging.Logger,
) -> list[NewTokenCandidate]:
    """ë°”ì´ê·¸ë¨ ë¹ˆë„ì—ì„œ ì‹ ê·œ í† í° í›„ë³´ë¥¼ ì¶”ì¶œí•œë‹¤.

    ë°”ì´ê·¸ë¨ì„ ë””ì½”ë”©í•˜ì—¬ ë³‘í•© ë¬¸ìì—´ì„ ìƒì„±í•˜ê³ ,
    í•´ë‹¹ ë¬¸ìì—´ì´ ì´ë¯¸ ë‹¨ì¼ í† í°ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ê²½ìš°ëŠ” ì œì™¸í•œë‹¤.
    """
    # ìƒìœ„ ë°”ì´ê·¸ë¨ë§Œ ê²€ì‚¬ (í•„ìš”ëŸ‰ì˜ 10 ë°° í•œë„)
    check_limit = max_candidates * 10
    top_bigrams = bigram_counts.most_common(check_limit)

    seen_merged: set[str] = set()
    candidates: list[NewTokenCandidate] = []

    for (left_id, right_id), freq in tqdm(
        top_bigrams,
        desc="ğŸ§© ì‹ ê·œ í† í° í›„ë³´ íƒìƒ‰",
        unit="ìŒ",
    ):
        if len(candidates) >= max_candidates:
            break

        # ë°”ì´ê·¸ë¨ ë””ì½”ë”© â†’ ë³‘í•© ë¬¸ìì—´
        merged_str = cast(str, tokenizer.decode([left_id, right_id]))

        # ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” ê³µë°±ë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
        if not merged_str.strip():
            continue

        # ë™ì¼ ë³‘í•© ë¬¸ìì—´ ì¤‘ë³µ ì œì™¸
        if merged_str in seen_merged:
            continue

        # ì´ë¯¸ ë‹¨ì¼ í† í°ìœ¼ë¡œ ì¡´ì¬í•˜ë©´ ì œì™¸
        encoded: list[int] = tokenizer.encode(merged_str, add_special_tokens=False)
        if len(encoded) <= 1:
            continue

        seen_merged.add(merged_str)
        candidates.append(
            NewTokenCandidate(
                merged_str=merged_str,
                left_id=left_id,
                right_id=right_id,
                bigram_freq=freq,
            )
        )

    logger.info("ì‹ ê·œ í† í° í›„ë³´ %dê°œë¥¼ ì„ ì •í–ˆìŠµë‹ˆë‹¤.", len(candidates))
    return candidates


# ---------------------------------------------------------------------------
# ë§¤ì¹­ ë° ì¶œë ¥
# ---------------------------------------------------------------------------


def match_candidates(
    sacrifices: list[SacrificeCandidate],
    new_tokens: list[NewTokenCandidate],
) -> list[ReplacementPair]:
    """í¬ìƒ í›„ë³´ì™€ ì‹ ê·œ í† í° í›„ë³´ë¥¼ 1:1 ìˆœìœ„ ë§¤ì¹­í•œë‹¤.

    ì ìˆ˜ = ``bigram_freq / (sacrifice_freq + 1)`` â€” ë†’ì„ìˆ˜ë¡ êµì²´ ê°€ì¹˜ê°€ í¬ë‹¤.
    """
    count = min(len(sacrifices), len(new_tokens))
    pairs: list[ReplacementPair] = []

    for i in range(count):
        sacrifice = sacrifices[i]
        new_token = new_tokens[i]
        score = new_token.bigram_freq / (sacrifice.frequency + 1)
        pairs.append(
            ReplacementPair(
                rank=i + 1,
                sacrifice=sacrifice,
                new_token=new_token,
                score=score,
            )
        )

    return pairs


def write_replacement_csv(
    pairs: list[ReplacementPair],
    output_path: Path,
) -> None:
    """replacement_candidates.csv ë¥¼ ì €ì¥í•œë‹¤."""
    output_path.parent.mkdir(parents=True, exist_ok=True)

    fieldnames = [
        "rank",
        "sacrifice_token_id",
        "sacrifice_token",
        "sacrifice_freq",
        "new_token",
        "new_token_left_id",
        "new_token_right_id",
        "bigram_freq",
        "score",
    ]

    with output_path.open("w", newline="", encoding="utf-8") as handle:
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        writer.writeheader()
        for pair in pairs:
            writer.writerow(
                {
                    "rank": pair.rank,
                    "sacrifice_token_id": pair.sacrifice.token_id,
                    "sacrifice_token": pair.sacrifice.token_str,
                    "sacrifice_freq": pair.sacrifice.frequency,
                    "new_token": pair.new_token.merged_str,
                    "new_token_left_id": pair.new_token.left_id,
                    "new_token_right_id": pair.new_token.right_id,
                    "bigram_freq": pair.new_token.bigram_freq,
                    "score": f"{pair.score:.4f}",
                }
            )


def write_selection_log(
    pairs: list[ReplacementPair],
    total_vocab: int,
    total_protected: int,
    total_sacrifice_pool: int,
    total_bigrams: int,
    output_path: Path,
) -> None:
    """selection_log.md ë¥¼ ì €ì¥í•œë‹¤."""
    output_path.parent.mkdir(parents=True, exist_ok=True)

    lines: list[str] = [
        "# IVR êµì²´ í›„ë³´ ì„ ì • ë¡œê·¸\n",
        "",
        "## ìš”ì•½ í†µê³„\n",
        "",
        "| í•­ëª© | ê°’ |",
        "|------|------|",
        f"| ì „ì²´ vocab í¬ê¸° | {total_vocab:,} |",
        f"| ë³´í˜¸ í† í° ìˆ˜ | {total_protected:,} |",
        f"| í¬ìƒ í›„ë³´ í’€ í¬ê¸° | {total_sacrifice_pool:,} |",
        f"| ê³ ìœ  ë°”ì´ê·¸ë¨ ìˆ˜ | {total_bigrams:,} |",
        f"| ìµœì¢… êµì²´ í›„ë³´ ìŒ | {len(pairs):,} |",
        "",
        "## ì„ ì • ê¸°ì¤€\n",
        "",
        "- **í¬ìƒ í›„ë³´**: ë³´í˜¸ ëŒ€ìƒ ì œì™¸ í›„ ì½”í¼ìŠ¤ ë¹ˆë„ê°€ ê°€ì¥ ë‚®ì€ í† í°",
        "- **ì‹ ê·œ í›„ë³´**: ì¸ì ‘ í† í° ë°”ì´ê·¸ë¨ ë¹ˆë„ê°€ ê°€ì¥ ë†’ìœ¼ë©´ì„œ "
        "ë‹¨ì¼ í† í°ìœ¼ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë³‘í•© ë¬¸ìì—´",
        "- **ì ìˆ˜**: `bigram_freq / (sacrifice_freq + 1)` â€” ë†’ì„ìˆ˜ë¡ êµì²´ ê°€ì¹˜ê°€ í¼",
        "",
        "## ìƒìœ„ êµì²´ í›„ë³´\n",
        "",
        "| ìˆœìœ„ | í¬ìƒ í† í° (id) | í¬ìƒ ë¹ˆë„ | ì‹ ê·œ í† í° | ë°”ì´ê·¸ë¨ ë¹ˆë„ | ì ìˆ˜ |",
        "|------|----------------|-----------|-----------|---------------|------|",
    ]

    display_count = min(len(pairs), 50)
    for pair in pairs[:display_count]:
        # ë§ˆí¬ë‹¤ìš´ íŒŒì´í”„ ì´ìŠ¤ì¼€ì´í”„
        sac_str = pair.sacrifice.token_str.replace("|", "\\|")
        new_str = pair.new_token.merged_str.replace("|", "\\|")
        lines.append(
            f"| {pair.rank} "
            f"| `{sac_str}` ({pair.sacrifice.token_id}) "
            f"| {pair.sacrifice.frequency:,} "
            f"| `{new_str}` "
            f"| {pair.new_token.bigram_freq:,} "
            f"| {pair.score:,.2f} |"
        )

    if len(pairs) > display_count:
        lines.append("")
        lines.append(f"> ì „ì²´ {len(pairs)}ìŒ ì¤‘ ìƒìœ„ {display_count}ìŒë§Œ í‘œì‹œí•©ë‹ˆë‹¤.")

    lines.append("")

    with output_path.open("w", encoding="utf-8") as handle:
        handle.write("\n".join(lines))


def select_replacement_candidates(
    frequency_path: Path,
    sequences_path: Path,
    output_csv: Path,
    output_log: Path,
    model_name: str = "openai-community/gpt2",
    max_candidates: int = 1000,
    min_token_len: int = 2,
) -> SelectionResult:
    """IVR êµì²´ í›„ë³´ë¥¼ ì„ ì •í•œë‹¤.

    Args:
        frequency_path: í† í° ë¹ˆë„ parquet íŒŒì¼ ê²½ë¡œ
        sequences_path: BPE í† í° ì‹œí€€ìŠ¤ íŒŒì¼ ê²½ë¡œ
        output_csv: êµì²´ í›„ë³´ CSV ì €ì¥ ê²½ë¡œ
        output_log: ì„ ì • ë¡œê·¸ ì €ì¥ ê²½ë¡œ
        model_name: ì‚¬ìš©í•  í† í¬ë‚˜ì´ì € ëª¨ë¸ëª…
        max_candidates: ìµœëŒ€ í›„ë³´ ê°œìˆ˜
        min_token_len: ë³´í˜¸ í† í° ìµœì†Œ ê¸¸ì´

    Returns:
        ì„ ì • ê²°ê³¼ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬

    Raises:
        FileNotFoundError: ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
    """
    # ì…ë ¥ íŒŒì¼ ê²€ì¦
    if not frequency_path.exists():
        raise FileNotFoundError(f"í† í° ë¹ˆë„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {frequency_path}")
    if not sequences_path.exists():
        raise FileNotFoundError(f"í† í° ì‹œí€€ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {sequences_path}")

    # 1) ë¹ˆë„ ë¡œë“œ
    logger.info("ğŸ“Š í† í° ë¹ˆë„ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: %s", frequency_path)
    freq = load_frequency(frequency_path)
    logger.info("ë¹ˆë„ ë°ì´í„° ë¡œë“œ ì™„ë£Œ (ê³ ìœ  í† í° %dê°œ)", len(freq))

    # 2) í† í¬ë‚˜ì´ì € ë¡œë“œ
    logger.info("ğŸ”¤ GPT-2 í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: %s", model_name)
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)

    # 3) ë³´í˜¸ í† í° ì§‘í•© êµ¬ì„±
    protected_ids = get_protected_token_ids(tokenizer, min_token_len)
    logger.info("ğŸ›¡ï¸ ë³´í˜¸ ëŒ€ìƒ í† í° %dê°œë¥¼ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.", len(protected_ids))

    # 4) í¬ìƒ í›„ë³´ ì„ ì •
    logger.info("ğŸ“‰ í¬ìƒ í›„ë³´ë¥¼ ì„ ì •í•©ë‹ˆë‹¤ (ìµœëŒ€ %dê°œ)...", max_candidates)
    sacrifices = select_sacrifice_candidates(
        freq, tokenizer, protected_ids, max_candidates
    )
    logger.info("í¬ìƒ í›„ë³´ %dê°œë¥¼ ì„ ì •í–ˆìŠµë‹ˆë‹¤.", len(sacrifices))
    if sacrifices:
        zero_freq_count = sum(1 for s in sacrifices if s.frequency == 0)
        logger.info("  â””â”€ ë¯¸ì¶œí˜„(ë¹ˆë„ 0) í† í°: %dê°œ", zero_freq_count)

    # 5) ë°”ì´ê·¸ë¨ ì§‘ê³„
    logger.info("ğŸ” ì¸ì ‘ í† í° ë°”ì´ê·¸ë¨ì„ ì§‘ê³„í•©ë‹ˆë‹¤...")
    bigram_counts = count_bigrams(sequences_path, logger)

    # 6) ì‹ ê·œ í† í° í›„ë³´ íƒìƒ‰
    logger.info("ğŸ§© ì‹ ê·œ í† í° í›„ë³´ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤ (ìµœëŒ€ %dê°œ)...", max_candidates)
    new_tokens = discover_new_token_candidates(
        bigram_counts, tokenizer, max_candidates, logger
    )

    # 7) ë§¤ì¹­
    pairs = match_candidates(sacrifices, new_tokens)
    logger.info("âœ… êµì²´ í›„ë³´ %dìŒì„ ë§¤ì¹­í–ˆìŠµë‹ˆë‹¤.", len(pairs))

    # 8) ê²°ê³¼ ì €ì¥
    write_replacement_csv(pairs, output_csv)
    logger.info("ğŸ“„ êµì²´ í›„ë³´ CSV ì €ì¥ ì™„ë£Œ: %s", output_csv)

    write_selection_log(
        pairs=pairs,
        total_vocab=tokenizer.vocab_size,
        total_protected=len(protected_ids),
        total_sacrifice_pool=tokenizer.vocab_size - len(protected_ids),
        total_bigrams=len(bigram_counts),
        output_path=output_log,
    )
    logger.info("ğŸ“ ì„ ì • ë¡œê·¸ ì €ì¥ ì™„ë£Œ: %s", output_log)

    return SelectionResult(
        pairs_count=len(pairs),
        sacrifice_count=len(sacrifices),
        new_token_count=len(new_tokens),
        csv_path=output_csv,
        log_path=output_log,
    )

