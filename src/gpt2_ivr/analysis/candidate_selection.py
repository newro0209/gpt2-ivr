"""IVR êµì²´ í›„ë³´ í† í° ì„ ì • ëª¨ë“ˆ.

token_frequency.parquet ì™€ bpe_token_id_sequences.txt ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
ì €ë¹ˆë„ í¬ìƒ í† í°ê³¼ ê³ ë¹ˆë„ ë„ë©”ì¸ ë°”ì´ê·¸ë¨ ë³‘í•© í›„ë³´ë¥¼ ë§¤ì¹­í•œë‹¤.
"""

from __future__ import annotations

import csv
from collections import Counter
from dataclasses import dataclass
import logging
from pathlib import Path
from typing import TypedDict, cast

import pyarrow.parquet as pq
from transformers import GPT2Tokenizer

logger = logging.getLogger(__name__)


class SelectionResult(TypedDict):
    """ì„ ì • ê²°ê³¼ íƒ€ì….

    Attributes:
        pairs_count: êµì²´ í›„ë³´ ìŒ ê°œìˆ˜
        sacrifice_count: í¬ìƒ í›„ë³´ ê°œìˆ˜
        new_token_count: ì‹ ê·œ í† í° í›„ë³´ ê°œìˆ˜
        csv_path: CSV íŒŒì¼ ê²½ë¡œ
        log_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    """

    pairs_count: int
    sacrifice_count: int
    new_token_count: int
    csv_path: Path
    log_path: Path


@dataclass(frozen=True, slots=True)
class SacrificeCandidate:
    """í¬ìƒ(ì €ë¹ˆë„) í›„ë³´ í† í°.

    Attributes:
        token_id: í† í° ID
        token_str: í† í° ë¬¸ìì—´ (ë””ì½”ë”© ê²°ê³¼)
        frequency: ì½”í¼ìŠ¤ ì¶œí˜„ ë¹ˆë„
    """

    token_id: int
    token_str: str
    frequency: int


@dataclass(frozen=True, slots=True)
class NewTokenCandidate:
    """ì‹ ê·œ(ë°”ì´ê·¸ë¨ ë³‘í•©) í† í° í›„ë³´.

    Attributes:
        merged_str: ë³‘í•©ëœ ë¬¸ìì—´
        left_id: ì™¼ìª½ í† í° ID
        right_id: ì˜¤ë¥¸ìª½ í† í° ID
        bigram_freq: ë°”ì´ê·¸ë¨ ì¶œí˜„ ë¹ˆë„
    """

    merged_str: str
    left_id: int
    right_id: int
    bigram_freq: int


@dataclass(frozen=True, slots=True)
class ReplacementPair:
    """êµì²´ í›„ë³´ ìŒ: í¬ìƒ í† í° â†’ ì‹ ê·œ í† í°.

    Attributes:
        rank: ìˆœìœ„ (1ë¶€í„° ì‹œì‘)
        sacrifice: í¬ìƒ í›„ë³´ í† í°
        new_token: ì‹ ê·œ í† í° í›„ë³´
        score: êµì²´ ê°€ì¹˜ ì ìˆ˜
    """

    rank: int
    sacrifice: SacrificeCandidate
    new_token: NewTokenCandidate
    score: float


def load_frequency(path: Path) -> dict[int, int]:
    """token_frequency.parquet ì—ì„œ {token_id: frequency} ì‚¬ì „ì„ ë¡œë“œí•œë‹¤.

    Args:
        path: Parquet íŒŒì¼ ê²½ë¡œ

    Returns:
        í† í° IDë¥¼ í‚¤ë¡œ, ë¹ˆë„ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
    """
    table = pq.read_table(path, columns=["token_id", "frequency"])
    token_ids: list[int] = table.column("token_id").to_pylist()
    frequencies: list[int] = table.column("frequency").to_pylist()
    return dict(zip(token_ids, frequencies))


def get_protected_token_ids(
    tokenizer: GPT2Tokenizer,
    min_token_len: int,
) -> set[int]:
    """ë³´í˜¸ ëŒ€ìƒ í† í° id ì§‘í•©ì„ êµ¬ì„±í•œë‹¤.

    ë³´í˜¸ ëŒ€ìƒ:
        - ìŠ¤í˜ì…œ í† í° (``<|endoftext|>`` ë“±)
        - ë””ì½”ë”© ì‹œ *min_token_len* ë¯¸ë§Œ ë¬¸ìì—´ë¡œ ë³€í™˜ë˜ëŠ” í† í°
          (ë°”ì´íŠ¸ ìˆ˜ì¤€ ë‹¨ì¼ ë¬¸ì í† í° í¬í•¨)

    Args:
        tokenizer: GPT-2 í† í¬ë‚˜ì´ì €
        min_token_len: ë³´í˜¸ ëŒ€ìƒ ìµœì†Œ ê¸¸ì´ (ì´ ê¸¸ì´ ë¯¸ë§Œì€ ë³´í˜¸)

    Returns:
        ë³´í˜¸ ëŒ€ìƒ í† í° ID ì§‘í•©
    """
    protected: set[int] = set()

    # 1) ìŠ¤í˜ì…œ í† í° ë³´í˜¸
    for token_id in tokenizer.all_special_ids:
        protected.add(token_id)

    # 2) ì§§ì€ í† í° ë³´í˜¸ (ë°”ì´íŠ¸ ìˆ˜ì¤€ í† í° í¬í•¨)
    vocab_size: int = tokenizer.vocab_size
    for token_id in range(vocab_size):
        decoded = cast(str, tokenizer.decode([token_id]))
        if len(decoded) < min_token_len:
            protected.add(token_id)

    return protected


def select_sacrifice_candidates(
    freq: dict[int, int],
    tokenizer: GPT2Tokenizer,
    protected_ids: set[int],
    max_candidates: int,
) -> list[SacrificeCandidate]:
    """ì €ë¹ˆë„ í¬ìƒ í›„ë³´ í† í°ì„ ì„ ì •í•œë‹¤.

    ë³´í˜¸ ëŒ€ìƒì„ ì œì™¸í•œ ì „ì²´ vocab ì—ì„œ ë¹ˆë„ê°€ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬
    ìƒìœ„ *max_candidates* ê°œë¥¼ ë°˜í™˜í•œë‹¤. ë¹ˆë„ 0(ì½”í¼ìŠ¤ì— ë¯¸ì¶œí˜„)ì¸ í† í°ì´ ìµœìš°ì„ .

    Args:
        freq: í† í° IDë³„ ë¹ˆë„ ë”•ì…”ë„ˆë¦¬
        tokenizer: GPT-2 í† í¬ë‚˜ì´ì €
        protected_ids: ë³´í˜¸ ëŒ€ìƒ í† í° ID ì§‘í•©
        max_candidates: ìµœëŒ€ í›„ë³´ ê°œìˆ˜

    Returns:
        ë¹ˆë„ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ëœ í¬ìƒ í›„ë³´ ë¦¬ìŠ¤íŠ¸
    """
    vocab_size: int = tokenizer.vocab_size
    candidates: list[SacrificeCandidate] = []

    for token_id in range(vocab_size):
        if token_id in protected_ids:
            continue
        token_str = cast(str, tokenizer.decode([token_id]))
        frequency = freq.get(token_id, 0)
        candidates.append(SacrificeCandidate(token_id, token_str, frequency))

    # ë¹ˆë„ ì˜¤ë¦„ì°¨ìˆœ â†’ ë™ì¼ ë¹ˆë„ ì‹œ token_id ì˜¤ë¦„ì°¨ìˆœ
    candidates.sort(key=lambda c: (c.frequency, c.token_id))
    return candidates[:max_candidates]


def count_bigrams(
    sequences_path: Path,
    logger: logging.Logger,
) -> Counter[tuple[int, int]]:
    """bpe_token_id_sequences.txt ì—ì„œ ì¸ì ‘ í† í° ë°”ì´ê·¸ë¨ ë¹ˆë„ë¥¼ ì§‘ê³„í•œë‹¤.

    Args:
        sequences_path: í† í° ì‹œí€€ìŠ¤ íŒŒì¼ ê²½ë¡œ
        logger: ë¡œê±° ì¸ìŠ¤í„´ìŠ¤

    Returns:
        (left_id, right_id) íŠœí”Œì„ í‚¤ë¡œ í•˜ëŠ” ë¹ˆë„ ì¹´ìš´í„°
    """
    counter: Counter[tuple[int, int]] = Counter()

    with sequences_path.open("r", encoding="utf-8") as handle:
        for line in handle:
            parts = line.split()
            if len(parts) < 2:
                continue
            ids = [int(p) for p in parts]
            for i in range(len(ids) - 1):
                counter[(ids[i], ids[i + 1])] += 1

    logger.info("ê³ ìœ  ë°”ì´ê·¸ë¨ %dê°œë¥¼ ì§‘ê³„í–ˆìŠµë‹ˆë‹¤.", len(counter))
    return counter


def discover_new_token_candidates(
    bigram_counts: Counter[tuple[int, int]],
    tokenizer: GPT2Tokenizer,
    max_candidates: int,
    logger: logging.Logger,
) -> list[NewTokenCandidate]:
    """ë°”ì´ê·¸ë¨ ë¹ˆë„ì—ì„œ ì‹ ê·œ í† í° í›„ë³´ë¥¼ ì¶”ì¶œí•œë‹¤.

    ë°”ì´ê·¸ë¨ì„ ë””ì½”ë”©í•˜ì—¬ ë³‘í•© ë¬¸ìì—´ì„ ìƒì„±í•˜ê³ ,
    í•´ë‹¹ ë¬¸ìì—´ì´ ì´ë¯¸ ë‹¨ì¼ í† í°ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ê²½ìš°ëŠ” ì œì™¸í•œë‹¤.

    Args:
        bigram_counts: ë°”ì´ê·¸ë¨ ë¹ˆë„ ì¹´ìš´í„°
        tokenizer: GPT-2 í† í¬ë‚˜ì´ì €
        max_candidates: ìµœëŒ€ í›„ë³´ ê°œìˆ˜
        logger: ë¡œê±° ì¸ìŠ¤í„´ìŠ¤

    Returns:
        ë¹ˆë„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ëœ ì‹ ê·œ í† í° í›„ë³´ ë¦¬ìŠ¤íŠ¸
    """
    check_limit = max_candidates * 10
    top_bigrams = bigram_counts.most_common(check_limit)

    seen_merged: set[str] = set()
    candidates: list[NewTokenCandidate] = []

    # 1) ìƒìœ„ ë°”ì´ê·¸ë¨ ìˆœíšŒí•˜ë©° í›„ë³´ ì„ ì •
    # 2) ë³‘í•© ë¬¸ìì—´ì´ ë¹„ì–´ìˆê±°ë‚˜ ì¤‘ë³µì´ê±°ë‚˜ ì´ë¯¸ ë‹¨ì¼ í† í°ì¸ ê²½ìš° ì œì™¸
    for (left_id, right_id), freq in top_bigrams:
        if len(candidates) >= max_candidates:
            break

        merged_str = cast(str, tokenizer.decode([left_id, right_id]))

        if not merged_str.strip():
            continue

        if merged_str in seen_merged:
            continue

        encoded: list[int] = tokenizer.encode(merged_str, add_special_tokens=False)
        if len(encoded) <= 1:
            continue

        seen_merged.add(merged_str)
        candidates.append(
            NewTokenCandidate(
                merged_str=merged_str,
                left_id=left_id,
                right_id=right_id,
                bigram_freq=freq,
            )
        )

    logger.info("ì‹ ê·œ í† í° í›„ë³´ %dê°œë¥¼ ì„ ì •í–ˆìŠµë‹ˆë‹¤.", len(candidates))
    return candidates


def match_candidates(
    sacrifices: list[SacrificeCandidate],
    new_tokens: list[NewTokenCandidate],
) -> list[ReplacementPair]:
    """í¬ìƒ í›„ë³´ì™€ ì‹ ê·œ í† í° í›„ë³´ë¥¼ 1:1 ìˆœìœ„ ë§¤ì¹­í•œë‹¤.

    ì ìˆ˜ = ``bigram_freq / (sacrifice_freq + 1)`` â€” ë†’ì„ìˆ˜ë¡ êµì²´ ê°€ì¹˜ê°€ í¬ë‹¤.

    Args:
        sacrifices: í¬ìƒ í›„ë³´ ë¦¬ìŠ¤íŠ¸ (ë¹ˆë„ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬)
        new_tokens: ì‹ ê·œ í† í° í›„ë³´ ë¦¬ìŠ¤íŠ¸ (ë¹ˆë„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)

    Returns:
        êµì²´ í›„ë³´ ìŒ ë¦¬ìŠ¤íŠ¸
    """
    count = min(len(sacrifices), len(new_tokens))
    pairs: list[ReplacementPair] = []

    for i in range(count):
        sacrifice = sacrifices[i]
        new_token = new_tokens[i]
        score = new_token.bigram_freq / (sacrifice.frequency + 1)
        pairs.append(
            ReplacementPair(
                rank=i + 1,
                sacrifice=sacrifice,
                new_token=new_token,
                score=score,
            )
        )

    return pairs


def write_replacement_csv(
    pairs: list[ReplacementPair],
    output_path: Path,
) -> None:
    """replacement_candidates.csv ë¥¼ ì €ì¥í•œë‹¤.

    Args:
        pairs: êµì²´ í›„ë³´ ìŒ ë¦¬ìŠ¤íŠ¸
        output_path: CSV íŒŒì¼ ì €ì¥ ê²½ë¡œ
    """
    output_path.parent.mkdir(parents=True, exist_ok=True)

    fieldnames = [
        "rank",
        "sacrifice_token_id",
        "sacrifice_token",
        "sacrifice_freq",
        "new_token",
        "new_token_left_id",
        "new_token_right_id",
        "bigram_freq",
        "score",
    ]

    with output_path.open("w", newline="", encoding="utf-8") as handle:
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        writer.writeheader()
        for pair in pairs:
            writer.writerow(
                {
                    "rank": pair.rank,
                    "sacrifice_token_id": pair.sacrifice.token_id,
                    "sacrifice_token": pair.sacrifice.token_str,
                    "sacrifice_freq": pair.sacrifice.frequency,
                    "new_token": pair.new_token.merged_str,
                    "new_token_left_id": pair.new_token.left_id,
                    "new_token_right_id": pair.new_token.right_id,
                    "bigram_freq": pair.new_token.bigram_freq,
                    "score": f"{pair.score:.4f}",
                }
            )


def write_selection_log(
    pairs: list[ReplacementPair],
    total_vocab: int,
    total_protected: int,
    total_sacrifice_pool: int,
    total_bigrams: int,
    output_path: Path,
) -> None:
    """selection_log.md ë¥¼ ì €ì¥í•œë‹¤.

    Args:
        pairs: êµì²´ í›„ë³´ ìŒ ë¦¬ìŠ¤íŠ¸
        total_vocab: ì „ì²´ ì–´íœ˜ í¬ê¸°
        total_protected: ë³´í˜¸ í† í° ê°œìˆ˜
        total_sacrifice_pool: í¬ìƒ í›„ë³´ í’€ í¬ê¸°
        total_bigrams: ê³ ìœ  ë°”ì´ê·¸ë¨ ê°œìˆ˜
        output_path: ë§ˆí¬ë‹¤ìš´ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    """
    output_path.parent.mkdir(parents=True, exist_ok=True)

    lines: list[str] = [
        "# IVR êµì²´ í›„ë³´ ì„ ì • ë¡œê·¸\n",
        "",
        "## ìš”ì•½ í†µê³„\n",
        "",
        "| í•­ëª© | ê°’ |",
        "|------|------|",
        f"| ì „ì²´ vocab í¬ê¸° | {total_vocab:,} |",
        f"| ë³´í˜¸ í† í° ìˆ˜ | {total_protected:,} |",
        f"| í¬ìƒ í›„ë³´ í’€ í¬ê¸° | {total_sacrifice_pool:,} |",
        f"| ê³ ìœ  ë°”ì´ê·¸ë¨ ìˆ˜ | {total_bigrams:,} |",
        f"| ìµœì¢… êµì²´ í›„ë³´ ìŒ | {len(pairs):,} |",
        "",
        "## ì„ ì • ê¸°ì¤€\n",
        "",
        "- **í¬ìƒ í›„ë³´**: ë³´í˜¸ ëŒ€ìƒ ì œì™¸ í›„ ì½”í¼ìŠ¤ ë¹ˆë„ê°€ ê°€ì¥ ë‚®ì€ í† í°",
        "- **ì‹ ê·œ í›„ë³´**: ì¸ì ‘ í† í° ë°”ì´ê·¸ë¨ ë¹ˆë„ê°€ ê°€ì¥ ë†’ìœ¼ë©´ì„œ "
        "ë‹¨ì¼ í† í°ìœ¼ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë³‘í•© ë¬¸ìì—´",
        "- **ì ìˆ˜**: `bigram_freq / (sacrifice_freq + 1)` â€” ë†’ì„ìˆ˜ë¡ êµì²´ ê°€ì¹˜ê°€ í¼",
        "",
        "## ìƒìœ„ êµì²´ í›„ë³´\n",
        "",
        "| ìˆœìœ„ | í¬ìƒ í† í° (id) | í¬ìƒ ë¹ˆë„ | ì‹ ê·œ í† í° | ë°”ì´ê·¸ë¨ ë¹ˆë„ | ì ìˆ˜ |",
        "|------|----------------|-----------|-----------|---------------|------|",
    ]

    display_count = min(len(pairs), 50)
    for pair in pairs[:display_count]:
        sac_str = pair.sacrifice.token_str.replace("|", "\\|")
        new_str = pair.new_token.merged_str.replace("|", "\\|")
        lines.append(
            f"| {pair.rank} "
            f"| `{sac_str}` ({pair.sacrifice.token_id}) "
            f"| {pair.sacrifice.frequency:,} "
            f"| `{new_str}` "
            f"| {pair.new_token.bigram_freq:,} "
            f"| {pair.score:,.2f} |"
        )

    if len(pairs) > display_count:
        lines.append("")
        lines.append(f"> ì „ì²´ {len(pairs)}ìŒ ì¤‘ ìƒìœ„ {display_count}ìŒë§Œ í‘œì‹œí•©ë‹ˆë‹¤.")

    lines.append("")

    with output_path.open("w", encoding="utf-8") as handle:
        handle.write("\n".join(lines))


def select_replacement_candidates(
    frequency_path: Path,
    sequences_path: Path,
    output_csv: Path,
    output_log: Path,
    tokenizer_dir: Path,
    max_candidates: int,
    min_token_len: int,
) -> tuple[
    Counter[tuple[int, int]],
    list[NewTokenCandidate],
    GPT2Tokenizer,
    list[SacrificeCandidate],
    list[ReplacementPair],
]:
    """IVR êµì²´ í›„ë³´ë¥¼ ì„ ì •í•œë‹¤.

    Args:
        frequency_path: í† í° ë¹ˆë„ parquet íŒŒì¼ ê²½ë¡œ
        sequences_path: BPE í† í° ì‹œí€€ìŠ¤ íŒŒì¼ ê²½ë¡œ
        output_csv: êµì²´ í›„ë³´ CSV ì €ì¥ ê²½ë¡œ (SelectCommandì—ì„œ ì‚¬ìš©)
        output_log: ì„ ì • ë¡œê·¸ ì €ì¥ ê²½ë¡œ (SelectCommandì—ì„œ ì‚¬ìš©)
        tokenizer_dir: ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬
        max_candidates: ìµœëŒ€ í›„ë³´ ê°œìˆ˜
        min_token_len: ë³´í˜¸ í† í° ìµœì†Œ ê¸¸ì´

    Returns:
        í•„ìš”í•œ ì§‘ê³„ ë°ì´í„° íŠœí”Œ: bigram_counts, new_tokens, tokenizer, sacrifices, pairs

    Raises:
        FileNotFoundError: ì…ë ¥ íŒŒì¼ ë˜ëŠ” ì›ë³¸ í† í¬ë‚˜ì´ì €ê°€ ì—†ëŠ” ê²½ìš°
    """
    # ì…ë ¥ íŒŒì¼ ê²€ì¦
    if not frequency_path.exists():
        raise FileNotFoundError(f"í† í° ë¹ˆë„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {frequency_path}")
    if not sequences_path.exists():
        raise FileNotFoundError(f"í† í° ì‹œí€€ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {sequences_path}")

    # 1) ë¹ˆë„ ë¡œë“œ
    logger.info("ğŸ“Š í† í° ë¹ˆë„ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: %s", frequency_path)
    freq = load_frequency(frequency_path)
    logger.info("ë¹ˆë„ ë°ì´í„° ë¡œë“œ ì™„ë£Œ (ê³ ìœ  í† í° %dê°œ)", len(freq))

    # 2) í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer_files = list(tokenizer_dir.glob("*")) if tokenizer_dir.exists() else []
    has_tokenizer_files = any(
        f.name in ["tokenizer.json", "vocab.json", "merges.txt"]
        for f in tokenizer_files
    )
    if not has_tokenizer_files:
        raise FileNotFoundError(f"ì›ë³¸ í† í¬ë‚˜ì´ì € íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {tokenizer_dir}")

    logger.info("ğŸ”¤ GPT-2 í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: %s", tokenizer_dir)
    tokenizer = GPT2Tokenizer.from_pretrained(str(tokenizer_dir))

    # 3) ë³´í˜¸ í† í° ì§‘í•© êµ¬ì„±
    protected_ids = get_protected_token_ids(tokenizer, min_token_len)
    logger.info("ğŸ›¡ï¸ ë³´í˜¸ ëŒ€ìƒ í† í° %dê°œë¥¼ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.", len(protected_ids))

    # 4) í¬ìƒ í›„ë³´ ì„ ì •
    logger.info("ğŸ“‰ í¬ìƒ í›„ë³´ë¥¼ ì„ ì •í•©ë‹ˆë‹¤ (ìµœëŒ€ %dê°œ)...", max_candidates)
    sacrifices = select_sacrifice_candidates(
        freq, tokenizer, protected_ids, max_candidates
    )
    logger.info("í¬ìƒ í›„ë³´ %dê°œë¥¼ ì„ ì •í–ˆìŠµë‹ˆë‹¤.", len(sacrifices))
    if sacrifices:
        zero_freq_count = sum(1 for s in sacrifices if s.frequency == 0)
        logger.info("  â””â”€ ë¯¸ì¶œí˜„(ë¹ˆë„ 0) í† í°: %dê°œ", zero_freq_count)

    # 5) ë°”ì´ê·¸ë¨ ì§‘ê³„
    logger.info("ğŸ” ì¸ì ‘ í† í° ë°”ì´ê·¸ë¨ì„ ì§‘ê³„í•©ë‹ˆë‹¤...")
    bigram_counts = count_bigrams(sequences_path, logger)

    # 6) ì‹ ê·œ í† í° í›„ë³´ íƒìƒ‰
    logger.info("ğŸ§© ì‹ ê·œ í† í° í›„ë³´ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤ (ìµœëŒ€ %dê°œ)...", max_candidates)
    new_tokens = discover_new_token_candidates(
        bigram_counts, tokenizer, max_candidates, logger
    )

    # 7) ë§¤ì¹­
    pairs = match_candidates(sacrifices, new_tokens)
    logger.info("âœ… êµì²´ í›„ë³´ %dìŒì„ ë§¤ì¹­í–ˆìŠµë‹ˆë‹¤.", len(pairs))

    return (
        bigram_counts,
        new_tokens,
        tokenizer,
        sacrifices,
        pairs,
    )
