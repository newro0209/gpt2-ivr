"""IVR êµì²´ í›„ë³´ ì„ ì • ì»¤ë§¨ë“œ.

í† í° ë¹ˆë„ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì €ë¹ˆë„ í¬ìƒ í† í°ê³¼ ê³ ë¹ˆë„ ë°”ì´ê·¸ë¨ ë³‘í•© í›„ë³´ë¥¼
ë§¤ì¹­í•˜ì—¬ IVR(Infrequent Vocabulary Replacement) êµì²´ í›„ë³´ë¥¼ ì„ ì •í•œë‹¤.
"""

from __future__ import annotations

import logging
from pathlib import Path
from typing import Any

from rich.progress import track

from gpt2_ivr.analysis.candidate_selection import (
    select_replacement_candidates,
    write_replacement_csv,
    write_selection_log,
)

from .base import Command

logger = logging.getLogger(__name__)


class SelectCommand(Command):
    """IVR êµì²´ í›„ë³´ ì„ ì • ì»¤ë§¨ë“œ.

    ì €ë¹ˆë„ í† í°ì„ í¬ìƒ í›„ë³´ë¡œ, ê³ ë¹ˆë„ ë°”ì´ê·¸ë¨ì„ ì‹ ê·œ í† í° í›„ë³´ë¡œ ì„ ì •í•˜ì—¬
    êµì²´ ìŒì„ ìƒì„±í•œë‹¤.

    Attributes:
        frequency_path: í† í° ë¹ˆë„ parquet íŒŒì¼ ê²½ë¡œ
        sequences_path: BPE í† í° ì‹œí€€ìŠ¤ íŒŒì¼ ê²½ë¡œ
        output_csv: êµì²´ í›„ë³´ CSV ì €ì¥ ê²½ë¡œ
        output_log: ì„ ì • ë¡œê·¸ ì €ì¥ ê²½ë¡œ
        tokenizer_dir: ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬
        max_candidates: ìµœëŒ€ í›„ë³´ ê°œìˆ˜
        min_token_len: ë³´í˜¸ í† í° ìµœì†Œ ê¸¸ì´
    """

    def __init__(
        self,
        frequency_path: Path,
        sequences_path: Path,
        output_csv: Path,
        output_log: Path,
        tokenizer_dir: Path,
        max_candidates: int,
        min_token_len: int,
    ):
        self.frequency_path = frequency_path
        self.sequences_path = sequences_path
        self.output_csv = output_csv
        self.output_log = output_log
        self.tokenizer_dir = tokenizer_dir
        self.max_candidates = max_candidates
        self.min_token_len = min_token_len

    def execute(self, **kwargs: Any) -> dict[str, Any]:
        """êµì²´ í›„ë³´ ì„ ì •ì„ ì‹¤í–‰í•œë‹¤.

        í† í° ë¹ˆë„ì™€ ë°”ì´ê·¸ë¨ í†µê³„ë¥¼ ë¶„ì„í•˜ì—¬ êµì²´ í›„ë³´ ìŒì„ ìƒì„±í•˜ê³ 
        CSVì™€ ë§ˆí¬ë‹¤ìš´ ë¡œê·¸ë¡œ ì €ì¥í•œë‹¤.

        Args:
            **kwargs: ì‚¬ìš©ë˜ì§€ ì•ŠìŒ

        Returns:
            ì„ ì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (pairs_count, csv_path, log_path, sacrifice_count, new_token_count)
        """
        (
            bigram_counts,
            new_tokens_list,
            tokenizer,
            sacrifices,
            pairs,
        ) = select_replacement_candidates(
            frequency_path=self.frequency_path,
            sequences_path=self.sequences_path,
            tokenizer_dir=self.tokenizer_dir,
            max_candidates=self.max_candidates,
            min_token_len=self.min_token_len,
        )

        # 5) ë°”ì´ê·¸ë¨ ì§‘ê³„ (íŠ¸ë˜í‚¹ ì¶”ê°€)
        # select_replacement_candidates ë‚´ë¶€ì—ì„œ bigram_countsê°€ ì´ë¯¸ ê³„ì‚°ë˜ì—ˆìœ¼ë¯€ë¡œ,
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ logger.infoë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
        # trackì€ ì´ë¯¸ ë„ë©”ì¸ í•¨ìˆ˜ì—ì„œ ì œê±°ë˜ì—ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œ trackì„ ê°ìŒ€ í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤.
        # bigram_countsëŠ” Counter ê°ì²´ì´ë¯€ë¡œ iterableì´ ì•„ë‹™ë‹ˆë‹¤.
        logger.info("ê³ ìœ  ë°”ì´ê·¸ë¨ %dê°œë¥¼ ì§‘ê³„í–ˆìŠµë‹ˆë‹¤.", len(bigram_counts))

        # 6) ì‹ ê·œ í† í° í›„ë³´ íƒìƒ‰ (íŠ¸ë˜í‚¹ ì¶”ê°€)
        # new_tokens_listë„ ì´ë¯¸ ê³„ì‚°ëœ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ, ì—¬ê¸°ì„œ trackì„ ê°ìŒ€ í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤.
        logger.info("ì‹ ê·œ í† í° í›„ë³´ %dê°œë¥¼ ì„ ì •í–ˆìŠµë‹ˆë‹¤.", len(new_tokens_list))

        # 8) ê²°ê³¼ ì €ì¥
        write_replacement_csv(pairs, self.output_csv)
        logger.info("ğŸ“„ êµì²´ í›„ë³´ CSV ì €ì¥ ì™„ë£Œ: %s", self.output_csv)

        write_selection_log(
            pairs=pairs,
            total_vocab=tokenizer.vocab_size,
            total_protected=len(
                sacrifices
            ),  # Corrected from protected_ids to sacrifices
            total_sacrifice_pool=tokenizer.vocab_size - len(sacrifices),  # Corrected
            total_bigrams=len(bigram_counts),
            output_path=self.output_log,
        )
        logger.info("ğŸ“ ì„ ì • ë¡œê·¸ ì €ì¥ ì™„ë£Œ: %s", self.output_log)

        return {
            "pairs_count": len(pairs),
            "csv_path": self.output_csv,
            "log_path": self.output_log,
            "sacrifice_count": len(sacrifices),
            "new_token_count": len(new_tokens_list),
        }

    def get_name(self) -> str:
        """ì»¤ë§¨ë“œ ì´ë¦„ì„ ë°˜í™˜í•œë‹¤.

        Returns:
            ì»¤ë§¨ë“œ ì´ë¦„ "select"
        """
        return "select"
