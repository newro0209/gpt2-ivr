"""IVR êµì²´ í›„ë³´ ì„ ì • ì»¤ë§¨ë“œ.

í† í° ë¹ˆë„ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì €ë¹ˆë„ í¬ìƒ í† í°ê³¼ ê³ ë¹ˆë„ ë°”ì´ê·¸ë¨ ë³‘í•© í›„ë³´ë¥¼
ë§¤ì¹­í•˜ì—¬ IVR(Infrequent Vocabulary Replacement) êµì²´ í›„ë³´ë¥¼ ì„ ì •í•œë‹¤.
"""

from __future__ import annotations

import argparse
import logging
from pathlib import Path
from typing import Any

from rich.console import Console
from rich.table import Table

from gpt2_ivr.analysis.candidate_selection import (
    select_replacement_candidates,
    write_replacement_csv,
    write_selection_log,
)
from gpt2_ivr.constants import (
    BPE_TOKEN_ID_SEQUENCES_FILE,
    REPLACEMENT_CANDIDATES_FILE,
    SELECTION_LOG_FILE,
    TOKENIZER_ORIGINAL_DIR,
    TOKEN_FREQUENCY_FILE,
)
from gpt2_ivr.parser import CliHelpFormatter, positive_int

from .base import Command, SubparsersLike

logger = logging.getLogger(__name__)


class SelectCommand(Command):
    """IVR êµì²´ í›„ë³´ ì„ ì • ì»¤ë§¨ë“œ.

    ì €ë¹ˆë„ í† í°ì„ í¬ìƒ í›„ë³´ë¡œ, ê³ ë¹ˆë„ ë°”ì´ê·¸ë¨ì„ ì‹ ê·œ í† í° í›„ë³´ë¡œ ì„ ì •í•˜ì—¬
    êµì²´ ìŒì„ ìƒì„±í•œë‹¤.

    Attributes:
        console: Rich ì½˜ì†” ì¸ìŠ¤í„´ìŠ¤
        frequency_path: í† í° ë¹ˆë„ parquet íŒŒì¼ ê²½ë¡œ
        sequences_path: BPE í† í° ì‹œí€€ìŠ¤ íŒŒì¼ ê²½ë¡œ
        output_csv: êµì²´ í›„ë³´ CSV ì €ì¥ ê²½ë¡œ
        output_log: ì„ ì • ë¡œê·¸ ì €ì¥ ê²½ë¡œ
        tokenizer_dir: ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬
        max_candidates: ìµœëŒ€ í›„ë³´ ê°œìˆ˜
        min_token_len: ë³´í˜¸ í† í° ìµœì†Œ ê¸¸ì´
    """

    @staticmethod
    def configure_parser(subparsers: SubparsersLike) -> None:
        """ì„œë¸Œì»¤ë§¨ë“œ íŒŒì„œë¥¼ ì„¤ì •í•œë‹¤.

        Args:
            subparsers: ì„œë¸ŒíŒŒì„œ ì•¡ì…˜ ê°ì²´
        """
        parser = subparsers.add_parser("select", help="IVR ëŒ€ìƒ í† í° ì„ ì •", formatter_class=CliHelpFormatter)
        parser.add_argument(
            "--frequency-path", type=Path, default=TOKEN_FREQUENCY_FILE, help="í† í° ë¹ˆë„ parquet íŒŒì¼ ê²½ë¡œ"
        )
        parser.add_argument(
            "--sequences-path", type=Path, default=BPE_TOKEN_ID_SEQUENCES_FILE, help="BPE í† í° ì‹œí€€ìŠ¤ íŒŒì¼ ê²½ë¡œ"
        )
        parser.add_argument(
            "--output-csv", type=Path, default=REPLACEMENT_CANDIDATES_FILE, help="êµì²´ í›„ë³´ CSV ì €ì¥ ê²½ë¡œ"
        )
        parser.add_argument("--output-log", type=Path, default=SELECTION_LOG_FILE, help="ì„ ì • ë¡œê·¸ ì €ì¥ ê²½ë¡œ")
        parser.add_argument(
            "--tokenizer-dir",
            type=Path,
            default=TOKENIZER_ORIGINAL_DIR,
            help="ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
        )
        parser.add_argument("--max-candidates", type=positive_int, default=1000, help="ìµœëŒ€ í›„ë³´ ê°œìˆ˜")
        parser.add_argument("--min-token-len", type=positive_int, default=2, help="ë³´í˜¸ í† í° ìµœì†Œ ê¸¸ì´")

    def __init__(
        self,
        console: Console,
        frequency_path: Path,
        sequences_path: Path,
        output_csv: Path,
        output_log: Path,
        tokenizer_dir: Path,
        max_candidates: int,
        min_token_len: int,
    ):
        self.console = console
        self.frequency_path = frequency_path
        self.sequences_path = sequences_path
        self.output_csv = output_csv
        self.output_log = output_log
        self.tokenizer_dir = tokenizer_dir
        self.max_candidates = max_candidates
        self.min_token_len = min_token_len

    def execute(self) -> dict[str, Any]:
        """êµì²´ í›„ë³´ ì„ ì •ì„ ì‹¤í–‰í•œë‹¤.

        í† í° ë¹ˆë„ì™€ ë°”ì´ê·¸ë¨ í†µê³„ë¥¼ ë¶„ì„í•˜ì—¬ êµì²´ í›„ë³´ ìŒì„ ìƒì„±í•˜ê³ 
        CSVì™€ ë§ˆí¬ë‹¤ìš´ ë¡œê·¸ë¡œ ì €ì¥í•œë‹¤.

        Returns:
            ì„ ì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (pairs_count, csv_path, log_path, sacrifice_count, new_token_count)
        """
        (
            bigram_counts,
            new_tokens_list,
            tokenizer,
            sacrifices,
            pairs,
        ) = select_replacement_candidates(
            frequency_path=self.frequency_path,
            sequences_path=self.sequences_path,
            output_csv=self.output_csv,
            output_log=self.output_log,
            tokenizer_dir=self.tokenizer_dir,
            max_candidates=self.max_candidates,
            min_token_len=self.min_token_len,
        )

        # ê²°ê³¼ ì €ì¥
        write_replacement_csv(pairs, self.output_csv)
        write_selection_log(
            pairs=pairs,
            total_vocab=tokenizer.vocab_size,
            total_protected=len(sacrifices),
            total_sacrifice_pool=tokenizer.vocab_size - len(sacrifices),
            total_bigrams=len(bigram_counts),
            output_path=self.output_log,
        )

        # Rich í…Œì´ë¸”ë¡œ ê²°ê³¼ ì¶œë ¥
        table = Table(title="ğŸ¯ êµì²´ í›„ë³´ ì„ ì • ê²°ê³¼", show_header=True, title_style="bold green")
        table.add_column("í•­ëª©", style="bold cyan", width=20)
        table.add_column("ê°’", style="yellow", justify="right")

        table.add_row("êµì²´ í›„ë³´ ìŒ", f"{len(pairs):,}ê°œ")
        table.add_row("í¬ìƒ í›„ë³´", f"{len(sacrifices):,}ê°œ")
        table.add_row("ì‹ ê·œ í† í° í›„ë³´", f"{len(new_tokens_list):,}ê°œ")
        table.add_row("ê³ ìœ  ë°”ì´ê·¸ë¨", f"{len(bigram_counts):,}ê°œ")
        table.add_row("", "")  # ë¹ˆ ì¤„
        table.add_row("CSV íŒŒì¼", str(self.output_csv))
        table.add_row("ë¡œê·¸ íŒŒì¼", str(self.output_log))

        self.console.print()
        self.console.print(table)

        # ìƒ˜í”Œ êµì²´ í›„ë³´ í‘œì‹œ (ìƒìœ„ 5ê°œ)
        if pairs:
            sample_table = Table(title="ğŸ“‹ êµì²´ í›„ë³´ ìƒ˜í”Œ (ìƒìœ„ 5ê°œ)", show_header=True, border_style="dim")
            sample_table.add_column("í¬ìƒ í† í° ID", style="red", width=15, justify="center")
            sample_table.add_column("â†’", style="dim", width=3, justify="center")
            sample_table.add_column("ì‹ ê·œ í† í°", style="green", width=30)
            sample_table.add_column("ë¹ˆë„", style="yellow", width=12, justify="right")

            for pair in pairs[:5]:
                sacrifice_id = pair.sacrifice.token_id
                new_token = pair.new_token.merged_str
                frequency = pair.new_token.bigram_freq
                sample_table.add_row(f"{sacrifice_id}", "â†’", f"{new_token}", f"{frequency:,}íšŒ")

            self.console.print()
            self.console.print(sample_table)

        self.console.print()

        return {
            "pairs_count": len(pairs),
            "csv_path": self.output_csv,
            "log_path": self.output_log,
            "sacrifice_count": len(sacrifices),
            "new_token_count": len(new_tokens_list),
        }

    def get_name(self) -> str:
        """ì»¤ë§¨ë“œ ì´ë¦„ì„ ë°˜í™˜í•œë‹¤.

        Returns:
            ì»¤ë§¨ë“œ ì´ë¦„ "select"
        """
        return "select"
