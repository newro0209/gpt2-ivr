"""í† í° ë¹ˆë„ ë¶„ì„ ì»¤ë§¨ë“œ.

ì½”í¼ìŠ¤ íŒŒì¼ì„ BPE í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”í•˜ì—¬ ê° í† í°ì˜ ì¶œí˜„ ë¹ˆë„ë¥¼
ë¶„ì„í•˜ê³  ì‹œí€€ìŠ¤ íŒŒì¼ê³¼ ë¹ˆë„ í†µê³„ë¥¼ ìƒì„±í•œë‹¤.
"""

from __future__ import annotations

import argparse
import logging
from collections import Counter
from pathlib import Path
from typing import Any

from rich.console import Console
from rich.panel import Panel
from rich.progress import track
from rich.table import Table

from gpt2_ivr.constants import (
    BPE_TOKEN_ID_SEQUENCES_FILE,
    CORPORA_CLEANED_DIR,
    TOKENIZER_ORIGINAL_DIR,
    TOKEN_FREQUENCY_FILE,
)
from gpt2_ivr.parser import CliHelpFormatter, non_negative_int

from .base import Command, SubparsersLike

logger = logging.getLogger(__name__)


class AnalyzeCommand(Command):
    """í† í° ë¹ˆë„ ë¶„ì„ ì»¤ë§¨ë“œ.

    ì½”í¼ìŠ¤ë¥¼ í† í°í™”í•˜ì—¬ BPE í† í° ID ì‹œí€€ìŠ¤ì™€ ë¹ˆë„ í†µê³„ë¥¼ ìƒì„±í•œë‹¤.
    ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ëŒ€ìš©ëŸ‰ ì½”í¼ìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.

    Attributes:
        console: Rich ì½˜ì†” ì¸ìŠ¤í„´ìŠ¤
        input_dir: ì½”í¼ìŠ¤ ì…ë ¥ ë””ë ‰í† ë¦¬
        output_sequences: BPE í† í° ì‹œí€€ìŠ¤ ì¶œë ¥ ê²½ë¡œ
        output_frequency: í† í° ë¹ˆë„ parquet ì¶œë ¥ ê²½ë¡œ
        tokenizer_dir: ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬
        workers: ìŠ¤ë ˆë“œ ì›Œì»¤ ìˆ˜ (0ì´ë©´ CPU - 1)
        chunk_size: ìŠ¤ë ˆë“œ ì²­í¬ í¬ê¸° (0ì´ë©´ ìë™ ì„¤ì •)
        max_texts: ì²˜ë¦¬í•  ìµœëŒ€ í…ìŠ¤íŠ¸ ìˆ˜ (0ì´ë©´ ì „ì²´)
        encoding: ì…ë ¥ íŒŒì¼ ì¸ì½”ë”©
    """

    @staticmethod
    def configure_parser(subparsers: SubparsersLike) -> None:
        """ì„œë¸Œì»¤ë§¨ë“œ íŒŒì„œë¥¼ ì„¤ì •í•œë‹¤.

        Args:
            subparsers: ì„œë¸ŒíŒŒì„œ ì•¡ì…˜ ê°ì²´
        """
        parser = subparsers.add_parser("analyze", help="BPE í† í° ì‹œí€€ìŠ¤ ë¶„ì„", formatter_class=CliHelpFormatter)
        parser.add_argument("--input-dir", type=Path, default=CORPORA_CLEANED_DIR, help="ì½”í¼ìŠ¤ ì…ë ¥ ë””ë ‰í† ë¦¬")
        parser.add_argument(
            "--output-sequences", type=Path, default=BPE_TOKEN_ID_SEQUENCES_FILE, help="BPE í† í° ì‹œí€€ìŠ¤ ì¶œë ¥ ê²½ë¡œ"
        )
        parser.add_argument(
            "--output-frequency", type=Path, default=TOKEN_FREQUENCY_FILE, help="í† í° ë¹ˆë„ parquet ì¶œë ¥ ê²½ë¡œ"
        )
        parser.add_argument(
            "--tokenizer-dir",
            type=Path,
            default=TOKENIZER_ORIGINAL_DIR,
            help="ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬",
        )
        parser.add_argument("--workers", type=non_negative_int, default=0, help="ìŠ¤ë ˆë“œ ì›Œì»¤ ìˆ˜ (0ì´ë©´ CPU - 1)")
        parser.add_argument("--chunk-size", type=non_negative_int, default=0, help="ìŠ¤ë ˆë“œ ì²­í¬ í¬ê¸°(0ì´ë©´ ìë™ ì„¤ì •)")
        parser.add_argument("--max-texts", type=non_negative_int, default=0, help="ì²˜ë¦¬í•  ìµœëŒ€ í…ìŠ¤íŠ¸ ìˆ˜ (0ì´ë©´ ì „ì²´)")
        parser.add_argument("--encoding", default="utf-8", help="ì…ë ¥ íŒŒì¼ ì¸ì½”ë”©")

    def __init__(
        self,
        console: Console,
        input_dir: Path,
        output_sequences: Path,
        output_frequency: Path,
        tokenizer_dir: Path,
        workers: int,
        chunk_size: int,
        max_texts: int,
        encoding: str,
    ):
        """ë¹ˆë„ ë¶„ì„ ì»¤ë§¨ë“œë¥¼ ìƒì„±í•œë‹¤.

        Args:
            console: Rich ì½˜ì†” ì¸ìŠ¤í„´ìŠ¤
            input_dir: ì½”í¼ìŠ¤ ì…ë ¥ ë””ë ‰í† ë¦¬
            output_sequences: BPE í† í° ì‹œí€€ìŠ¤ ì¶œë ¥ ê²½ë¡œ
            output_frequency: í† í° ë¹ˆë„ parquet ì¶œë ¥ ê²½ë¡œ
            tokenizer_dir: ì›ë³¸ í† í¬ë‚˜ì´ì € ë””ë ‰í† ë¦¬
            workers: ìŠ¤ë ˆë“œ ì›Œì»¤ ìˆ˜ (0ì´ë©´ CPU - 1)
            chunk_size: ìŠ¤ë ˆë“œ ì²­í¬ í¬ê¸° (0ì´ë©´ ìë™ ì„¤ì •)
            max_texts: ì²˜ë¦¬í•  ìµœëŒ€ í…ìŠ¤íŠ¸ ìˆ˜ (0ì´ë©´ ì „ì²´)
            encoding: ì…ë ¥ íŒŒì¼ ì¸ì½”ë”©
        """
        self.console = console
        self.input_dir = input_dir
        self.output_sequences = output_sequences
        self.output_frequency = output_frequency
        self.tokenizer_dir = tokenizer_dir
        self.workers = workers
        self.chunk_size = chunk_size
        self.max_texts = max_texts
        self.encoding = encoding

    def execute(self) -> dict[str, Any]:
        """í† í° ë¹ˆë„ ë¶„ì„ì„ ì‹¤í–‰í•œë‹¤.

        ì½”í¼ìŠ¤ë¥¼ ì½ì–´ í† í°í™”í•˜ê³ , í† í° ID ì‹œí€€ìŠ¤ íŒŒì¼ê³¼ ë¹ˆë„ í†µê³„ íŒŒì¼ì„ ìƒì„±í•œë‹¤.

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (sequences_path, frequency_path, total_tokens, unique_tokens)
        """

        from gpt2_ivr.analysis.token_frequency import (
            analyze_token_frequency,
            write_frequency_parquet,
        )

        encoded_chunks_iterator, tokenizer = analyze_token_frequency(
            input_dir=self.input_dir,
            inputs=[],
            output_frequency=self.output_frequency,
            tokenizer_dir=self.tokenizer_dir,
            workers=self.workers,
            chunk_size=self.chunk_size,
            max_texts=self.max_texts,
            encoding=self.encoding,
        )

        counter: Counter[int] = Counter()
        self.output_sequences.parent.mkdir(parents=True, exist_ok=True)
        with self.output_sequences.open("w", encoding="utf-8") as handle:
            for chunk_ids in track(encoded_chunks_iterator, description="í† í°í™” ì¤‘"):
                for token_ids in chunk_ids:
                    counter.update(token_ids)
                    handle.write(" ".join(str(token_id) for token_id in token_ids))
                    handle.write("\n")

        # ê²°ê³¼ë¬¼ ì €ì¥ (ë¹ˆë„ parquet)
        self.output_frequency.parent.mkdir(parents=True, exist_ok=True)
        write_frequency_parquet(counter, self.output_frequency)

        total_tokens = sum(counter.values())
        unique_tokens = len(counter)

        # ì¶”ê°€ í†µê³„ ê³„ì‚°
        top_10 = counter.most_common(10)
        avg_frequency = total_tokens / unique_tokens if unique_tokens > 0 else 0

        # Rich í…Œì´ë¸”ë¡œ ê²°ê³¼ ì¶œë ¥
        table = Table(title="âœ¨ í† í° ë¹ˆë„ ë¶„ì„ ê²°ê³¼", show_header=True, title_style="bold green")
        table.add_column("í•­ëª©", style="bold cyan", width=20)
        table.add_column("ê°’", style="yellow", justify="right")

        table.add_row("ì´ í† í° ìˆ˜", f"{total_tokens:,}ê°œ")
        table.add_row("ê³ ìœ  í† í° ìˆ˜", f"{unique_tokens:,}ê°œ")
        table.add_row("í‰ê·  ë¹ˆë„", f"{avg_frequency:.2f}íšŒ")
        table.add_row("", "")  # ë¹ˆ ì¤„
        table.add_row("ë¹ˆë„ íŒŒì¼", str(self.output_frequency))
        table.add_row("ì‹œí€€ìŠ¤ íŒŒì¼", str(self.output_sequences))

        self.console.print()
        self.console.print(table)

        # ìƒìœ„ 10ê°œ í† í° í‘œì‹œ
        if top_10:
            top_table = Table(title="ğŸ† ìƒìœ„ 10ê°œ ë¹ˆë„ í† í°", show_header=True, border_style="dim")
            top_table.add_column("ìˆœìœ„", style="dim", width=6, justify="center")
            top_table.add_column("í† í° ID", style="cyan", width=10, justify="right")
            top_table.add_column("ë¹ˆë„", style="yellow", width=15, justify="right")

            for idx, (token_id, freq) in enumerate(top_10, 1):
                rank_style = "bold green" if idx <= 3 else "dim"
                top_table.add_row(f"{idx}", f"{token_id}", f"{freq:,}íšŒ", style=rank_style)

            self.console.print()
            self.console.print(top_table)

        self.console.print()

        return {
            "sequences_path": self.output_sequences,
            "frequency_path": self.output_frequency,
            "total_tokens": total_tokens,
            "unique_tokens": unique_tokens,
        }

    def get_name(self) -> str:
        """ì»¤ë§¨ë“œ ì´ë¦„ì„ ë°˜í™˜í•œë‹¤.

        Returns:
            ì»¤ë§¨ë“œ ì´ë¦„ "analyze"
        """
        return "analyze"
