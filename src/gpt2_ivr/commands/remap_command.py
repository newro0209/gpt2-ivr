"""í† í¬ë‚˜ì´ì € ì¬í• ë‹¹ ì»¤ë§¨ë“œ"""

from __future__ import annotations

import logging
from pathlib import Path
from typing import Any

import pandas as pd
import yaml
from tokenizers import Tokenizer

from gpt2_ivr.commands.base import Command
from gpt2_ivr.utils.logging_config import get_logger


class RemapCommand(Command):
    """í† í° ì¬í• ë‹¹ ê·œì¹™ ì ìš© ì»¤ë§¨ë“œ"""

    def __init__(
        self,
        distilled_tokenizer_dir: Path,
        remapped_tokenizer_dir: Path,
        remap_rules_path: Path,
        replacement_candidates_path: Path,
    ) -> None:
        self.logger = get_logger("gpt2_ivr.remap")
        self.distilled_tokenizer_path = distilled_tokenizer_dir
        self.remapped_tokenizer_path = remapped_tokenizer_dir
        self.remap_rules_path = remap_rules_path
        self.replacement_candidates_path = replacement_candidates_path

    def execute(self, **kwargs: Any) -> dict[str, Any]:
        """ì»¤ë§¨ë“œ ì‹¤í–‰ ë¡œì§"""
        self.logger.info("ğŸš€ remap ë‹¨ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

        # 1. ì¦ë¥˜ í† í¬ë‚˜ì´ì € ë¡œë“œ
        if not self.distilled_tokenizer_path.exists():
            raise FileNotFoundError(
                "ì¦ë¥˜ í† í¬ë‚˜ì´ì €ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                f"ë¨¼ì € `uv run ivr distill-tokenizer`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: "
                f"{self.distilled_tokenizer_path}"
            )

        self.logger.info("ì¦ë¥˜ í† í¬ë‚˜ì´ì € ë¡œë“œ: %s", self.distilled_tokenizer_path)
        tokenizer = Tokenizer.from_file(
            str(self.distilled_tokenizer_path / "tokenizer.json")
        )

        # 2. êµì²´ í›„ë³´ ë¡œë“œ (ì„ íƒ, ë¡œê·¸ ì •ë³´ìš©)
        if self.replacement_candidates_path.exists():
            candidates_df = pd.read_csv(self.replacement_candidates_path)
            self.logger.info(
                "êµì²´ í›„ë³´ %dê°œ ë¡œë“œ: %s",
                len(candidates_df),
                self.replacement_candidates_path,
            )
            # self.logger.debug("êµì²´ í›„ë³´ ìƒ˜í”Œ:\n%s", candidates_df.head())
        else:
            self.logger.warning(
                "êµì²´ í›„ë³´ CSVê°€ ì—†ì–´ ìƒì„¸ ë¡œê·¸ë¥¼ ìƒëµí•©ë‹ˆë‹¤: %s",
                self.replacement_candidates_path,
            )

        # 3. ì¬í• ë‹¹ ê·œì¹™ ë¡œë“œ
        if not self.remap_rules_path.exists():
            raise FileNotFoundError(
                "ì¬í• ë‹¹ ê·œì¹™ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: " f"{self.remap_rules_path}"
            )

        with self.remap_rules_path.open("r", encoding="utf-8") as handle:
            loaded_rules = yaml.safe_load(handle)

        if loaded_rules is None:
            self.logger.warning(
                "ì¬í• ë‹¹ ê·œì¹™ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤: %s", self.remap_rules_path
            )
            remap_rules: dict[str, str] = {}
        elif isinstance(loaded_rules, dict):
            remap_rules = loaded_rules
        else:
            raise ValueError(
                "ì¬í• ë‹¹ ê·œì¹™ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                "YAML ë§¤í•‘(dict) í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
            )

        self.logger.info(
            "ì¬í• ë‹¹ ê·œì¹™ %dê°œ ë¡œë“œ: %s",
            len(remap_rules),
            self.remap_rules_path,
        )

        # 4. ì¬í• ë‹¹ ê·œì¹™ ì ìš©
        # í˜„ì¬ êµ¬í˜„ì€ ë‹¨ìˆœí™”ëœ í˜•íƒœì´ë©°, ì‹¤ì œ IVRì—ì„œëŠ”
        # í† í° ID/ì–´íœ˜ ì¬ë°°ì¹˜ë¥¼ ë” ì •êµí•˜ê²Œ ë‹¤ë£° í•„ìš”ê°€ ìˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ì‹ ê·œ í† í° ì¶”ê°€ ì¤‘ì‹¬ìœ¼ë¡œ ë™ì‘í•œë‹¤.
        current_vocab_size = tokenizer.get_vocab_size()
        self.logger.info("í˜„ì¬ í† í¬ë‚˜ì´ì € vocab í¬ê¸°: %d", current_vocab_size)

        new_tokens_to_add = []
        for old_token, new_token in remap_rules.items():
            old_id = tokenizer.token_to_id(old_token)
            new_id = tokenizer.token_to_id(new_token)

            if old_id is None and new_id is None:
                # ì–‘ìª½ ëª¨ë‘ ì‹ ê·œ í† í°ì¸ ê²½ìš°
                new_tokens_to_add.append(new_token)
                self.logger.info(
                    "ê·œì¹™: '%s' -> '%s' (ì‹ ê·œ í† í° ì¶”ê°€ ì˜ˆì •)",
                    old_token,
                    new_token,
                )
            elif old_id is not None and new_id is None:
                # ê¸°ì¡´ í† í°ì€ ìˆê³  ì‹ ê·œ í† í°ì€ ì—†ëŠ” ê²½ìš°
                new_tokens_to_add.append(new_token)
                self.logger.info(
                    "ê·œì¹™: '%s'(id:%d) -> '%s' (ì‹ ê·œ í† í° ì¶”ê°€ ì˜ˆì •)",
                    old_token,
                    old_id,
                    new_token,
                )
            elif old_id is None and new_id is not None:
                self.logger.warning(
                    "ê·œì¹™ ë¬´ì‹œ: '%s' -> '%s'(id:%d), ëŒ€ìƒ í† í°ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.",
                    old_token,
                    new_token,
                    new_id,
                )
            else:  # ì–‘ìª½ ëª¨ë‘ ê¸°ì¡´ í† í°ì¸ ê²½ìš°
                self.logger.info(
                    "ê·œì¹™: '%s'(id:%d) -> '%s'(id:%d), ê¸°ì¡´ í† í° ìœ ì§€",
                    old_token,
                    old_id,
                    new_token,
                    new_id,
                )

        if new_tokens_to_add:
            self.logger.info("ì‹ ê·œ í† í° %dê°œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.", len(new_tokens_to_add))
            # ì¤‘ë³µ ì œê±° í›„ ì‹ ê·œ í† í° ì¶”ê°€
            tokenizer.add_tokens(list(set(new_tokens_to_add)))
            self.logger.info(
                "í† í° ì¶”ê°€ í›„ vocab í¬ê¸°: %d",
                tokenizer.get_vocab_size(),
            )
        else:
            self.logger.info("ì¶”ê°€í•  ì‹ ê·œ í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")

        # 5. ì¬í• ë‹¹ í† í¬ë‚˜ì´ì € ì €ì¥
        self.remapped_tokenizer_path.mkdir(parents=True, exist_ok=True)
        tokenizer.save(str(self.remapped_tokenizer_path / "tokenizer.json"))

        self.logger.info(
            "ì¬í• ë‹¹ í† í¬ë‚˜ì´ì € ì €ì¥: %s",
            self.remapped_tokenizer_path / "tokenizer.json",
        )
        self.logger.info("âœ… remap ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return {
            "status": "success",
            "remapped_tokenizer_path": str(self.remapped_tokenizer_path),
        }

    def get_name(self) -> str:
        """ì»¤ë§¨ë“œ ì´ë¦„ ë°˜í™˜"""
        return "remap"
