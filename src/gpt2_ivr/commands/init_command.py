"""ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì»¤ë§¨ë“œ.

Hugging Face Hubì—ì„œ GPT-2 ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬
ë¡œì»¬ì— ì €ì¥í•˜ëŠ” ì´ˆê¸°í™” ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•œë‹¤.
"""

from __future__ import annotations

import argparse
from pathlib import Path
from typing import Any

from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from gpt2_ivr.constants import CORPORA_CLEANED_DIR, CORPORA_RAW_DIR, TOKENIZER_ORIGINAL_DIR
from gpt2_ivr.corpus.normalize import normalize_raw_corpora
from gpt2_ivr.parser import CliHelpFormatter
from gpt2_ivr.tokenizer import initialize_assets

from .base import Command, SubparsersLike


class InitCommand(Command):
    """ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì»¤ë§¨ë“œ.

    Hugging Face Hubì—ì„œ ì§€ì •ëœ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬
    ë¡œì»¬ ë””ë ‰í† ë¦¬ì— ì €ì¥í•œë‹¤.

    Attributes:
        console: Rich ì½˜ì†” ì¸ìŠ¤í„´ìŠ¤
        model_name: Hugging Face Hub ëª¨ë¸ ì´ë¦„
        tokenizer_dir: í† í¬ë‚˜ì´ì € ì €ì¥ ë””ë ‰í† ë¦¬
        force: ê¸°ì¡´ íŒŒì¼ì´ ìˆì–´ë„ ì¬ë‹¤ìš´ë¡œë“œ ì—¬ë¶€
        raw_corpora_dir: ì •ì œ ì „ ì›ë³¸ ì½”í¼ìŠ¤ ë””ë ‰í† ë¦¬
        cleaned_corpora_dir: ì •ì œëœ ì½”í¼ìŠ¤ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
        text_key: JSON/JSONL íŒŒì¼ì—ì„œ ì¶”ì¶œí•  í…ìŠ¤íŠ¸ í‚¤
        encoding: ì½”í¼ìŠ¤ íŒŒì¼ ì¸ì½”ë”©
        normalize_force: ì¡´ì¬í•˜ëŠ” ì •ì œë³¸ì´ ìˆì–´ë„ ë®ì–´ì“¸ì§€ ì—¬ë¶€
    """

    @staticmethod
    def configure_parser(subparsers: SubparsersLike) -> None:
        """ì„œë¸Œì»¤ë§¨ë“œ íŒŒì„œë¥¼ ì„¤ì •í•œë‹¤.

        Args:
            subparsers: ì„œë¸ŒíŒŒì„œ ì•¡ì…˜ ê°ì²´
        """
        parser = subparsers.add_parser("init", help="ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”", formatter_class=CliHelpFormatter)
        parser.add_argument("--model-name", default="openai-community/gpt2", help="Hugging Face Hub ëª¨ë¸ ì´ë¦„")
        parser.add_argument(
            "--tokenizer-dir", type=Path, default=TOKENIZER_ORIGINAL_DIR, help="í† í¬ë‚˜ì´ì € ì €ì¥ ë””ë ‰í† ë¦¬"
        )
        parser.add_argument("--force", action="store_true", help="ê¸°ì¡´ íŒŒì¼ì´ ìˆì–´ë„ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ")
        parser.add_argument(
            "--raw-corpora-dir",
            type=Path,
            default=CORPORA_RAW_DIR,
            help="raw ì½”í¼ìŠ¤ê°€ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬",
        )
        parser.add_argument(
            "--cleaned-corpora-dir",
            type=Path,
            default=CORPORA_CLEANED_DIR,
            help="ì •ì œëœ ì½”í¼ìŠ¤ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬",
        )
        parser.add_argument("--text-key", default="text", help="JSON/JSONL íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì½ì–´ì˜¬ í‚¤")
        parser.add_argument("--encoding", default="utf-8", help="ì…ë ¥ ì½”í¼ìŠ¤ íŒŒì¼ ì¸ì½”ë”©")
        parser.add_argument(
            "--normalize-force",
            action="store_true",
            help="ì´ë¯¸ ì •ì œë³¸ì´ ìˆì–´ë„ raw íŒŒì¼ì„ ë‹¤ì‹œ ë³€í™˜í•©ë‹ˆë‹¤",
        )

    def __init__(
        self,
        console: Console,
        model_name: str,
        tokenizer_dir: Path,
        force: bool,
        raw_corpora_dir: Path,
        cleaned_corpora_dir: Path,
        text_key: str,
        encoding: str,
        normalize_force: bool,
    ):
        self.console = console
        self.model_name = model_name
        self.tokenizer_dir = tokenizer_dir
        self.force = force
        self.raw_corpora_dir = raw_corpora_dir
        self.cleaned_corpora_dir = cleaned_corpora_dir
        self.text_key = text_key
        self.encoding = encoding
        self.normalize_force = normalize_force

    def execute(self) -> dict[str, Any]:
        """í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”ë¥¼ ì‹¤í–‰í•œë‹¤.

        Returns:
            ì´ˆê¸°í™” ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (tokenizer_dir, vocab_size, model_name)
        """
        result = initialize_assets(
            model_name=self.model_name,
            tokenizer_dir=self.tokenizer_dir,
            force=self.force,
        )

        normalized_corpora = normalize_raw_corpora(
            raw_dir=self.raw_corpora_dir,
            cleaned_dir=self.cleaned_corpora_dir,
            text_key=self.text_key,
            encoding=self.encoding,
            force=self.normalize_force,
        )

        # Rich í…Œì´ë¸”ë¡œ ê²°ê³¼ ì¶œë ¥
        table = Table(title="ğŸš€ ì´ˆê¸°í™” ì™„ë£Œ", show_header=True, title_style="bold green")
        table.add_column("í•­ëª©", style="bold cyan", width=25)
        table.add_column("ê°’", style="yellow", justify="left")

        table.add_row("ëª¨ë¸", f"[bold]{result['model_name']}[/bold]")
        table.add_row("Vocab í¬ê¸°", f"{result['vocab_size']:,}ê°œ")
        table.add_row("", "")  # ë¹ˆ ì¤„
        table.add_row("í† í¬ë‚˜ì´ì € ê²½ë¡œ", str(result["tokenizer_dir"]))
        table.add_row("ì •ì œëœ ì½”í¼ìŠ¤", f"{len(normalized_corpora):,}ê°œ íŒŒì¼")
        table.add_row("ì½”í¼ìŠ¤ ê²½ë¡œ", str(self.cleaned_corpora_dir))

        self.console.print()
        self.console.print(table)
        self.console.print()

        # ì„±ê³µ ë©”ì‹œì§€
        self.console.print(
            Panel(
                "[bold green]âœ… ëª¨ë¸ ë° ì½”í¼ìŠ¤ ì´ˆê¸°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤[/bold green]\n"
                "[dim]ë‹¤ìŒ ë‹¨ê³„: [cyan]ivr analyze[/cyan] ëª…ë ¹ìœ¼ë¡œ í† í° ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”[/dim]",
                border_style="green",
                padding=(1, 2),
            )
        )
        self.console.print()

        return {
            "tokenizer_dir": result["tokenizer_dir"],
            "vocab_size": result["vocab_size"],
            "model_name": result["model_name"],
            "normalized_corpora": len(normalized_corpora),
            "cleaned_dir": self.cleaned_corpora_dir,
        }

    def get_name(self) -> str:
        """ì»¤ë§¨ë“œ ì´ë¦„ì„ ë°˜í™˜í•œë‹¤.

        Returns:
            ì»¤ë§¨ë“œ ì´ë¦„ "init"
        """
        return "init"
